1.Explain
Answer A wrong because ElastiCache is an in-memory caching service that optimizes read operations, not S3 PUTs which are write operations to object storage.

Answer B wrong because CodeDeploy deployments involve orchestration and rolling updates; ElastiCache does not influence deployment latency.

Answer C correct because ElastiCache caches frequently accessed data in memory, reducing database load for read-heavy workloads to improve latency and throughput.

Answer D wrong because CodeCommit branch merges are version control operations handled by Git; caching is irrelevant.

Answer E correct because ElastiCache can cache results of compute-intensive operations or intermediate data, offloading the database and speeding up repeated computations.

link ref: https://aws.amazon.com/elasticache/faqs/

2.Explain
Answer A correct because ElastiCache (Redis or Memcached) is an in-memory key-value store for caching.

Answer B wrong because SNS is a pub/sub messaging service for notifications, not a key-value store.

Answer C correct because DynamoDB is a NoSQL database supporting key-value and document data models.

Answer D wrong because SWF (Simple Workflow Service) orchestrates workflows, not stores key-value data.

Answer E correct because S3 is an object storage service where objects are addressed by keys (object names).

link ref: https://aws.amazon.com/products/databases/

3.Explain
Answer A wrong because account placement affects resource sharing but not header forwarding in ALB-Lambda integration.

Answer B wrong because request body size limits (1MB for sync) are unrelated to header handling.

Answer C wrong because Base64 encoding is about payload encoding for binary data, not about supporting multiple header values.

Answer D correct because ALB supports multi-value headers via a listener rule flag, allowing multiple values to be passed to Lambda.

By default, ALB sends single-value headers to Lambda targets.

To send multi-value headers (like multiple Set-Cookie or repeated custom headers), you must enable the multi-value headers feature on the ALB.

Once enabled, the Lambda function will receive all header values in the multiValueHeaders field in the event object.

link ref: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/lambda-functions.html

4.Explain
Answer A wrong because writing to cache first risks stale data and violates strong consistency.

Answer B wrong because cache expiration delays updates, not ensuring immediate consistency.

Answer C wrong because write-through (simultaneous) can fail if cache write succeeds but backend fails.

Answer D correct because write-behind (backend first, invalidate cache) ensures consistency while allowing responsive reads until invalidation.

link ref: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Strategies.html

5.Explain
Answer A wrong because VPN/VPC endpoint secures the network path but does not encrypt the data payload in transit (relies on HTTPS).

Answer B correct because client-side encryption with KMS encrypts data before upload over HTTPS, securing transit.

Answer C wrong because SSE-KMS is for at-rest encryption after upload.

Answer D correct because SSL/TLS (HTTPS) encrypts data during transfer to S3.

Answer E wrong because SSE-S3 is for at-rest encryption.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingEncryption.html

6.Explain
Answer A wrong because SSE-S3 uses AWS-managed keys without user-specific audit trails.

Answer B correct because SSE-KMS uses customer-managed keys with CloudTrail auditing for key usage.

Answer C wrong because client-side symmetric keys lack AWS-managed auditing.

Answer D wrong because client-side KMS still requires app-level auditing, not automatic like SSE-KMS.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html

7.Explain
Answer A correct because cross-account IAM roles allow secure temporary access without sharing long-lived credentials.

Answer B wrong because S3 replication and Lambda events are for data sync, not API access.

Answer C wrong because per-account deployments increase overhead and blast radius.

Answer D wrong because access keys are long-lived and insecure for cross-account use.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html

8.Explain
Answer A wrong because CodeDeploy requires bundles in accessible storage like S3, not local.

Answer B correct because S3 is the standard storage for CodeDeploy bundles from on-premises.

Answer C wrong because CodeCommit triggers builds, not direct deployments.

Answer D wrong because CodeBuild builds artifacts, not deploys to EC2.

link ref: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-config-files.html

9.Explain
Answer A correct because encrypted EBS volumes provide at-rest encryption with minimal performance impact (hardware-accelerated).

Answer B wrong because S3 adds latency for frequent compute access.

Answer C wrong because custom algorithms add CPU overhead.

Answer D wrong because ephemeral disks lose data on stop; AMI root doesn't encrypt attached volumes.

link ref: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html

10.Explain
Answer A wrong because prefixes help throughput but not global latency.

Answer B wrong because ElastiCache caches app data, not ideal for static images.

Answer C correct because CloudFront is a CDN that caches S3 content edge-located, reducing global latency.

Answer D wrong because rate limits are per prefix; issue is distribution, not limits.

link ref: https://aws.amazon.com/cloudfront/features/

11.Explain
Answer A wrong because default S3 KMS is SSE, not client-side.

Answer B wrong because S3 managed keys are SSE; GenerateDataKey is client-side.

Answer C correct because GenerateDataKey provides a data key for client-side encryption in Lambda before S3 upload.

Answer D wrong because custom KMS needs explicit policy; GenerateDataKey is the mechanism.

link ref: https://docs.aws.amazon.com/kms/latest/developerguide/create-data-keys.html

12.Explain
Answer A wrong because versioning tracks changes, not CORS.

Answer B wrong because public access exposes, but CORS is for browser cross-origin.

Answer C wrong because Content-MD5 is for integrity checks.

Answer D correct because CORS policy allows the website domain to access fonts in another bucket.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html

13.Explain
Answer A wrong because sam init initializes new projects.

Answer B wrong because sam validate syntax-checks templates.

Answer C correct because sam build packages dependencies and code.

Answer D correct because sam deploy creates/updates the stack.

Answer E wrong because sam publish shares templates.

link ref: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference.html

14.Explain
Answer A wrong because all-at-once stops all instances, causing downtime.

Answer B wrong because rolling replaces in batches, reducing capacity temporarily.

Answer C correct because rolling with additional batch launches extras to maintain full capacity during update.

Answer D wrong because immutable launches new fleet, not using existing instances.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html

15.Explain
Answer A correct because instance metadata endpoint provides public IP at /latest/meta-data/public-ipv4.

Answer B wrong because userdata is user-provided script/config.

Answer C wrong because ifconfig shows local IPs, not public.

Answer D wrong because ipconfig is Windows; EC2 typically Linux.

link ref: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html

16.Explain
Answer A wrong because single API caches all stages constantly, high cost.

Answer B wrong because three APIs triple management/cost.

Answer C wrong because separate accounts add complexity/cost.

Answer D correct because cache is billed by size/time; enable only for active use in dev/test.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html

17.Explain
Answer A wrong because Vertical Scaling (increasing instance size) and Retries address single-instance capacity and reliability, not the distribution of read load. Vertical scaling has limits and is expensive.

Answer B wrong because Multi-AZ provides a synchronous standby instance for failover only. The standby instance is not used to serve read traffic and therefore does not improve read performance or scalability.

Answer C correct because Read Replicas are asynchronous copies of the primary instance specifically designed to offload read-heavy traffic. By modifying the application code to direct all read queries to the Read Replica's distinct endpoint, the primary instance is reserved for write operations, achieving optimum read performance and horizontal scaling.

Answer D wrong because While technically possible, this involves setting up and managing MySQL replication manually on an EC2 instance, which defeats the purpose and benefits of using the fully managed Amazon RDS Read Replica feature.

link ref: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html

18.Explain
Answer A correct because This pattern involves components reacting immediately to an event (a change in state). When new data is written to the DynamoDB table, an event (via DynamoDB Streams) is generated, which can instantly trigger a processor (e.g., an AWS Lambda function).
-> Best Fit. Guarantees processing starts in near-real time as soon as the data is received, fulfilling the requirement to eliminate the nightly delay.

Answer B wrong because A generic term often referring to a request-response model where a client explicitly requests a service (e.g., via an API call).
-> Does not describe how backend data processing is initiated upon data receipt; focuses on the client-server interaction model.

Answer C wrong because An event pattern where a single message or event is replicated and sent to multiple consumers simultaneously (e.g., SNS topic publishing to multiple SQS queues).
-> While useful for scaling, "fan-out" is a distribution pattern, not the core initiation pattern for processing data upon receipt.

Answer D wrong because Processing is initiated at fixed, predetermined times (e.g., nightly, hourly, etc.), often using a cron job or services like Amazon CloudWatch/EventBridge Scheduler.
-> This describes the current nightly batch process and fails the near-real time requirement.

link ref: https://aws.amazon.com/event-driven-architecture/

19.Explain
Answer A wrong because EC2 size doesn't affect DynamoDB throttling.

Answer B wrong because RCUs for reads; issue is writes (ProvisionedThroughputExceeded).

Answer C correct because exponential backoff retries throttled requests without overwhelming.

Answer D wrong because increasing frequency exacerbates throttling.

Answer E correct because on-demand auto-scales capacity, avoiding provisioned limits.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-ddb-rate-limit.html

20.Explain
Answer A wrong because SSE-S3 you Zero control. AWS manages the keys completely.

Answer B wrong because SSE-C Customer generates, provides, and must manage the key for every upload/download request.

Answer C correct because SSE-KMS uses AWS-managed CMK with customer control/audit.

Answer D wrong because client-side requires app encryption management.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html

21.Explain
Answer A wrong because KCL allows multiple consumers per shard, but max instances = shards without standby.

Answer B correct because after resharding to 6 shards, max 6 instances (one primary per shard).

Answer C wrong because initial 4, but post-reshard 6.

Answer D wrong because not 1.

link ref: https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-using-sdk-java-add-shard.html

22.Explain
Answer A wrong because Lambda is a serverless compute service used to run code. If you use Lambda, you would be required to write and manage the backend application (the logic for receiving data, handling conflicts, and writing to a database), which is what the requirement explicitly seeks to avoid.
Requires creating a backend application/code.

Answer B wrong because S3 is object storage for files (like images, videos, documents). While it can store user files, it does not provide a built-in mechanism for cross-device synchronization, conflict resolution, or offline data management for small, frequently changing user data like game progress or settings.
Not a dedicated synchronization service; requires custom logic.

Answer C wrong because DynamoDB is a NoSQL database. While it could store the synchronized data, it does not include the client-side libraries necessary to handle offline storage, push updates, or manage conflict resolution across devices automatically. You would still have to create a backend application (e.g., using Lambda/API Gateway) to manage the connection and synchronization logic between the mobile app and the database.
Requires creating a backend application for sync logic.

Answer D correct because Amazon Cognito Sync (part of the Cognito Identity Pools) is specifically designed to synchronize application-related user data (like game state or user preferences) across mobile devices (iOS and Android). It stores data in the AWS Cloud and provides client libraries that automatically handle local caching, offline data storage, and synchronization when the device is online, all without requiring the developer to write and manage a custom backend application (Lambda, API Gateway, etc.).
This is the dedicated, managed, serverless service for cross-device data synchronization, directly matching the requirement.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-sync.html

23.Explain
Answer A correct The standard and simplest method is to **package the application code into a `.zip` file**, upload this source bundle directly via the Elastic Beanstalk console, and then select the option to deploy it to the environment. The console handles the creation of the new application version behind the scenes.

Answer B wrong Elastic Beanstalk supports `.zip` and `.war` files, but **not typically `.tar` files** as a standard source bundle format for direct upload/deployment. Also, creating the version via the console and updating the environment via CLI mixes methods unnecessarily.

Answer C wrong Elastic Beanstalk supports `.zip` or `.war` files for most environments, **not `.tar` files**.

Answer D correct This represents the standard automated/CLI workflow. The developer **packages the code into a `.zip` file**, then uses the **AWS CLI** (or EB CLI) commands to first **create a new application version** referencing the S3 location of the `.zip` file, and then uses the CLI to **update the environment** to use this new version. This is the common practice for CI/CD pipelines.

Answer C wrong You only need to **update** the environment with a new version; **rebuilding** the environment is an operation used to terminate and recreate all resources, which is generally overkill and time-consuming for a simple code change. Also, `.zip` is the correct package format.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html

24.Explain
Answer A correct because /tmp for local caching in Lambda.

Answer B wrong because timeout increase not cache.

Answer C wrong because ELB not Lambda.

Answer D wrong because S3 adds latency.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html

25.Explain
Answer A wrong because batch writes affect writes.

Answer B correct because GSI projects attributes, reduces RCU.

Answer C wrong because backoff for throttling.

Answer D wrong because ALB HTTP.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html

26.Explain
Answer A wrong because (Data Validation) Request validation checks if the incoming request payload (body or parameters) adheres to a defined schema. It does not transform or map the data structure.

Answer B wrong because (Service Identification) The ARN is used to identify the specific Lambda function to be invoked. It is necessary for integration but does not perform data transformation.

Answer C wrong because (Integration Mode) While the integration type (e.g., Lambda Proxy vs. Lambda Custom/Non-Proxy) dictates the default payload structure, converting query strings into a custom, specific JSON structure always requires a mapping template.

Answer D correct because A mapping template, written in Velocity Template Language (VTL), is executed in the Integration Request phase. The developer uses VTL to read the query string parameters (e.g., using $input.params('item')) and map them into the custom JSON structure expected by the Lambda function.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/request-response-data-mappings.html

27.Explain
Answer A wrong because roles better.

Answer B wrong because shared insecure.

Answer C correct because unique users keys secure local.

Answer D wrong because Cognito app.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html

28.Explain
Answer A wrong because strong more.

Answer B correct because strong 2x RCU eventual.

Answer C wrong because more not less.

Answer D wrong because fixed 2x.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html

29.Explain
Answer A correct because console upload deploy.

Answer B wrong because eb init setup.

Answer C wrong because terminate destructive.

Answer D wrong because ebextensions config.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-versions.html

30.Explain
Answer A correct because condition authenticated update user_name.

Answer B wrong because no web identity.

Answer C wrong because broad access.

Answer D wrong because misses condition.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_dynamodb_items.html

31.Explain
Answer A wrong because no traffic split.

Answer B correct because alias 10% shift safe.

Answer C wrong because ARN change risky.

Answer D wrong because multiple aliases overkill.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html

32.Explain
Answer A wrong because A single environment means all components (HTTP and background) would be deployed to the same set of EC2 instances and share a single Auto Scaling Group. This violates the core requirement for independent scaling. Scaling up the HTTP component would also unnecessarily scale up the background component instances, wasting resources.

Answer B correct because This creates two distinct environments, each with its own dedicated Auto Scaling Group and scaling policy. The HTTP component can scale based on CPU/Request count, and the Background component can scale based on an SQS queue depth, ensuring independent, cost-effective scaling for each workload.

Answer C wrong because This would be done for A/B testing or Blue/Green deployment of the HTTP tier, but it still puts the two distinct workloads (HTTP vs. Background) in separate environments, which is the correct principle. The simple requirement is just one environment per component. The simplest correct solution is one environment for each of the two components.

Answer D wrong because Same rationale as above. Multiple environments are used for deployment strategies (A/B or Blue/Green), not for separating the two primary logical components.The simplest correct solution is one environment for each of the two components.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.environments.html

33.Explain
Answer A correct because changesets preview impacts.

Answer B wrong because stack policies prevent.

Answer C wrong because Metadata info.

Answer D wrong because Resources define.

link ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html

34.Explain
Answer A wrong because different functions management.

Answer B wrong because stages versions, aliases no endpoint change.

Answer C correct because aliases point versions, no API change.

Answer D wrong because tags routing no.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html

35.Explain
Answer A wrong because Certificates are required on the server (S3 in this case, which is managed by AWS and already has them) or the client needs to trust the server's certificate. Installing certificates on the client (EC2 instance) does not force the client to use HTTPS; it only enables the client to trust the server if it chooses to connect securely.
-> Insufficient. Does not enforce HTTPS usage.

Answer B wrong because Insufficient. A policy with an Allow effect for secure transport does not block insecure requests unless an implicit Deny is active, which is not guaranteed. Using an explicit Deny with the aws:SecureTransport condition is the AWS best practice for enforcement.
-> Not Enforcing. An explicit Deny is needed for a hard mandate.

Answer C wrong because EC2 instances are the clients trying to connect to S3. A redirect configured on the client side is unreliable and easily bypassed, and it does not guarantee that the initial connection attempt is secure or that all subsequent traffic will be secure.
-> Ineffective. Incorrect location for enforcement.

Answer D correct because The aws:SecureTransport condition key is a boolean value that indicates if the request was sent over SSL/TLS (HTTPS). Setting a bucket policy to deny access when this key is false forces all clients, including the EC2 instances, to use HTTPS (TLS/SSL) for all S3 interactions, ensuring encryption in transit.
-> Guarantees Encryption using S3's native policy mechanism.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html

36.Explain
Answer A wrong because SQS/SNS messaging.

Answer B correct because ELB EC2 REST.

Answer C wrong because ElastiCache/Elasticsearch storage/search.

Answer D correct because API Gateway Lambda serverless REST.

Answer E wrong because S3/CloudFront static.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/welcome.html

37.Explain
Answer A correct because /tmp ephemeral temp.

Answer B wrong because EFS persistent slow.

Answer C wrong because EBS block.

Answer D wrong because S3 object latency.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html

38.Explain
Answer A wrong because EC2 NoSQL self.

Answer B correct because ElastiCache cache-aside profiles.

Answer C wrong because RDS write-through relational.

Answer D wrong because write-through populates, cache-aside reads better.

link ref: https://aws.amazon.com/elasticache/

39.Explain
Answer A wrong because VM Import migration self.

Answer B wrong because Lightsail simple not dynamic.

Answer C correct because Beanstalk manages deployment.

Answer D wrong because S3/CloudFront static.

link ref: https://aws.amazon.com/elasticbeanstalk/

40.Explain
Answer A correct because context request ID logging.

Answer B wrong because event no ID, file less reliable.

Answer C wrong because event ID console standard.

Answer D wrong because context ID file less.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/nodejs-logging.html

41.Explain
Answer A correct The EC2 instances in Account B are running under a specific EC2 Instance Profile Role. This role must have an IAM policy attached that explicitly allows the sts:AssumeRole action against the ARN of the AccessPII role in Account A. This is the identity-based policy that grants the EC2 instance the ability to make the request.

Answer B wrong because This is insufficient. The PII table resides in Account A. The EC2 role in Account B cannot directly access resources in Account A unless a Resource-Based Policy is attached to the DynamoDB table itself, which is an alternative method not described in the setup steps provided. In the IAM role delegation model, the EC2 role only needs sts:AssumeRole permission.

Answer C wrong because The EC2 role's credentials are automatically provided to the application, but they only grant access to Account B resources and the sts:AssumeRole action. They are not the credentials needed to access the DynamoDB table in Account A.

Answer D correct 
-The application code running on the EC2 instance must use the AWS SDK to call the AssumeRole operation from the AWS Security Token Service (STS). This call uses the EC2 role's credentials to assume the target role (AccessPII in Account A).

-The successful AssumeRole response returns a set of temporary credentials (Access Key ID, Secret Access Key, and Session Token) which are then used by the application code to create a DynamoDB client object for Account A, allowing access to the PII table.

Answer E wrong because This operation is used to get temporary credentials for an IAM user in the same account. It is not used for cross-account role delegation.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html

42.Explain
Answer A wrong because event source no trace API.

Answer B wrong because ALB logs no DynamoDB.

Answer C wrong because limit invocations no timing.

Answer D correct because X-Ray traces timings.

link ref: https://aws.amazon.com/xray/

43.Explain
Answer A wrong because Multi-AZ is a High Availability (HA) solution for disaster recovery. The standby instance does not serve read traffic. It uses synchronous replication, meaning the intensive write traffic on the primary instance will still cause performance degradation and contention with read traffic.
-> No performance benefit for read traffic.

Answer B correct because RDS Read Replicas are designed to offload read traffic from the primary DB instance. By directing constant read traffic to the replica, the Primary DB instance is free to handle the intensive daily write traffic (the historical data update) without competition, thus eliminating the read performance impact on application users.
-> Eliminates impact by isolating read and write operations.

Answer C wrong because ElastiCache is an in-memory caching service used to offload read traffic. It has no functionality to buffer or absorb write traffic to the underlying database
-> Irrelevant to write traffic buffering.

Answer D wrong because DynamoDB is a NoSQL database. While switching database types might be an option, it requires a complete migration and re-coding of the application. More importantly, it does not solve the immediate problem of isolating read contention, which is solved much more simply and efficiently with an RDS Read Replica.
-> Overkill/Incorrect. Requires re-coding and is not the most efficient fix.

link ref: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html

44.Explain
Answer A wrong because BatchWriteItem no transactional.

Answer B correct because TransactWriteItems all-or-nothing.

Answer C wrong because SQS messaging.

Answer D wrong because Aurora sync complex.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html

45.Explain
Answer A wrong because Inefficient and Overly Complex. This adds unnecessary latency, complexity, and cost (S3 and Lambda execution) to a simple, direct operation.

Answer B wrong because Overkill and Expensive. Kinesis is for high-throughput stream processing and persistence. Using it just to pipe a metric to CloudWatch is inefficient and introduces several unnecessary layers.

Answer C wrong because Major Security Risk. Storing static AWS credentials (access keys/secrets) directly on an EC2 instance is a security anti-pattern. If the instance is compromised, the keys are exposed.

Answer D correct because PutMetricData is the native API for custom metrics. Launching the EC2 instance with an IAM Instance Profile Role grants the application the necessary, frequently rotated temporary credentials automatically. This is secure and requires no static keys on the instance.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html

46.Explain
Answer A wrong While Amazon S3 is used, the standard **Amazon SQS CLI** (or standard SDKs) does not automatically handle the logic for sending and retrieving large payloads stored in S3.

Answer B correct because Extended Library S3 >256KB. The **Amazon SQS Extended Client Library for Java** (or similar pattern in other languages) is specifically designed to handle large SQS messages by:
    1. **Sending:** Automatically uploading the message payload (1GB) to **Amazon S3**.
    2. **Queuing:** Sending a small SQS message that contains a pointer (reference) to the S3 object.
    3. **Consuming:** When the consumer receives the small SQS message, the library automatically downloads the large payload from S3.
    This pattern allows SQS to manage the message delivery while leveraging S3 for high-capacity storage.

Answer C wrong Amazon EBS (Elastic Block Store) is block storage attached to an EC2 instance; it is not suitable for storing transient large payloads referenced by a serverless queue.

Answer D wrong Amazon EFS (Elastic File System) is file storage; it is also not suitable for storing transient large payloads referenced by SQS messages.

link ref: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-s3-messages.html

47.Explain
Answer A wrong because X-Ray is designed to map application components, analyze latency, and trace requests. It is not designed for generic time-series resource utilization metrics like thread count.

Answer B correct because CloudWatch is the dedicated monitoring service. Custom Metrics are explicitly for application-specific data. The PutMetricData API is the simplest and most direct method for the application to push its current thread count to CloudWatch, which then automatically provides graphing and dashboarding capabilities.

Answer C wrong because This introduces unnecessary complexity and cost by chaining three services (S3 for storage, Kinesis for streaming processing, plus a custom dashboard layer) when CloudWatch provides a direct API for ingestion and visualization. It is highly inefficient.

Answer D wrong because This requires the developer to build and maintain a custom web application (using a rendering library like D3.js) to query the DynamoDB data and draw the graph. This is far more complex and less efficient than using the native CloudWatch dashboard.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html

48.Explain
Answer A correct because package modules reduce size cold start.
Reduces Deployment Package Size: A smaller deployment package size reduces the time required for Lambda to download, unpack, and initialize the environment during a cold start. This directly translates to lower latency.

Answer B wrong because Violates "without increasing the cost" constraint.
Increases Cost. While DynamoDB might offer better performance, migrating to a different database system involves significant development effort and could potentially increase the overall cost, violating the constraint.

Answer C correct because connection outside handler reuse. Significantly Reduces Latency for warm invocations.
Optimizes Warm Starts: Placing initialization code (like connecting to the RDS database) outside the lambda_handler function allows it to run only once during the container's lifecycle (cold start). Subsequent invocations on the same warm container (warm start) reuse the existing connection, avoiding the 1-second overhead of establishing a new database connection every time.

Answer D wrong because Increased Complexity/Overhead.
Increased Complexity and Cost. While pooling is good practice, implementing a custom, reliable pooling solution within a serverless function adds complexity and management overhead. The simplest, most effective step is reusing the existing connection outside the handler.

Answer E wrong because Increased Cost.
Increased Cost. "Local caching" usually implies using an in-memory solution like Amazon ElastiCache, which is a separate, managed service and incurs additional cost, violating the constraint. If "local" means in-memory variables, the benefit is already covered by optimizing the initialization outside the handler.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html

49.Explain
Answer A wrong because While publishing a custom metric is correct for monitoring third-party errors (as AWS does not provide default metrics for external APIs), Amazon SES is a bulk email sending service, not a CloudWatch Alarm action.

Answer B wrong because CloudWatch API-error metrics are specific to AWS Service APIs (e.g., Lambda, DynamoDB, S3) and do not automatically track errors from third-party services called by the application code.

Answer C wrong because This fails on both counts: CloudWatch does not automatically track the third-party API errors, and SES cannot be directly triggered by a CloudWatch Alarm.

Answer D correct because The developer must instrument the code to call PutMetricData and publish a custom metric (e.g., ThirdPartyApiErrorCount). A CloudWatch Alarm then monitors this custom metric and triggers an Amazon SNS Topic when the threshold is crossed. SNS, being a general-purpose pub/sub service, integrates directly with CloudWatch Alarms and can deliver the notification via email, SMS, or other endpoints.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html

50.Explain
Answer A wrong because multiple pipelines complex.

Answer B correct because approval pauses manual.

Answer C wrong because disable transition manual.

Answer D wrong because disable stage no workflow.

link ref: https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html

51.Explain
Answer A correct because Redis Cluster HA uptime.

Answer B wrong because EC2 Redis self uptime.

Answer C wrong because Memcached no persistence.

Answer D wrong because Redshift analytics.

link ref: https://aws.amazon.com/elasticache/redis/

52.Explain
Answer A correct because Lambda allocates CPU proportionally to memory. EC2 test with 1GB RAM took 500s. Default Lambda memory (512MB or less) provides ~half the CPU â†’ ~1,000s+ execution â†’ exceeds 900s max timeout â†’ fails silently. Increasing to 1GB+ matches EC2 performance and completes within limit.

Answer B wrong because cross-bucket copy is valid and common.

Answer C wrong because max timeout is 900s (15 min); 500s fits if CPU is sufficient.

Answer D wrong because Java runtime is fully supported; EC2 proves logic works.
Root cause: Insufficient memory â†’ insufficient CPU â†’ execution too slow â†’ hits 900s limit.
Fix: Increase memory to â‰¥1GB (e.g., 1536MB) to reduce runtime <900s.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html

53.Explain
Answer A wrong because SSL/TLS encrypts data in transit (between the client and Kinesis service), which is important but does not protect the data while it is stored in the Kinesis stream (data at rest).

Answer B wrong because The Kinesis Client Library (KCL) is used by consumer applications to simplify reading and processing data from shards. It is a client-side library and does not control the server-side encryption of data at rest within the Kinesis service.

Answer C wrong because The data is already "at rest" in the stream as soon as it's written. A Lambda function can only process data after it's been retrieved from the stream. This would require the developer to implement client-side encryption before writing the data, which is less efficient than using the native SSE feature, or it attempts an impossible operation (encrypting data already stored by the service).

Answer D correct because The question asks specifically about encryption at rest for data stored inside Amazon Kinesis Streams.
Kinesis supports this directly through server-side encryption (SSE) using AWS KMS-managed keys.
When SSE is enabled:
- Data records are encrypted before being written to storage in the stream.
- Encrypted data remains protected for the full retention period (up to 7 days or extended retention).
- Decryption is handled transparently when consumers read the records.
- This is the most secure, simplest, and lowest-maintenance option, requiring no application changes.

link ref: https://docs.aws.amazon.com/streams/latest/dev/server-side-encryption.html

54.Explain
Answer A wrong because Lambda is compute only. It does not provide authentication or SAML federation functionality.

Answer B correct 
The requirement is to allow customers to sign up and authenticate in a mobile app.

It must integrate with the organizationâ€™s existing SAML 2.0 identity provider.

Needs to be scalable and with a limited budget.

ðŸ‘‰ Amazon Cognito User Pools support:

SAML 2.0 federation

OAuth / OpenID Connect

Social logins (Google, Facebook, etc.)

Built-in sign-up / sign-in / token issuance

Fully managed and low cost

This makes Cognito the best choice.

Answer C wrong because IAM is for workforce access and AWS resource permissions â€” not designed for customer authentication in mobile apps. Also IAM doesnâ€™t support user self-sign-up.

Answer D wrong because EC2 could host your own authentication service, but it does not provide authentication features by itself and would be expensive and not scalable automatically.

link ref: https://aws.amazon.com/cognito/

55.Explain
Answer A wrong because SNS notification.

Answer B wrong because NetworkIn bytes.

Answer C wrong because CloudFront CDN.

Answer D correct because custom metric users scaling.

link ref: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html

56.Explain
Answer A wrong because API manual.

Answer B wrong because standard CloudFormation Lambda.

Answer C correct because SAM simplifies serverless.

Answer D wrong because bash no automated.

link ref: https://aws.amazon.com/serverless/sam/

57.Explain
Answer A wrong because encryption no restrict.

Answer B correct because item-level primary key conditions.

Answer C wrong because SQS complexity.

Answer D wrong because client discard inefficient.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html

58.Explain
Answer A wrong because STS temp no guest.

Answer B wrong because Directory AD.

Answer C correct because Cognito unauth roles guest.

Answer D wrong because SAML federated.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html

59.Explain
Answer A wrong because The AWS CLI does not have a cloudformation compile command. CloudFormation templates are configuration files, not executable code that requires compilation in this context.

Answer B correct because The package command does three essential things for serverless applications: 
1) Zips the local code (store.py) 
2) Uploads the zip file to a specified S3 bucket
3) Modifies the template's CodeUri property (which is implicitly required but missing from the example) to point to the newly uploaded S3 location. The modified template is then ready for the aws cloudformation deploy command.

Answer C wrong because The template must remain separate from the code zip. The template itself needs to be modified by the package command to contain the S3 reference.

Answer D wrong because Embedding a source file directly into a template is generally not possible or practical, especially for larger files. Furthermore, the command is aws cloudformation package, not aws serverless create-package.

link ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-cli-package.html

60.Explain
Answer A wrong because unzip size no.

Answer B wrong because compression no unzip.

Answer C correct because smaller functions size.

Answer D wrong because double zip no.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html

61.Explain
Answer A correct because DynamoDB persistent sessions.

Answer B wrong because SQS queue.

Answer C wrong because local no across.

Answer D wrong because SQLite no.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html

62.Explain
Answer A wrong because Global Secondary Indexes (GSIs) are used to support different query patterns. Adding one does not increase the provisioned throughput capacity of the base table for the existing query, and creating an index would actually consume more WCU to propagate changes from the base table.

Answer B correct 
When an application receives a ProvisionedThroughputExceededException, it means the request rate for a specific partition or the entire table is momentarily exceeding the allocated Read Capacity Units (RCUs) or Write Capacity Units (WCUs).
The recommended and most effective way to handle this in your application code is to implement exponential backoff and retry logic.
  - Exponential Backoff is a standard network error-handling strategy where a client progressively waits longer between retries of a failed request. For example, the first retry might wait for 50ms, the next for 100ms, then 200ms, and so on.
  - Why it works: This mechanism significantly reduces the likelihood of the repeated requests immediately causing more throttling. It gives DynamoDB's adaptive capacity a chance to reallocate resources or simply allows the temporary spike in traffic to subside.

Answer C wrong because Retrying immediately will only guarantee that the second request is also throttled, as the load on the table hasn't changed. This makes the problem worse and can lead to a sustained throttling loop.

Answer D wrong because The UpdateItem API is for modifying data within an item (e.g., updating a bicycle's price), not for changing the table's capacity setting. While you could manually increase the provisioned capacity (or switch to On-Demand mode) using the UpdateTable API to solve the root cause of recurrent throttling, the most immediate and standard way to handle the exception in the application code is with retries and backoff.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html

63.Explain
Answer A wrong because immutable replacement.

Answer B wrong because rolling reduces.

Answer C wrong because all at once down.

Answer D correct because rolling batch capacity cost.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html

64.Explain
Answer A wrong because legibility no performance.

Answer B correct because connection reuse cold start.

Answer C wrong because error unrelated.

Answer D wrong because new instance per invocation bad reuse.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html

65.Explain
Answer A wrong because Data Pipeline ETL.

Answer B wrong because SNS/SQS messaging.

Answer C wrong because EMR big data.

Answer D correct because Step Functions state.

link ref: https://aws.amazon.com/step-functions/

66.Explain
Answer A wrong because single profile no.

Answer B wrong because roles per service task no.

Answer C wrong because group cluster no per task.

Answer D correct because roles per task least.

link ref: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html

67.Explain
Answer A correct
The process requires orchestration, parallel vendor requests, and can take up to a week.

AWS Step Functions support:

Long-running workflows (up to 1 year runtime)

Built-in parallel execution

Automatic retries and error handling

Waiting/sleeping without cost

Ability to join results from multiple branches

This makes Step Functions the simplest and most efficient solution for long-running and multi-step logic.

Answer B wrong because Complex architecture: requires EC2 workers, polling, custom state tracking, and result correlation. Long-running orchestration becomes hard to manage.

Answer C wrong because Lambda has a maximum execution duration of 15 minutes, but the workflow can take a week, so this fails. Also complex to coordinate async joins.

Answer D wrong because CloudWatch Events (EventBridge) can trigger functions, but it is not designed for long-running stateful workflows. No built-in parallel join or long-duration state tracking.

link ref: https://aws.amazon.com/step-functions/

68.Explain
Answer A wrong because String scan inefficient.

Answer B wrong because container scan costly.

Answer C wrong because Date GSI minute over.

Answer D correct because TTL Number auto-delete cost-free.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/time-to-live-ttl-how-to.html

69.Explain
Answer A wrong because Not the first recommended action. Retry with exponential backoff should be implemented before requesting a quota increase.

Answer B wrong because Does not fix throttling caused by an application making too many API calls programmatically.

Answer C wrong because Removing the API call does not solve throttling â€” the application likely needs the data.

Answer D correct
The developer receives intermittent HTTP 400 â€“ ThrottlingException errors when calling the CloudWatch API.
This means the application is exceeding the allowed API request rate.

The AWS best practice for throttling errors is:
ðŸ” Retry the API request using exponential backoff (and optionally jitter)
Exponential backoff helps prevent repeatedly hammering the API by increasing wait time between retries.

link ref: https://docs.aws.amazon.com/general/latest/gr/api-retries.html

70.Explain
Answer A wrong because SNS SQS no concurrent.

Answer B wrong because FIFO ordered cost no concurrent.

Answer C wrong because Firehose delivery no processing.

Answer D correct because Streams multiple consumers sharded cost.

link ref: https://aws.amazon.com/kinesis/data-streams/

71.Explain
Answer A correct The **`appspec.yml`** file is the configuration file used by **AWS CodeDeploy** to manage a deployment. It must be placed in the **root** of the application source code bundle (ZIP, JAR, or folder) that is registered as the deployment revision. CodeDeploy agent expects to find this file in the root when the deployment bundle is downloaded.

Answer B wrong because bin binaries. Placing it in a sub-folder like `bin` will prevent the CodeDeploy agent from finding and using the deployment configuration.

Answer C wrong because S3 no structure. The source code bundle containing the `appspec.yml` file is uploaded to an S3 bucket or GitHub/CodeCommit, but the file itself must be inside the application source structure.

Answer D wrong because config separate. While it may be logically grouped with config files, CodeDeploy requires it to be at the **root** of the deployment bundle structure, not just any folder.

link ref: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html

72.Explain
Answer A correct because BEST FIT. Specifically designed for high-throughput, serverless data streaming into S3, handling buffering, compression, and error retry logic automatically.
-> Managed data ingestion and delivery service. Automatically captures, transforms, and loads high-volume streaming data into destinations like Amazon S3, Redshift, or OpenSearch Service.

Answer B wrong because This is a network feature to speed up uploads, not a service for orchestrating ingestion from many sources or handling streaming data transformation/delivery.
-> Network optimization service. Speeds up transfers to S3 over long distances using CloudFront edge locations.

Answer C wrong because SQS is designed for queuing individual messages (up to 256 KB). It is not optimized for continuous, very high-throughput data streams and would require custom application code to poll the queue, aggregate messages, and write files to S3.
-> Message Queuing Service. Decouples components using asynchronous messages.

Answer D wrong because SNS is a fan-out notification service, not a data ingestion or storage delivery pipeline. It is unsuitable for handling the continuous flow of application data that needs to be aggregated and stored in S3.
-> Notification Service. Publishes messages to multiple subscribers (e.g., SQS, Lambda, email).

link ref: https://aws.amazon.com/kinesis/data-firehose/

73.Explain
Answer A wrong because install runtime time.

Answer B wrong because package libs.

Answer C wrong because S3 reference download.

Answer D correct because Layers share libs time.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html

74.Explain
Answer A correct because parallel scans rate minimize time overload.

Answer B wrong because sequential slower.

Answer C wrong because RCU during scan temporary.

Answer D wrong because eventual accuracy no optimization.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html

75.Explain
Answer A correct because CloudFront caches S3 global.

Answer B wrong because replication durability.

Answer C wrong because logs deletion no.

Answer D wrong because lifecycle management.

Answer E correct because random prefixes distribute.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html

76.Explain
Answer A correct because API Gateway JSON to XML SOAP.

Answer B wrong because ALB JSON no transform.

Answer C wrong because ALB XML no JSON input.

Answer D wrong because transform XML no input JSON.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-mapping-template-reference.html

77.Explain
Answer A wrong because S3 encryption no rate.

Answer B correct because KMS API limits latency.

Answer C wrong because client algo no.

Answer D wrong because alias optional.

link ref: https://docs.aws.amazon.com/kms/latest/developerguide/limits.html

78.Explain
Answer A correct because rolling batches minimal outage existing.

Answer B wrong because all at once outage.

Answer C wrong because additional batch adds.

Answer D wrong because immutable replaces.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html

79.Explain
Answer A wrong because reverse no FIFO.

Answer B wrong because exact order shard.

Answer C correct because FIFO shard no across.

Answer D wrong because no options getRecords.

link ref: https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-get-records.html

80.Explain
Answer A wrong because Lambda reactive no prevent.

Answer B correct because policy prevent no encryption header.

Answer C wrong because Events after no prevent.

Answer D wrong because prevent with header no require.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html

81.Explain
Answer A correct because unique MessageGroupId order per sender.

Answer B wrong because dedup duplicates no order.

Answer C wrong because message level group FIFO.

Answer D wrong because content dedup no order.

link ref: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/using-messagegroupid-property.html

82.Explain
Answer A wrong because KMS encryption.

Answer B correct because Cognito auth cross-platform preferences.

Answer C wrong because Directory AD.

Answer D wrong because IAM internal.

link ref: https://aws.amazon.com/cognito/

83.Explain
Answer A wrong because tags no env vars.

Answer B wrong because hardcore no flexible.

Answer C correct because env vars inject per env.

Answer D wrong because separate duplicate.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/configuration-envvars.html

84.Explain
Answer A wrong because Cognito app auth.

Answer B wrong because keys mail insecure.

Answer C correct because AssumeRole temp cross-account.

Answer D wrong because SSH no AWS API.

link ref: https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html

85.Explain
Answer A wrong because SDK annotation app.

Answer B correct because daemon EC2 traces.

Answer C wrong because daemon CloudWatch no X-Ray.

Answer D wrong because SDK code daemon EC2.

link ref: https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html

86.Explain
Answer A wrong because Storing long-term credentials in any repository (even private) is a security risk. If the repository is compromised, the keys are compromised.

Answer B wrong because Injecting long-term credentials directly into user data exposes them to anyone who can view the instance metadata or configuration. This is also a significant security risk.

Answer C wrong because CloudWatch metrics do not have resource-based policies that allow granting permissions to specific EC2 instances directly. IAM policies (attached via a role) must be used for authentication and authorization.

Answer D correct because This method uses IAM Roles for EC2 Instances (Instance Profiles). This grants temporary credentials to the running application, eliminating the need to store or manage long-term access keys. This is the AWS security best practice for applications running on EC2.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html

87.Explain
Answer A wrong because starRating uneven.

Answer B correct because reviewID unique even.

Answer C wrong because comment poor hashing.

Answer D wrong because productID hot popular.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html

88.Explain
Answer A correct because condition ${aws:username}.

Answer B wrong because principal who.

Answer C wrong because variables ${aws:username}.

Answer D wrong because resource what.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_variables.html

89.Explain
Answer A wrong because SSE-S3 AWS key.

Answer B correct because SSE-KMS own CMK AWS.

Answer C wrong because client-side own.

Answer D wrong because roles/policies access no encrypt.

link ref: https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html

90.Explain
Answer A wrong The AWS CLI command to retrieve IAM group information (e.g., aws iam get-group) applies only to IAM users, not IAM roles attached to EC2 instances. IAM roles do not belong to IAM groups, and retrieving group details does not help verify specific access to Kinesis Streams (such as the GetRecords action). This is not directly relevant to checking Kinesis permissions.

Answer B wrong The EC2 instance metadata service (accessed via http://169.254.169.254/latest/meta-data/iam/) provides details like the IAM role name, ARN, and temporary credentials, but it does not provide inline policy details or attached policies. To view policies, you'd need to call IAM APIs (e.g., get-role-policy), which requires separate IAM permissions and does not directly verify access for the GetRecords action on Kinesis. It only identifies the role, not specific permissions.

Answer C wrong AWS STS (Security Token Service) is used to request temporary tokens (e.g., via AssumeRole), but on an EC2 instance with an IAM role, tokens are automatically provided via metadata without needing STS calls. Performing a "describe action" (e.g., DescribeStream on Kinesis) might check permissions for descriptive actions, but it is not specific to GetRecords and could still have minor impacts (e.g., logging). It's not the standard way to simulate or verify permissions without execution.

Answer D correct The --dry-run parameter in the AWS CLI (e.g., aws kinesis get-records --shard-iterator <shard-iterator> --dry-run) simulates the GetRecords action without actually retrieving data from Kinesis Streams. If the IAM permissions allow it, the CLI succeeds; otherwise, it returns "AccessDenied." This is an efficient way to verify permissions from the EC2 instance without real impact.

Answer E correct The IAM Policy Simulator (available in the AWS Management Console or via API) allows simulation of the IAM role's policies for specific actions like kinesis:GetRecords, resources (Kinesis stream ARN), and context (EC2 instance). It clearly shows whether permissions are allowed or denied, making it a safe way to verify without executing the actual action. This is AWS's official tool for testing IAM policies.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_testing.html

91.Explain
Answer A correct because CodeCommit pipeline SNS failures.

Answer B wrong because GitHub SES no AWS.

Answer C wrong because GitHub CloudWatch no pipeline.

Answer D wrong because CodeCommit CloudWatch no SNS.

link ref: https://aws.amazon.com/codepipeline/

92.Explain
Answer A correct because ChangeMessageVisibility increase timeout process delete.

Answer B wrong because DeleteQueue entire queue.

Answer C wrong because decrease timeout visible sooner re-process.

Answer D wrong because no DeleteMessageVisibility API.

link ref: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html

93.Explain
Answer A wrong because DynamoDB/SNS manual MFA.

Answer B correct because Cognito native MFA.

Answer C wrong because Directory AD no MFA app.

Answer D wrong because IAM MFA console/CLI.

link ref: https://aws.amazon.com/cognito/

94.Explain
Answer A correct because eventual 5 RCU 4KB max throughput.

Answer B wrong because strong 10 RCU.

Answer C wrong because 15 RCU 1KB less data.

Answer D wrong because strong more RCU.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html

95.Explain
Answer A wrong because Versioning multiplies storage, but the initial data volume was only 100 KB. It is impossible for 100 KB of data, even versioned, to grow into 50 GB in a few days.

Answer B wrong because Replication copies data to another bucket; it does not explain the origin of the 50 GB of data in the source bucket itself.

Answer C correct because s3://mycoolapp to deliver its access logs to a folder within itself (s3://mycoolapp/logs). Every time a log file is written, that write operation is logged, which triggers another write, which is logged, creating a recursive log storm that consumes massive storage capacity very quickly.

Answer D wrong because A lifecycle policy only changes the storage class of existing objects. It does not create new data or multiply the size of the bucket's contents.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html

96.Explain
Answer A correct The Task IAM Role grants permissions directly to the running container, providing temporary, frequently rotated credentials via the ECS task metadata service. The AWS SDK automatically finds and uses these temporary credentials, eliminating the need to store static keys.

Answer B wrong because AssumeRole instance complex. While technically secure, this is overly complex. The AssumeRole logic is meant for cross-account access or environments without direct task roles. Task IAM Roles handle the assumption process automatically and natively, requiring no changes to the application's AWS SDK code.

Answer C wrong because This is a major security anti-pattern for production environments. If the container or its logs are compromised, the permanent keys are exposed, leading to a high-risk security incident. The best practice is to never store long-lived credentials on a compute resource.

Answer D wrong because Similar to environment variables, storing a static credentials file within the container or on the host is a critical security risk. It requires manual rotation and fails to leverage AWS's temporary credential mechanism.

link ref: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html

97.Explain
Answer A wrong because buildspec future no current.

Answer B wrong because X-Ray tracing no build logs.

Answer C correct because history logs failed phases.

Answer D wrong because local no AWS env.

link ref: https://docs.aws.amazon.com/codebuild/latest/userguide/view-build-details.html

98.Explain
Answer A wrong because order not true.

Answer B correct because in-place stop before after start.

Answer C wrong because validate after.

Answer D wrong because validate no order.

link ref: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html

99.Explain
Answer A wrong because KMS data.

Answer B correct because STS decode IAM errors.

Answer C wrong because open source no AWS format.

Answer D wrong because IAM no API.

link ref: https://docs.aws.amazon.com/STS/latest/APIReference/API_DecodeAuthorizationMessage.html

100.Explain
Answer A wrong because S3 signed manual.

Answer B wrong because metadata insecure.

Answer C wrong because DynamoDB client complex.

Answer D correct because Parameter Store secure effort.

link ref: https://aws.amazon.com/systems-manager/parameter-store/

101.Explain
Answer A wrong because Elastic Load Balancer logs capture load balancer-specific events like HTTP requests/responses, but not S3 API calls like DeleteBucket, which are service-level actions.

Answer B wrong because application logs in CloudWatch Logs would show app-generated errors (e.g., from SDK calls), but not the underlying AWS service events like bucket deletion; they might show the error but not the root cause event.

Answer C wrong because AWS X-Ray is for tracing application requests and performance, not for auditing or alarming on management API events like DeleteBucket; it doesn't capture CloudTrail-level audit logs.

Answer D correct because AWS CloudTrail records all API calls, including management actions like DeleteBucket, providing a chronological audit trail to identify who/when deleted the bucket, starting root cause analysis.

link ref: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-user-guide.html

102.Explain
Answer A wrong because specifying keys as parameters for each command is insecure (exposes in scripts/logs) and inefficient (manual per call), violating least privilege and credential management best practices.

Answer B correct because `aws configure` stores the access key ID and secret access key in the CLI config file (~/.aws/credentials), allowing the CLI to use IAM permissions automatically for all commands without per-command input.

Answer C wrong because IAM uses access keys, not username/password; passwords are for console federation, and passing them is insecure and unsupported for CLI.

Answer D wrong because IAM roles are for AWS services/instances (e.g., EC2), not local CLI on development servers; CLI requires configured credentials.

link ref: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html

103.Explain
Answer A correct because Lambda automatically scales out by provisioning additional execution environments concurrently to handle event bursts like S3 notifications, ensuring high availability without sequential queuing.

Answer B wrong because Lambda processes invocations asynchronously and concurrently by default; sequential handling would require custom queuing, which isn't the case for S3 events.

Answer C wrong because S3 event notifications trigger one invocation per event (per image), not batching multiple into a single execution unless configured via batch size in event source mapping (not applicable here).

Answer D wrong because Lambda doesn't dynamically add compute per execution; memory allocation sets CPU proportionally, but scaling is horizontal (more instances), not vertical per invocation.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/lambda-scaling.html

104.Explain
Answer A wrong because Global Secondary Indexes (GSIs) add query flexibility but don't reduce read latency or provisioning needs; they consume additional capacity and may increase costs.

Answer B wrong because S3 is object storage with higher latency (ms to s) unsuitable for sub-ms trading; Transfer Acceleration speeds global uploads, not low-latency reads.

Answer C wrong because retries with backoff mitigate throttling but don't address root latency from data retrieval or over-provisioning costs during spikes.

Answer D correct because DynamoDB Accelerator (DAX) is an in-memory cache that provides sub-ms reads for frequently accessed data, reducing direct DynamoDB calls and allowing lower provisioned capacity while handling spikes via caching.

link ref: https://aws.amazon.com/dynamodb/dax/

105.Explain
Answer A wrong because Lambda automatically generates logs for invocations (e.g., start/end times) even without explicit statements if the runtime supports it; explicit logs (console.log) add detail but aren't required for basic logging.

Answer B wrong because CloudWatch Logs is a destination, not a source; Lambda pushes logs without needing a trigger.

Answer C correct because the execution role requires logs:CreateLogGroup, logs:CreateLogStream, and logs:PutLogEvents permissions to write to CloudWatch Logs; missing these prevents log generation.

Answer D wrong because Lambda auto-creates log groups/streams per function if permissions allow; no manual target needed.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html

106.Explain
Answer A wrong because view access is separate (via IAM policies like xray:Get*); if traces reach X-Ray, the issue is sending, not viewing.

Answer B correct because the X-Ray daemon must run on EC2 to collect/send traces from the host; without it, app instrumentation (SDK) can't forward traces.

Answer C wrong because endpoint misconfig would affect local testing too if same code; assumes correct in app but deployment-specific issue.

Answer D wrong because BatchGetTraces/GetTraceGraph are for querying traces (viewing), not sending; the problem is traces not available (not sent).

Answer E correct because the instance role needs xray:PutTraceSegments (send traces) and xray:PutTelemetryRecords (send metadata) to allow the daemon to write to X-Ray.

link ref: https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon.html

107.Explain
Answer A wrong because DynamoDB stores data but doesn't generate unique cross-device identities; requires custom logic for uniqueness.

Answer B wrong because IAM access key IDs are for AWS access, not app user identities; storing without secrets is insecure and not scalable.

Answer C correct because Cognito developer-authenticated identities provide unique, persistent IDs for users across devices, with temporary AWS credentials for access.

Answer D wrong because IAM users/roles are for AWS services, not app end-users; resource IDs aren't suitable for app logic.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html

108.Explain
Answer A wrong because get-template/execute-change-set is for existing stacks, not launching new templatized serverless apps.
Missing initial deployment and packaging steps. get-template retrieves an existing template. execute-change-set applies a previously created change set. This sequence is for inspecting and then applying changes to an existing stack, not for the initial deployment of a new serverless application.

Answer B wrong because validate/create-change-set prepares but doesn't deploy/package serverless specifics. validate-template only checks the syntax of the template. create-change-set generates a plan of changes without applying them. This is part of a non-destructive update workflow, but it skips the critical packaging step needed for serverless artifacts (like Lambda code) and the final deployment step.

Answer C correct 
This uses the AWS Serverless Application Model (SAM) CLI commands, which are extensions of the AWS CLI specifically designed for serverless applications.
AWS cloudformation package prepares the SAM template by zipping Lambda code, processing local files (like code or dependencies), and uploading them to an Amazon S3 bucket. aws cloudformation deploy then takes the packaged template (now pointing to the S3 artifacts) and creates or updates the underlying CloudFormation stack to launch the serverless application.

Answer D wrong because create/update-stack assumes template is already packaged; skips bundling step. 
Create-stack is the classic CloudFormation command for launching a stack. However, it cannot automatically upload artifacts (like Lambda code) to S3, which is required for a templatized serverless application. It is functionally replaced and optimized for serverless deployments by the deploy command after packaging.

link ref: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html

109.Explain
Answer A wrong because inline policies are embedded in the role and evaluated with it; wouldn't bypass restrictions.

Answer B correct because managed policies can be attached to multiple principals; if a broader managed policy is attached (overriding restrictive), it allows access despite role assumption.

Answer C wrong because CLI corruption would affect all commands, not just S3 writes.

Answer D wrong because credential chain prefers env vars first (used here), then shared credentials, instance profile last; but env vars take precedence, bypassing role.

link ref: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-precedence.html

110.Explain
Answer A correct because S3 PUT overwrites are eventually consistent in the same partition; reads may see old version until propagated (seconds).

Answer B wrong because metadata labeling doesn't affect consistency; versioning would, but not mentioned.

Answer C wrong because new object PUTs are immediately consistent, only overwrites/modifies are eventual.

Answer D wrong because no explicit "latest" parameter; consistency model dictates behavior.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html

111.Explain
Answer A wrong because KMS Encrypt is for small payloads (<4KB); videos are large, requiring data keys.

Answer B wrong because custom library keys aren't AWS-managed; lacks integration/rotation.

Answer C correct because GenerateDataKey provides a per-file data key for envelope encryption; encrypt video with data key, store encrypted key/data for large payloads.

Answer D wrong because SSE-KMS is server-side after upload, not app-level prior encryption.

link ref: https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html

112.Explain
Answer A wrong because CloudTrail logs API calls, not async invocation details/failures.

Answer B correct because Dead Letter Queues (DLQ) capture unprocessed events after retries; SQS DLQ allows inspection of failed messages.

Answer C wrong because SWF is workflow orchestration, not for Lambda async failures.

Answer D wrong because Config tracks resource configs, not event processing.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html#dlq

113.Explain
Answer A wrong because stage throttling is global, not per-user; creating stages per user is high overhead.

Answer B wrong because CloudWatch filter/alarm/Lambda is custom/complex for throttling.

Answer C wrong because alarms deny but not per-user; user-specific alarms scale poorly.

Answer D correct because usage plans associate API keys with rate/burst limits; custom plans per package with low management.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html

114.Explain
Answer A wrong because SQS queues messages but doesn't orchestrate sequence/parallel.

Answer B wrong because activities are for external tasks; Lambda integration is via states.

Answer C wrong because SNS fans out but no sequencing.

Answer D correct because Step Functions state machines define workflows with parallel/sequence branches for Lambda invocations.

link ref: https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html

115.Explain
Answer A wrong because ConsistentRead ensures fresh data but not atomic transactions across items.

Answer B wrong because Memcached lacks transaction support.

Answer C correct because Aurora MySQL supports ACID transactions within blocks for atomic updates.

Answer D correct because DynamoDB TransactWriteItems/TransactGetItems provide atomic, consistent operations.

Answer E wrong because Redshift is analytics, limited transaction support.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/transaction-apis.html

116.Explain
Answer A wrong because The Git CLI is not included in Lambda's runtime, requiring a custom Layer. Cloning the full repository consumes excessive time, memory, and disk space, violating serverless efficiency.

Answer B wrong because Direct API calls via cURL require the developer to manually implement AWS SigV4 cryptographic signing for authentication. This is complex, time-consuming, and error-prone.

Answer C correct API-Native Commit (PutFile): The PutFile API action (available through the AWS SDKs, e.g., put_file in Boto3) is designed to commit a single file directly to a repository without needing to clone the entire history. The Lambda function sends the file's binary content and metadata (commit message, author) directly to the CodeCommit service.

Efficiency: It consumes minimal resources, avoiding the heavy I/O, disk space usage, and latency associated with cloning a potentially large repository into Lambda's limited /tmp directory.

Security & Authentication: The AWS SDK automatically handles the cryptographic signing of the API request using the Lambda function's IAM Execution Role, making credential management simple and secure.

Answer D wrong because Introduces S3 and AWS Step Functions as unnecessary intermediaries. This adds complexity, cost, and latency to a task that can be completed in a single direct step from the Lambda function.

link ref: https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-migrate-repository.html

117.Explain
Answer A wrong because env vars long-lived insecure.

Answer B wrong because file insecure.

Answer C correct because instance profile rotated secure.

Answer D wrong because options expose.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html

118.Explain
Answer A wrong because keys code insecure.

Answer B wrong because policies users no scalable.

Answer C correct because Cognito auth/unauth roles.

Answer D wrong because IAM per user no.

Answer E correct because STS AssumeRole roles type.

link ref: https://aws.amazon.com/cognito/

119.Explain
Answer A wrong because This configuration controls network access (making resources private), but it does not handle user authentication (SAML) or fine-grained authorization (which user gets which data). It's a network solution, not an identity solution.

Answer B wrong because User Pools are primarily for authentication and can federate with SAML. However, using User Pool Groups only allows for coarse-grained access control (e.g., all "Managers" get access to a resource). It does not provide the necessary fine-grained, per-user restriction to access their own data only, which is the core requirement.

Answer C correct because identity SAML condition sub.
1. Amazon Cognito Identity Pool + SAML Federation
Federation: An Amazon Cognito identity pool is designed to allow users authenticated by external Identity Providers (IdPs) like the company's SAML employee directory to access AWS services. The identity pool handles the token exchange with the SAML provider.
No Mirroring: By using only the Identity Pool, the user directory and all employee details remain entirely within the legacy SAML system, meeting the requirement to avoid mirroring employee information on AWS.

2. Fine-Grained Access Control (Per-User)
- Temporary Credentials: After a user successfully authenticates via SAML, the Identity Pool issues them temporary AWS credentials (via an assumed IAM role).
- Unique Identity ID (sub): Every authenticated user is assigned a unique Identity ID (UUID) within the Identity Pool. This ID is exposed as the cognito-identity.amazonaws.com:sub variable in the IAM policy context.
- IAM Condition Key: The developer can attach a policy to the IAM role assumed by the user. This policy uses the cognito-identity.amazonaws.com:sub variable in its Condition block to dynamically grant access only to resources that are tagged or named with that same unique ID.
   + For S3 (images), the policy can restrict access to objects with a path structure like s3://bucket-name/${cognito-identity.amazonaws.com:sub}/*.
   + For RDS (application data), this is typically handled by passing the user's ID to the application backend, which then uses the ID to filter data queries (e.g., WHERE user_id = 'cognito-identity.amazonaws.com:sub').
   + This provides authorized access for each employee to their personal data only, without having to create individual roles for 25,000+ employees.

Answer D wrong because While this achieves per-user access control, creating and managing 25,000+ IAM roles is an unsustainable and inefficient administrative burden that does not scale well with a growing company. The dynamic policy variable approach is the scalable solution.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/iam-roles.html

120.Explain
Answer A wrong because zip Lambda direct.

Answer B wrong because X-Ray after.

Answer C correct because sam package bundles.

Answer D wrong because eb Beanstalk.

link ref: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-package.html

121.Explain
Answer A wrong because S3-managed at rest.

Answer B wrong because KMS AWS-managed.

Answer C wrong because client-side on-premises keys.

Answer D correct because SSE-C customer-provided keys.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html

122.Explain
Answer A wrong because folder check polling inefficient.

Answer B correct because S3 to SQS decouples, Lambda processes.

Answer C wrong because API Gateway direct synchronous.

Answer D wrong because Step for folder check polling.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html

123.Explain
Answer A wrong because pods Kubernetes.

Answer B wrong because tasks not share volume easily.

Answer C correct because one task multiple containers share volume.

Answer D wrong because pods not ECS.

link ref: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html#volume_definitions

124.Explain
Answer A correct because CloudWatch alarm on CPUUtilization to SNS.

Answer B wrong because CloudTrail no CPU metrics.

Answer C wrong because cron on instance manual.

Answer D wrong because Lambda on CloudTrail mismatch.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html

125.Explain
Answer A wrong because role for access, but creds in app.

Answer B correct because Secrets Manager stores/rotates creds.

Answer C wrong because S3 file insecure.

Answer D wrong because code creds insecure.

link ref: https://aws.amazon.com/secrets-manager/

126.Explain
Answer A wrong because all at once downtime.

Answer B wrong because rolling batch partial.

Answer C correct because new env swap no downtime.

Answer D wrong because rolling gradual downtime.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html

127.Explain
Answer A wrong because lifecycle deletes, no auth.

Answer B correct because presigned URL timed access.

Answer C wrong because SSE-KMS at rest.

Answer D wrong because policy change temporary but not per doc.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html

128.Explain
Answer A correct because ResultPath includes error in output.

Answer B wrong because InputPath null discards.

Answer C wrong because Retry includes error.

Answer D wrong because OutputPath $ full output.

link ref: https://docs.aws.amazon.com/step-functions/latest/dg/concepts-error-handling.html

129.Explain
Answer A wrong because IAM role on EC2 is for instance actions, not for client-side boto3 throttling.

Answer B correct because error `RequestLimitExceeded` indicates API rate limits exceeded; exponential backoff retries with increasing delays to respect limits.

Answer C wrong because network bandwidth not related to API throttling.

Answer D wrong because CLI version not cause of AWS service throttling.

link ref: https://docs.aws.amazon.com/ec2/latest/devguide/ec2-api-throttling.html
	      https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html

130.Explain
Answer A correct because `Export` in `Outputs` allows cross-stack reference via `Fn::ImportValue` in other templates.

Answer B wrong because `Exported: true` not valid; export via `Outputs` with `Export`.

Answer C wrong because custom resource overkill and complex.

Answer D wrong because `Fn::Include` embeds templates, not references resources.

link ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/walkthrough-crossstackref.html
	      https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/intrinsic-function-reference-importvalue.html

131.Explain
Answer A correct because AfterInstall post permissions.

Answer B wrong because DownloadBundle download.

Answer C wrong because BeforeInstall pre.

Answer D wrong because ValidateService after.

link ref: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html

132.Explain
Answer A wrong because Streams changes.

Answer B correct because DAX accelerates reads/writes.

Answer C wrong because global replication.

Answer D wrong because transactions consistency.

link ref: https://aws.amazon.com/dynamodb/dax/

133.Explain
Answer A correct because This is the modern, recommended approach. sam package prepares the template by uploading local assets (like Lambda code ZIPs or static assets) to an S3 bucket and replacing the local paths in the template with S3 URLs. sam deploy then uses that processed template to create or update the CloudFormation stack.

Answer B correct because This is the low-level, direct approach. aws cloudformation package performs the exact same asset uploading and template processing as sam package. aws cloudformation deploy (which is newer and preferred over create-stack/update-stack) performs the deployment. This confirms that the SAM CLI commands are syntactic sugar for the CloudFormation commands.

Answer C wrong because update-function-code only updates an existing Lambda function's code. It does not create the full application stack (IAM roles, API Gateway, S3 buckets, etc.) defined in the SAM template.

Answer D wrong because SAR is used for sharing and consuming pre-built applications, not for deploying a custom application from a local SAM template. The deployment process is different and intended for public sharing.

Answer E wrong because AWS SAM templates need a packaging step (sam package or aws cloudformation package) to replace local references to code/assets (e.g., CodeUri: build/app.zip) with the actual S3 locations before create-stack can be called. Uploading the template alone is insufficient.

link ref: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-deploy.html

134.Explain
Answer A correct because pool users.

Answer B wrong because SNS manual.

Answer C correct because enable MFA pool.

Answer D wrong because IAM teams.

Answer E wrong because IAM console.

link ref: https://aws.amazon.com/cognito/

135.Explain
Answer A wrong because If ECS task IAM support is disabled (false), tasks cannot use task roles, so this configuration will NOT work.

Answer B wrong because Permissions granted to the instance profile mean both microservices inherit ALL access â†’ not least privilege, too broad.

Answer C correct 
ECS_ENABLE_TASK_IAM_ROLE = true allows ECS tasks to assume their own IAM roles, instead of sharing the EC2 instance profile role.

Each microservice (each ECS task) can have a different IAM role â†’ Principle of least privilege.

Microservice 1 â†’ IAM role with read-only Aurora DB permissions

Microservice 2 â†’ IAM role with read-only DynamoDB permissions

Answer D wrong because Even with true, if access is given to the instance profile, both microservices can access both Aurora + DynamoDB â†’ not minimal privileges.

link ref: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html

136.Explain
Answer A wrong because CloudWatch metrics measure aggregate statistics (average duration, error count) and are not designed for granular, per-request timing of internal code sections. While you could technically manually calculate elapsed time and publish it as a custom metric, this is labor-intensive and only provides a single aggregated number, not the detailed trace timeline required to isolate an internal bottleneck.
- Metrics provide overall health, not deep, per-request performance tracing into code segments.

Answer B wrong because This confuses the services. The trace data (segments, subsegments, and detailed timing) is stored in and analyzed by the AWS X-Ray service. CloudWatch is for metrics and logs. The detailed timeline needed for bottleneck analysis is not available in the CloudWatch console.
- You trace with X-Ray but analyze in the X-Ray console, not CloudWatch.

Answer C correct because This approach leverages Distributed Tracing. The X-Ray SDK (for Java) allows the developer to add subsegments around specific sections of the code, external service calls (like DynamoDB or S3), or HTTP requests. The X-Ray console provides a Service Map and detailed Trace Timelines that visually show the time spent in each segment and subsegment, pinpointing the exact bottleneck (e.g., a slow database query or an inefficient calculation).
- X-Ray is specifically designed to analyze latency and isolate bottlenecks within distributed and microservices architectures like Lambda.

Answer D wrong because This also confuses the services. X-Ray can only analyze data that is provided via the X-Ray SDK/API (trace data). It cannot magically interpret raw timestamps written to the CloudWatch API.
- X-Ray cannot analyze raw CloudWatch metrics.

link ref: https://aws.amazon.com/xray/

137.Explain
Answer A correct because longer polling reduces calls.

Answer B wrong because scale no costs.

Answer C wrong because SNS push no polling.

Answer D wrong because FIFO costly.

link ref: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html

138.Explain
Answer A wrong because Trail logs.

Answer B correct because X-Ray end-to-end.

Answer C wrong because Logs execution.

Answer D wrong because Flow network.

link ref: https://aws.amazon.com/xray/

139.Explain
Answer A wrong because list no deny overrides.

Answer B wrong because one no.

Answer C wrong because full deny.

Answer D correct because explicit deny role overrides allow.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html

140.Explain
Answer A wrong because count no retain.

Answer B wrong because disable no retain.

Answer C wrong because age no retain.

Answer D correct because Retention keeps bundle.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.as-versions.html

141.Explain
Answer A correct because no invalidate stale.

Answer B wrong because write-through updates.

Answer C wrong because provision no.

Answer D wrong because write ok.

link ref: https://aws.amazon.com/elasticache/

142.Explain
Answer A wrong because KMS no CodeCommit.

Answer B correct because helper HTTPS.

Answer C wrong because Manager SSL.

Answer D wrong because CloudHSM custom.

link ref: https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-https-unixes.html

143.Explain
Answer A wrong because logs/S3 no X-Ray.

Answer B wrong because CloudWatch trigger no.

Answer C correct because role enable tracing.

Answer D wrong because daemon runtime.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/services-xray.html

144.Explain
Answer A wrong because trust DynamoDB wrong.

Answer B correct because permissions DynamoDB trust EC2 PassRole.

Answer C wrong because permissions EC2 wrong.

Answer D wrong because GetRole no.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html

145.Explain
Answer A correct because install local zip all.

Answer B wrong because lib/zip dependency no.

Answer C wrong because install code runtime.

Answer D wrong because no LB_LIBRARY_PATH.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/python-package.html

146.Explain
Answer A corrected
A Cognito user pool handles authentication (sign-up / sign-in).
But to call AWS services like DynamoDB from a front-end app, you must obtain temporary AWS credentials â€” never expose access or secret keys in the browser.
To do that securely, you use a Cognito Identity Pool (a.k.a. Federated Identities).
It exchanges the user pool JWT for temporary IAM credentials via STS.
These temporary credentials are then used by the AWS SDK for JavaScript to call DynamoDB securely.

Answer B wrong because The app is a front-end web application, not backend â€” running in EC2 is unnecessary and would still risk exposing keys if front-end calls are made.

Answer C wrong because Hardcoding keys is never secure, encryption of S3 objects does not protect exposed keys in code.

Answer D wrong because User pool tokens cannot directly authorize DynamoDB â€” AWS services require IAM credentials, not JWTs.

link ref: https://aws.amazon.com/cognito/

147.Explain
Answer A wrong because Neither provides the primary solution for declarative Infrastructure as Code (IaC) or version control needed for infrastructure definitions.

Answer B wrong because These are operational tools used after deployment; they do not define, deploy, or version the infrastructure itself.

Answer C wrong because Suboptimal. Elastic Beanstalk restricts the scope of infrastructure you can manage, failing the general "AWS infrastructure" requirement.

Answer D correct because CloudFormation create IaC, CodeCommit control version.

link ref: https://aws.amazon.com/cloudformation/

148.Explain
Answer A wrong because The ECS Service Definition is used to define how many copies of a task definition should run (scaling, load balancing), but it does not contain the container-specific parameters like environment variables.

Answer B correct because The ECS Task Definition defines the blueprint for the application, including which containers to run, their resource limits, ports, and crucially, container-specific configurations like environment variables, which are specified under the environment parameter within the container definition section.

Answer C wrong because he entryPoint parameter specifies the command that should be executed when the container starts. It is not used for defining key/value environment variables.

Answer D wrong because This combines two errors: The wrong parameter (entryPoint) and the wrong location (service definition).

link ref: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html

149.Explain
Answer A wrong because Server ID sort not time.

Answer B wrong because Redshift partition Customer, sort TS-Server wrong.

Answer C correct because DynamoDB partition Customer, sort TS-Server for range.

Answer D wrong because Redshift partition Customer, sort Server wrong.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html

150.Explain
Answer A correct because GSI on frequent keys.

Answer B wrong because LSI per partition.

Answer C wrong because global tables latency, scan costly.

Answer D wrong because Auto Scaling throughput.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html

151.Explain
Answer A wrong because CloudWatch alarms are designed to react to metrics breaching a threshold (e.g., the number of objects over time). It is not a direct, real-time trigger for a single object upload and would introduce unnecessary latency and complexity compared to a direct S3 event.
Higher Latency, Higher Cost (CloudWatch alarm fees).

Answer B correct This solution leverages a serverless, event-driven architecture, which is the most cost-effective and low-latency approach for this specific scenario.
You pay only when the function runs, so cost is extremely low when data arrives infrequently.
Lambda starts running within milliseconds of the S3 upload, so latency is very low.
AWS handles all scaling and server maintenance â€” no infrastructure to manage.
By configuring an S3 event notification, S3 automatically triggers the Lambda function as soon as a new object is uploaded. There is no need for polling, scheduling, or intermediate services.

Answer C wrong because A scheduled event (like a cron job) would run periodically (e.g., every 5 minutes) regardless of whether data was delivered. This is a polling mechanism and would be delayed if the data arrived just after the schedule ran, and it runs unnecessarily when no data is present.
Higher Latency (due to waiting for the next schedule), Higher Cost (paying for unnecessary Lambda executions 24/7).

Answer D wrong because EC2 is not serverless. You pay for the EC2 instance 24/7 whether it's processing data or not (high cost). The polling mechanism itself is inherently inefficient and introduces latency (you must wait for the next check interval).
Highest Cost (24/7 compute), Higher Latency (due to polling interval).

link ref: https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html

152.Explain
Answer A wrong because SSL auth not restrict account.

Answer B correct because resource policies restrict source.

Answer C wrong because CORS cross-origin.

Answer D wrong because usage plans throttling.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-resource-policies.html

153.Explain
Answer A wrong because same instance managed.

Answer B correct because DocumentDB Mongo compatible.

Answer C wrong because API Gateway translation complex.

Answer D wrong because replicate to DynamoDB changes app.

link ref: https://aws.amazon.com/documentdb/

154.Explain
Answer A wrong because Kinesis Streams streaming.

Answer B wrong because X-Ray tracing.

Answer C correct because CloudWatch Logs for errors.

Answer D wrong because Config compliance.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html

155.Explain
Answer A correct because ConsistentRead true on GetItem enforces strong consistency, ensuring latest data post-write, fixing old data issue in distributed DynamoDB.

Answer B wrong because DAX caches, eventual consistency default, strong reads possible but adds complexity/cost no guarantee fix without config.

Answer C wrong because UpdateTable sets schema/capacity, no Consistency param; consistency per read.

Answer D wrong because GetShardIterator for Streams changes, not item reads.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.ReadConsistency.html

156.Explain
Answer A wrong because RDS stored proc Lambda not serverless streams.

Answer B wrong because Direct Connect network Lambda no streams.

Answer C correct because Kinesis Streams ingests processes Lambda serverless downstream.

Answer D wrong because EC2 bash Lambda not serverless.

link ref: https://aws.amazon.com/kinesis/data-streams/

157.Explain
Answer A wrong because DynamoDB migrate changes structure.

Answer B correct because ElastiCache Redis caches queries minimal overhead.

Answer C wrong because Memcached EC2 managed overhead.

Answer D wrong because DAX DynamoDB no RDS.

link ref: https://aws.amazon.com/elasticache/redis/

158.Explain
Read Capacity Units (RCU) Calculation 
The base unit for Read Capacity is 1 RCU = one strongly consistent read of up to 4 KB per second.
- Step 1: Calculate RCUs per Read Operation 
  + Item size (10 KB) i vided by the ROU block ize (4 KB) and rounded up to the nearest. whole number. 
  + 10 KB/ 4 KB = 2.5 -> 3 RCUs per read 

- Step 2: Calculate Total RCUs 
  + RCUs per read x Reads per second 
  + 3 RCUs/read x 10 reads/sec = 30 RCUs 
---------------------------------------------------------
Write Capacity Units (WCU) Calculation 
The base unit for Write Capacity is 1 WCU = one standard write of up to 1 KB per second.
- Step 1: Calculate WCUs per Standard Write Operation 
  + Item size (10 KB) i divided by the WCU block size (1 KB). 
  + 10 KB/1 KB = 10 WCUs per standard write 

- Step 2: Apply Transactional Multiplier 
  + Transactional write requests consume double the capacity of a standard write. 
  + Standard WCUs x2 = Transaction
  + 10 WCUs/standard write x 2 = 20 WCUs per transactional write 

- Step 3: Calculate Total WCUs 
  + Transactional WCUs per write x Writes per second 
  + 20 WCUs/write x 2 writes/sec = 40 WCUs 


Answer A wrong because This is the unadjusted count of operations and doesn't account for the large item size, strongly consistent reads, or transactional writes. This would result in throttling.

Answer B correct because reads 30 RCU writes 40 WCU cost-effective provisioned.

Answer C wrong because This is explicitly disallowed by the requirement to be in provisioned mode.

Answer D wrong because This is ten times the required capacity and would be significantly more expensive than necessary.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html

159.Explain
Answer A correct because ElastiCache (Redis) is an in-memory data store that provides microsecond latency. By externalizing the session state here, the ECS tasks become stateless. Any task can handle any user request and retrieve the session instantly. Redis also offers high availability and fast failover (Multi-AZ), ensuring the user never loses their session, even if a container or an AZ fails. This is the standard best practice for scalable, high-performance session management.

Answer B wrong because Amazon Redshift is a data warehouse optimized for large-scale analytical queries (OLAP). It is not designed for the high-volume, low-latency, transactional key-value lookups required for session data. This would result in slow session reads and writes, leading to significant user latency and a poor experience.

Answer C wrong because Enabling stickiness attempts to route the user to the same container, but this is an anti-pattern for Fargate/ECS. If the container or the underlying host fails, the user is rerouted to a new container, and because the session data is local, the session is lost. Stickiness also hinders true load balancing and scaling.

Answer D wrong because Amazon S3 is Object Storage, optimized for durability and scale, not for low-latency transactional data like session state. The overhead and latency associated with constantly reading and writing small session objects to S3 would be too high for a good user experience.

link ref: https://aws.amazon.com/elasticache/redis/

160.Explain
Answer A correct because read replica Redis handles load failover.

Answer B wrong because Memcached no replication like Redis.

Answer C wrong because Elasticsearch search no cache.

Answer D wrong because vertical limited HA no.

link ref: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication.Redis.Groups.html

161.Explain
Answer A correct because X-Ray traces distributed microservices call stacks isolate faults.

Answer B wrong because Flow Logs network no app performance.

Answer C wrong because GuardDuty security threats.

Answer D wrong because Macie data privacy no performance.

link ref: https://aws.amazon.com/xray/

162.Explain
Answer A wrong because This is possible but overly complex and inefficient. Unit tests are a prerequisite for build artifacts and belong logically within the build stage of the primary pipeline. Running a nested pipeline adds latency and overhead.

Answer B correct The buildspec.yml file used by AWS CodeBuild defines the entire build and testing process. It includes dedicated phases (pre_build, build, post_build) where the developer can execute unit test commands (e.g., npm test, pytest). If the tests fail, CodeBuild fails the action, stopping the pipeline before artifacts are staged.

Answer C wrong because AWS CodeDeploy is an orchestration service for deployment (installing, updating, rolling back applications). It is not designed for running unit tests during the build phase; that is CodeBuild's job.

Answer D wrong because AWS CodeCommit is a Git repository used for storing source code and managing versions. Creating a branch is a source control practice, but it doesn't execute tests; a CI/CD service must be triggered to run the tests on that branch.

link ref: https://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html

163.Explain
Answer A wrong because DynamoDB has a maximum item size of 400 KB. It cannot store 100 MB messages. Storing thousands of terabytes would also be prohibitively expensive compared to S3.

Answer B correct S3 is designed for high throughput with acceptable read latency in seconds.practically unlimited and cost-effective at PB scale. supports objects up to 5 TB. S3 is an object store with key-based retrieval and eventual consistency for overwrite/delete. Cheaper than DynamoDB/RDS/ElastiCache at massive scale.

Answer C wrong because RDS is designed for relational data. Scaling an RDBMS to thousands of terabytes is operationally complex and extremely costly. It is not cost-effective for simple key/value storage at this scale.

Answer D wrong because ElastiCache is an in-memory caching service. It is not persistent (data loss on failure) and storing thousands of terabytes in RAM would be astronomically expensive, violating the cost-effective requirement entirely.

link ref: https://aws.amazon.com/dynamodb/

164.Explain
Answer A wrong because User Pool is for authentication (managing users who do log in). Creating a blank user ID is an operational anti-pattern and still requires the application to "log in" as that dummy user, which is complex and doesn't represent a true guest experience.

Answer B correct because An Identity Pool (Cognito Federated Identities) is designed to exchange identity information (from a login provider, or just a "guest" status) for AWS temporary credentials. Enabling unauthenticated identities allows the user to exchange their status for an IAM role's credentials (which has S3 read access), granting guest access without a login.

Answer C wrong because A User Pool manages users and groups, but it doesn't grant direct access to AWS services. Access to AWS services requires an Identity Pool. Furthermore, the requirement is for unauthenticated access.

Answer D wrong because A User Pool is designed for authentication. Disabling authentication makes the pool useless for the intended purpose, and it still doesn't solve the problem of granting AWS resource access to guests.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/iam-roles.html#unauthenticated-roles

165.Explain
Answer A wrong because CodeBuild compiles/tests code, not stores versions or tracks changes long-term.

Answer B wrong because S3 stores files but lacks Git-style versioning, branching, and batch change tracking (e.g., diffs, commits).

Answer C correct because CodeCommit is AWSâ€™s fully managed Git repository, supporting multiple versions, branching, pull requests, and change historyâ€”ideal for team collaboration and feedback.

Answer D wrong because Cloud9 is a cloud IDE for development, not a version control system.

link ref: https://aws.amazon.com/codecommit/

166.Explain
Answer A wrong because ChangeMessageVisibility controls how long a message is invisible after receipt, not batch size.

Answer B wrong because AddPermission grants queue access to other AWS services, not controls message batching.

Answer C correct because ReceiveMessage API has MaxNumberOfMessages parameter (default 1, max 10); setting >1 returns multiple messages per call, increasing throughput.

Answer D wrong because SetQueueAttributes sets queue-level settings (e.g., visibility timeout), but no MaxNumberOfMessages attribute exists.

link ref: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_ReceiveMessage.html

167.Explain
Answer A wrong because Lambda can be a target for ALB (via Lambda target groups).

Answer B wrong because registration is supported via CLI, Console, and SDKs.

Answer C correct because even if Lambda is registered as a target, ALB needs permission to invoke it via the functionâ€™s resource policy (lambda:InvokeFunction). Missing this causes invocation failure.

Answer D wrong because cross-zone load balancing affects traffic distribution, not Lambda invocation permissions.

link ref: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/lambda-functions.html#lambda-permissions

168.Explain
Answer A wrong because throttling limits rate but doesnâ€™t enforce per-user SLAs or require API keys.

Answer B correct because usage plans in API Gateway allow per-user rate limits, quotas, and require API keysâ€”enabling SLA compliance and tracking.

Answer C wrong because Cognito manages authentication, not API rate limiting.

Answer D wrong because default stage throttling is global, not per-user.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-create-usage-plans.html

169.Explain
Answer A wrong because Ref only works within the same stack, not across stacks.

Answer B correct
When using multiple CloudFormation stacks, the proper way to share resources between stacks is:

Export the output value from the first stack (infrastructure).

Import that value in the second stack (application) using Fn::ImportValue.

Example output in infrastructure template:

Outputs:
  VPCId:
    Value: !Ref MyVPC
    Export:
      Name: MyVPC-ID


Then in the application template:

VpcId: !ImportValue MyVPC-ID


This allows the application stack to reference the VPC created by the infrastructure stack safely and cleanly.

Answer C wrong because DependsOn only controls resource creation order within the same template. It does not share values across stacks.

Answer D wrong because Fn::GetAtt can only retrieve attributes of resources in the same stack, not from another template.

link ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html

170.Explain
Answer A wrong because AppSync is for GraphQL APIs, not SAML/Facebook + DynamoDB access.

Answer B correct because Cognito identity pools support SAML and Facebook federation, issue temporary AWS credentials for DynamoDB access with minimal code.

Answer C wrong because user pools handle authentication (sign-up/sign-in), not SAML or AWS service access directly.

Answer D wrong because Lambda@Edge is for edge logic, not identity federation.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-identity.html

171.Explain
Answer A wrong because CLI supports custom metrics via put-metric-data.

Answer B correct because put-metric-data publishes custom metrics; put-metric-alarm creates alarms, not metrics.

Answer C wrong because unified agent is optional; CLI/SDK can publish metrics.

Answer D wrong because location doesnâ€™t matterâ€”CLI works anywhere with credentials.

link ref: https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/put-metric-data.html

172.Explain
Answer A wrong because default EC2 metrics (CPU, network) donâ€™t include app-generated values.

Answer B wrong because S3 file storage requires manual parsing, not real-time graphing.

Answer C correct because publishing values via AWS SDK (e.g., PutMetricData) to CloudWatch enables real-time monitoring and graphing.

Answer D wrong because EC2 doesnâ€™t allow custom variable reporting to CloudWatch.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html

173.Explain
Answer A wrong because SES + custom app is complex and not integrated with CodePipeline.

Answer B wrong because tagging doesnâ€™t trigger approval steps.

Answer C wrong because CodeCommit has no built-in approval step before commit.

Answer D correct because CodePipeline supports manual approval actions that pause pipeline and notify via SNSâ€”standard and integrated.

link ref: https://docs.aws.amazon.com/codepipeline/latest/userguide/approvals.html

174.Explain
Answer A wrong because Lambda@Edge is used for running code at CloudFront edge locations, primarily for customizing content delivery and responses, not for defining the primary, regional REST API backend.

Answer B correct The standard pattern for a serverless REST API is to use Amazon API Gateway as the managed HTTP endpoint and connect it to an AWS Lambda function (the backend logic). API Gateway handles the routing, security, and traffic management, while Lambda runs the code.

Answer C correct Within the API Gateway resource definition (e.g., /users), you must explicitly define the GET HTTP method. This method is then configured to be the specific integration point that triggers the Lambda function.

Answer D wrong because The Lambda function itself contains the application logic (the handler), but it doesn't expose the method; it just processes the request event passed to it by the trigger (API Gateway). The method exposure happens entirely within API Gateway.

Answer E wrong because Amazon Route 53 is a Domain Name System (DNS) service. It handles mapping a domain name (like api.example.com) to the API Gateway endpoint but does not define or expose HTTP methods.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/getting-started.html

175.Explain
Answer A correct because Secrets Manager stores connection strings securely, supports rotation, and allows runtime retrieval without code changes.

Answer B wrong because IAM user keys are for AWS access, not app config.

Answer C wrong because KMS is for encryption, not config storage.

Answer D wrong because layers are for code/libraries, not runtime config.

link ref: https://aws.amazon.com/secrets-manager/

176.Explain
Answer A wrong because CloudTrail logs API calls (e.g., PutObject), not every GET/PUT request; expensive for 1,000 TPS.

Answer B wrong because lifecycle to expire in 90 days loses audit trail.

Answer C wrong because CloudTrail doesnâ€™t log data access events at scale.

Answer D correct because S3 server access logging captures every request (GET/PUT), and Glacier transition in 90 days minimizes cost while retaining audit.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html

177.Explain
Answer A wrong because SSE-S3 uses AWS-managed keysâ€”no granular control or rotation.

Answer B correct because SSE-KMS allows custom CMK with full control: create, rotate, disable, audit via CloudTrail.

Answer C wrong because Secrets Manager is for secrets, not S3 encryption.

Answer D wrong because customer-provided keys (SSE-C) require client-side key managementâ€”no AWS rotation.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html

178.Explain
Answer A wrong because EBS snapshot on boot doesnâ€™t share data across instances.

Answer B correct because S3 is durable, shared, and immediately accessible by all Auto Scaling instances.

Answer C wrong because instance storage is ephemeral and not shared.

Answer D wrong because file sync introduces complexity and eventual consistency.

link ref: https://aws.amazon.com/s3/

179.Explain
Answer A wrong because CORS on the S3 bucket would only be necessary if the API Gateway or Lambda function were trying to access assets from the S3 bucket. Here, the S3 site is the client, and the API Gateway is the server.

Answer B correct because The error No Access-Control-Allow-Origin header is present... is thrown by the browser when the requested server (the API Gateway endpoint) does not return the required Access-Control-Allow-Origin HTTP header in its response. Since the API Gateway is the resource being accessed from a different origin (the S3 static website), CORS must be configured on the API Gateway to allow the S3 domain to access it.

Answer C wrong because These headers are sent by the browser automatically during a preflight CORS request (an OPTIONS request) to ask the server for permission. Adding them manually to the primary request does not solve the underlying problem of the server (API Gateway) failing to return the required Access-Control-Allow-Origin response header.

Answer D wrong because Similar to the method header, this is part of the browser's preflight request for certain complex requests. It is a client-side header and does not force the API Gateway to return the necessary response header.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html

180.Explain
Answer A wrong because SSE-C requires client to manage keysâ€”no Security team control.

Answer B wrong because client-side master key means app manages keyâ€”violates policy.

Answer C correct because client-side encryption with KMS CMK allows app to encrypt before upload; Security team manages CMK in KMS.

Answer D wrong because S3-managed keys (SSE-S3) offer no key control.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html

181.Explain
Answer A wrong because Kinesis Data Streams doesnâ€™t support Auto Scaling.

Answer B wrong because delay between calls doesnâ€™t increase throughput.

Answer C correct because more shards increase write/read capacity (1 MB/s write, 2 MB/s read per shard).

Answer D wrong because ShardIterator is for reading, not fixing throughput.

Answer E correct because exponential backoff retries throttled PutRecords/GetRecords calls gracefully.

link ref: https://docs.aws.amazon.com/streams/latest/dev/kinesis-using-sdk-java-handle-throttling.html

182.Explain
Answer A wrong because console has no â€œencryptâ€ toggle for existing log groups.

Answer B wrong because create-log-group is for new groups only.

Answer C wrong because KMS console doesnâ€™t associate keys with log groups.

Answer D correct because associate-kms-key applies CMK to existing log group for future encryption.

link ref: https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_AssociateKmsKey.html

183.Explain
Answer A wrong because hardcoding keys is insecure and unscalable.

Answer B correct because IAM role with AmazonDynamoDBReadOnlyAccess provides temporary, rotated credentials securely.

Answer C wrong because root keys are dangerous and against best practices.

Answer D wrong because Administrator access violates least privilege.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html

184.Explain
Answer A wrong because rolling updates still cause session loss without shared state.

Answer B correct because ElastiCache (Redis/Memcached) externalizes session stateâ€”survives instance replacement in blue/green.

Answer C wrong because sticky sessions bind to instance; new instances in blue/green have no session.

Answer D wrong because multicast not supported in AWS.

link ref: https://aws.amazon.com/elasticache/

185.Explain
Answer A wrong because CloudWatch Events doesnâ€™t monitor S3 bucket changes natively.

Answer B correct because S3 event notifications trigger Lambda to insert into DynamoDBâ€”event-driven and serverless.

Answer C wrong because polling is inefficient and not real-time.

Answer D wrong because cron is scheduled, not event-driven.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html

186.Explain
Answer A wrong because CodeDeploy doesnâ€™t use S3 snapshots for rollback.

Answer B wrong because Route 53 alias swap is manual, not automatic in CodeDeploy.

Answer C correct because CodeDeploy auto-rolls back by deploying the last successful deployment with a new ID.

Answer D wrong because CodePipeline doesnâ€™t auto-promote on failure.

link ref: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-configurations.html#deployment-configuration-rollback

187.Explain
Answer A wrong because IAM roles are for EC2/ECS, not S3 bucket access.

Answer B correct because bucket policy on assets bucket can allow code bucketâ€™s principal (s3:GetObject).

Answer C wrong because public access violates security.

Answer D wrong because Lambda unnecessary for static hosting.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html

188.Explain
Answer A wrong because Lambda console doesnâ€™t support CodePipeline triggers.

Answer B correct because CodePipeline console allows adding Lambda invocations on state changes via event triggers.

Answer C wrong because CloudWatch alarm is reactive, not integrated.

Answer D wrong because CloudWatch Events rule requires manual setupâ€”less integrated.

link ref: https://docs.aws.amazon.com/codepipeline/latest/userguide/actions-invoke-lambda-function.html

189.Explain
Answer A wrong because SAM templates are not built in EC2 or EBS.

Answer B correct because sam build â†’ sam package (to S3) â†’ sam deploy (from S3) is the standard SAM CLI workflow.

Answer C wrong because packaging must precede deployment.

Answer D wrong because CodeCommit not used in packaging.

link ref: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference.html

190.Explain
Answer A/B wrong because storing credentials (even temporarily) in EBS or user data is insecure.

Answer C correct because EC2 instance profile with IAM role grants secure, rotating access to S3 without credentials.

Answer D wrong because S3 service role doesnâ€™t existâ€”roles are for EC2/Lambda.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html

191.Explain
Answer A correct because S3 can trigger CodePipeline on object changes (via EventBridge or S3 events).

Answer B wrong because EBS volumes donâ€™t trigger CodePipeline.

Answer C correct because CodeCommit push events natively trigger CodePipeline.

Answer D wrong because polling every 15 mins is not immediate.

Answer E wrong because ephemeral storage not suitable for source control.

link ref: https://docs.aws.amazon.com/codepipeline/latest/userguide/triggers.html

192.Explain
Answer A wrong because single EC2 instance not scalable.

Answer B correct because SQS + Auto Scaling EC2 group processes messages in parallel, scales with load.

Answer C wrong because Lambda timeout max is 15 minsâ€”cannot increase.

Answer D wrong because RDS insert not scalable for high volume.

link ref: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html

193.Explain
Answer A wrong because language change doesnâ€™t guarantee faster CPU.

Answer B wrong because layers reduce size slightly but donâ€™t increase CPU.

Answer C wrong because CPU is tied to memoryâ€”no direct allocation.

Answer D correct because increasing memory proportionally increases CPU, speeding up execution.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html

194.Explain
Answer A wrong because This requires provisioning and configuring EC2 instances, re-architecting the API Gateway integration, and resolving dependenciesâ€”a long, complex process that is the opposite of a quick fix.

Answer B wrong because Migrating a database is a major, time-consuming effort that involves downtime, schema changes, and application re-coding. It does not address the immediate code failure in the Lambda function.

Answer C correct because Since the application stopped working after the new release, the new Lambda function is the most likely culprit. AWS Lambda automatically retains previous versions. Rolling back involves a simple update of the Lambda function's alias or API Gateway stage to point to the last known working version number. This is an instantaneous configuration change that bypasses the broken code and restores service almost immediately.

Answer D wrong because Deploying the latest (broken) function elsewhere will just reproduce the failure in a different region. The problem is the code quality, not the region, and setting up multi-region deployment takes time.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/versioning-aliases.html

195.Explain
Answer A correct because DynamoDB is durable, scalable for session storage.

Answer B wrong because Cognito is for authentication, not session state.

Answer C correct because ElastiCache (Redis) is ideal for fast, in-memory session storage.

Answer D/E wrong because EBS not shared, SQS not for sessions.

link ref: https://aws.amazon.com/elasticache/

196.Explain
Answer A wrong because SDK doesnâ€™t auto-send logs.

Answer B correct because CloudWatch agent on-premises collects and sends logs to CloudWatch using IAM credentials.

Answer C/D wrong because S3 upload or EC2 forwarding is manual and complex.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartOnPrem.html

197.Explain
Answer A wrong because identity pools give temp creds, not user auth with refresh.

Answer B wrong because custom DB + Lambda authorizer is complex.

Answer C correct because Cognito user pools issue JWTs that expire and refreshâ€”integrated with API Gateway authorizer.

Answer D wrong because IAM users not suitable for app users.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html

198.Explain
Answer A wrong because Spring increases cold start.

Answer B correct because smaller package â†’ faster initialization.

Answer C correct because more memory â†’ more CPU â†’ faster cold start and execution.

Answer D wrong because timeout doesnâ€™t affect cold start.

Answer E wrong because sync/async doesnâ€™t impact cold start.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/best-practices.html

199.Explain
Answer A/B wrong because VPN/BGP logs show connectivity, not intra-VPC traffic.

Answer C correct because VPC Flow Logs capture traffic to/from subnet B.

Answer D wrong because CloudTrail logs API calls, not network traffic.

link ref: https://docs.aws.amazon.com/v Pluto/latest/userguide/flow-logs.html

200.Explain
Answer A wrong because The s3:EncryptionConfiguration action is used for managing the encryption settings on the S3 bucket itself (i.e., changing the default encryption). It is not required for a user to simply upload an object under those existing rules.

Answer B wrong because The prompt states the user already has s3:putObject permission (likely via the IAM user policy). If the S3 bucket policy were denying the upload, the user wouldn't need to add permission, but check for an explicit deny. The error here points to a missing dependency, not an S3 access issue.

Answer C correct because SSE-KMS requires kms:GenerateDataKey to create data key for encryption.
Since the IAM user received an Access Denied error despite having s3:PutObject permission, the denial is almost certainly coming from the KMS key policy or the IAM user policy lacking the necessary kms:GenerateDataKey permission. Updating the IAM user policy to include this action allows S3 to successfully encrypt the data using the KMS key, resolving the issue.

Answer D wrong because Access Control Lists (ACLs) are a legacy mechanism that only controls bucket/object ownership and specific read/write permissions. They do not control access to the KMS key, which is the source of the access denied error in an SSE-KMS scenario.

link ref: https://docs.aws.amazon.com/kms/latest/developerguide/iam-policies.html

201.Explain
Answer A correct because Amazon Cognito hosted UI allows customization with logos, CSS, and branding for login pages without building custom interfaces.

Answer B wrong because Cognito doesn't host uploaded login pages; it's for auth flows, not static hosting.

Answer C wrong because API Gateway proxies APIs, not hosts login pages; Cognito integration is for auth, not saving links.

Answer D wrong because Cognito app settings handle client details but don't upload logos for custom pages; hosted UI is for that.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/hosted-ui.html

202.Explain
Answer A wrong because DeleteItem removes items, unnecessary for update/create.

Answer B wrong because UpdateItem assumes existence; DescribeTable is metadata, not data ops.

Answer C wrong because GetRecords is Streams, UpdateTable schema.

Answer D correct because UpdateItem modifies existing or creates if absent (with condition); GetItem retrieves, PutItem creates.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html

203.Explain
Answer A wrong because SSE-S3 uses AWS keys. While S3 automatically encrypts the data, there is no visibility or logging of the S3 master key usage in AWS CloudTrail, as the key is managed internally by AWS.

Answer B correct because SSE-KMS provides CloudTrail-audited key usage for who/when. Every API call made to KMS (such as Decrypt or GenerateDataKey) to access the master key is logged in AWS CloudTrail. This log records the time, the AWS service (S3), and the IAM Principal (who) that requested the key operation, directly meeting the audit requirement.

Answer C wrong because SSE-C uses customer keys without AWS auditing. AWS never stores your master key and only uses it transiently for the request. Key usage is outside of AWS's control, so AWS cannot provide an audit log for its usage.

Answer D wrong because self-managed keys lack AWS integration/audit. This is not a standard S3 server-side encryption option.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html

204.Explain
Answer A wrong because doubling max servers increases cost without fixing static latency.

Answer B wrong because Lambda is serverless compute, not for static content hosting.

Answer C wrong because vertical scaling improves instance performance but not global distribution.

Answer D correct because CloudFront caches static content at edges, reducing latency worldwide.

Answer E correct because S3 stores static files durably/scalably; pair with CloudFront for delivery.

link ref: https://aws.amazon.com/cloudfront/

205.Explain
Answer A wrong because Generating a **presigned URL when the picture is uploaded** means the URL will eventually **expire** (by default, max 7 days, or less depending on the generating credentials). This is **not a viable long-term solution** for repeatedly displaying the picture every time the employee logs in over months or years.

Answer B wrong because An **Amazon S3 VPC endpoint** allows **private networking** (i.e., EC2 instances, Lambda functions) access to S3 but does **not** handle authentication or authorization for individual users accessing specific private objects from a web browser (outside the VPC). The browser (client) still needs authenticated access to the private S3 object. Additional proxying (e.g., via API Gateway) is needed, adding complexity without direct viability.

Answer C wrong because base64 bloats data, inefficient for images. **Base64 encoding** and saving the string in a database is extremely inefficient and costly, especially since there is **no size limit** for the pictures. Standard relational databases (like RDS) and NoSQL databases (like DynamoDB) are poorly suited for storing large binary objects (BLOBs) and have stringent size limits that would be quickly exceeded.

Answer D correct This is the standard best practice for providing secure, temporary access to private S3 content:
    1. **S3 Key Storage:** Save the object's **S3 key** (path) in a database (like DynamoDB or RDS). This is highly efficient.
    2. **Just-in-Time Access:** When the employee logs in, the application's backend retrieves the key and securely **generates a new presigned URL** with a short expiration time (e.g., 5 minutes) using its **IAM credentials**.
    3. **Browser Access:** The browser uses the temporary, short-lived presigned URL to securely download the private picture directly from S3. This ensures the picture remains private and secure long-term, and access is always authenticated and temporary.
Store image key in DB â†’ generate presigned URL at login â†’ send to frontend â†’ browser displays image securely.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/vpc-endpoints.html

206.Explain
Answer A wrong because multiple Regions add latency/complexity, no CPU boost.

Answer B wrong because AZs are for HA, not compute.

Answer C wrong because layers reduce size but not CPU.

Answer D correct because Lambda CPU scales with memory; max memory maximizes CPU.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/configuration-function-common.html

207.Explain
Answer A correct because update connection strings to RDS endpoint/port for seamless migration.

Answer B wrong because SDK creds unnecessary; RDS uses IAM DB auth optional.

Answer C wrong because new EC2 role doesn't update existing app connections.

Answer D wrong because Lambda routing adds latency/complexity.

link ref: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ConnectToInstance.html

208.Explain
Answer A correct **Target tracking scaling policy.** This is the most efficient and scalable solution. It allows the developer to set a target for the average SQS queue depth per task (e.g., 5 messages per task). ECS will automatically calculate the exact number of tasks required to maintain that target, **dynamically scaling the worker fleet** up or down based on demand, which improves performance and keeps costs low. 

Answer B wrong because Swarm is Docker orchestration, not AWS-native.

Answer C wrong because Service scheduler manages task placement, not autoscaling based on SQS metrics.

Answer D wrong because step scaling is reactive thresholds, less dynamic than target. Step scaling policy requires manual definition of capacity adjustments in fixed steps and is less efficient than target tracking for maintaining a desired metric value (queue depth).

link ref: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html

209.Explain
Answer A wrong because IAM is for AWS access, not social/app login.

Answer B wrong because **Amazon Cognito Identity Pools (Federated Identities)** are used to exchange credentials (from a User Pool, social provider, etc.) for **temporary AWS credentials**. They manage authorization (what resources the user can access) but **do not** handle the registration, storage, or local authentication of users.

Answer C correct because **Amazon Cognito User Pools** is the service designed for customer-facing application identity. It acts as a user directory that can:
    1. Handle **New User Registration** (local accounts) and manage their profile data.
    2. Support **Federation** (Social Login) by integrating with external Identity Providers (like Facebook, Google, Amazon, or SAML/OIDC) to allow users to sign in using their social media credentials.

Answer D wrong because Directory Service is enterprise AD, not social/mobile.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-social-idp.html

210.Explain
Answer A correct A Lambda Authorizer (or Custom Authorizer) is a separate Lambda function invoked by API Gateway before the main integration logic. It is the only option that allows the developer to write custom code to extract the two headers (Client ID and User ID), perform a lookup in the DynamoDB table using the AWS SDK, and return an IAM policy that either allows or denies the request.

Answer B wrong because A Model is used for validating the structure and format of the request body (payload), not for performing authentication or authorization against a custom database.

Answer C wrong because The Integration Request is used for transforming data after authentication has succeeded. API Gateway's native service integrations (like direct DynamoDB integration) perform simple operations, but they cannot perform complex authentication logic (read key/value, compare multiple headers, and then generate an IAM policy) before executing the main API method.

Answer D wrong because A Cognito User Pool Authorizer is designed to validate a JWT token issued by an Amazon Cognito User Pool. It cannot be directly configured to read arbitrary credentials from two separate headers and query a custom DynamoDB table for verification. This would require custom logic, which means using a Lambda Authorizer (even if Cognito was part of the wider solution).

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html

211.Explain
Answer A wrong because GetFederationToken for role assumption, no MFA.

Answer B wrong because GetCallerIdentity gets identity, no MFA temp creds.

Answer C correct because GetSessionToken generates MFA-protected temp creds for API calls.

Answer D wrong because DecodeAuthorizationMessage decodes errors, no MFA.

link ref: https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html

212.Explain
The error **`ThrottlingException`** from **AWSKMS** means the application is exceeding the transactions per second (TPS) limit for KMS API calls in that region (primarily `Decrypt` calls when reading SSE-KMS objects).

Answer A correct because KMS throttling (e.g., 10k req/s) requires support increase for high reads.

Answer B correct because exponential backoff retries throttled requests without overwhelming.

Answer C wrong because S3 limits separate; error is KMS.

Answer D wrong because The error is explicitly from **AWSKMS**, not S3. Requesting an S3 rate limit increase would not solve the KMS throttling problem.

Answer E wrong because The size of the CMK (e.g., 256-bit) is irrelevant to the API call transaction limit (TPS).

link ref: https://docs.aws.amazon.com/kms/latest/developerguide/quotas.html

213.Explain
Answer A wrong because handler/core separation is best practice, not for notifications.

Answer B wrong because CloudWatch Events schedules/triggers, no data send.

Answer C correct because SNS publishes processed data to admins (email/SMS).

Answer D wrong because SQS queues for processing, not direct notify.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/services-sns.html

214.Explain
Answer A wrong Encrypting data before sending it (Client-Side Encryption) requires the developer to manage the encryption, decryption, and key rotation logic outside of AWS, which is complex and high effort.

Answer B wrong Importing a custom key into AWS KMS allows you to manage the key material, but you cannot enable automatic rotation for imported key material. You would have to manually rotate the key material annually.

Answer C correct **Using AWS KMS (Server-Side Encryption with KMS keys - SSE-KMS)** and enabling the **automatic key rotation feature** is the easiest way. AWS KMS can be configured to automatically rotate the backing key (the cryptographic material) once a year, transparently and without any change to the S3 bucket or application code, while still encrypting the data at rest. 

Answer D wrong Exporting a key from KMS (a process called CMK export) is not standard practice for SSE-KMS and would lead to manual key management, negating the "easiest way" requirement.

link ref: https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html

215.Explain
Answer A wrong because x-Version header custom routing in Lambda, no Gateway support.

Answer B wrong because authorizer validates, no version routing.

Answer C wrong because resource policy secures, no version isolation.

Answer D correct because stages (dev/prod) unique endpoints; variables pass context.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html

216.Explain
Answer A wrong because The AWS KMS Encrypt API has a 4 KB limit on the data it can process in a single call. This call would fail for a 100 GB object.
-> Exceeds the 4 KB KMS limit.

Answer B wrong because This still calls the Encrypt API, meaning it still violates the 4 KB size limit, regardless of the key material source.
-> Exceeds the 4 KB KMS limit.

Answer C correct because This is the standard, best-practice process known as envelope encryption . AWS KMS has a 4 KB limit on the data it can encrypt directly. The solution is to use KMS to generate a unique, highly-available data key (which is small), use the plaintext data key to quickly encrypt the large (100 GB) object locally, and then store the encrypted data key alongside the encrypted object.
-> BEST Approach. Circumvents the 4 KB limit and is fast.

Answer D wrong because You cannot use an encrypted key to encrypt data; the key must be decrypted (in plaintext) for the encryption algorithm to work. While this API call is valid for generating the encrypted key, you must then call Decrypt to get the plaintext key before using it. The GenerateDataKey API call is superior because it returns both the plaintext and the encrypted key in a single step.
-> Inefficient. Cannot use encrypted key for encryption; requires an extra API call to Decrypt.

link ref: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys

217.Explain
Answer A wrong because This token is used to authenticate with GitHub, not AWS CodeCommit.
-> Not relevant to CodeCommit access.

Answer B wrong because SSH keys are an alternative method for connecting to CodeCommit, but the question specifies the connection will be made over HTTPS. SSH keys are not used for HTTPS authentication.
-> Only relevant for SSH connections.

Answer C correct because When connecting to CodeCommit over HTTPS, the required authentication mechanism is a unique username and password pair. This pair is generated within the AWS IAM console under the specific IAM user's security credentials section. This credential pair is then used by Git to authenticate the push/pull requests.
-> Required. This is the standard, secure way to authenticate Git operations over HTTPS with CodeCommit.

Answer D wrong because IAM Roles are for granting permissions to AWS services (like an EC2 instance) to access other AWS services. While an EC2 instance could use a role to access CodeCommit, the process of migrating a cloned repository (usually done locally or on a separate client machine) typically requires user-level credentials, not an EC2 service role.
-> A service role is not the direct authentication method for a Git client.

link ref: https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-https-unixes.html

218.Explain
Answer A wrong because GetItem single.

Answer B correct because BatchGetItem multiple.

Answer C wrong because no GetMultipleItems.

Answer D wrong because no GetItemRange.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html

219.Explain
Answer A wrong because While having two interfaces (one public, one private) is a valid design for some advanced routing or firewall scenarios, a basic NAT instance only requires one ENI in the public subnet to receive traffic from the private subnet via the route table and forward it to the Internet Gateway. The issue is with the packet forwarding rule, not the number of interfaces.

Answer B wrong because Instances in a private subnet should not have direct access to the public subnet via a second ENI. This defeats the purpose of the private subnet and doesn't resolve the core issue of the NAT device dropping forwarded packets.

Answer C correct 
1. Default Behavior: By default, every EC2 instance performs a Source/Destination Check. This means the instance verifies that it is either the source or the destination of any traffic it sends or receives. If the traffic's source or destination IP address does not match the instance's own IP, the traffic is dropped.

2. NAT Function: A NAT device's job is to receive traffic from a private instance (Source IP: Private Instance), translate the private IP to its own public IP, and then send the traffic to the Internet (Source IP: NAT Instance). When the NAT instance receives the traffic from the private subnet, the destination is the internet, and the source is the private instance. When the NAT instance sends the traffic out, the source is its own public IP.

3. The Problem: The NAT instance must accept traffic where the destination is not itself (it's the internet), and it must send traffic where the source is not its original private address. Because the NAT device is forwarding traffic on behalf of other instances, its IP address appears as the source/destination of traffic that it did not originate. To allow this packet forwarding behavior, the Source/Destination Check must be explicitly disabled on the NAT instance.

Answer D wrong because The private instance needs outbound internet access, not its own public IP address. Giving the private instance its own EIP would make it a public instance, negating the need for the NAT device. The NAT device (in the public subnet) must have the EIP.

link ref: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Instance.html

220.Explain
Answer A correct because eventual consistency GETs.

Answer B wrong because no replication.

Answer C wrong because no delay.

Answer D wrong because no limit.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/Introduction.html#ConsistencyModel

221.Explain
Answer A wrong because 100 region. The limit is global per account.

Answer B wrong because limit.

Answer C correct because The limit for the number of Amazon S3 buckets that can be created per AWS account is determined by two values:
1.  **Default Soft Limit:** The starting limit is **100 buckets** per AWS account.
2.  **Maximum Achievable Limit:** This limit can be increased by submitting a service limit increase request to AWS Support, up to a maximum of **1,000 buckets** per AWS account.
Since the options include 1,000, it represents the maximum capacity available to a single account.

Answer D wrong because 500 no.

Answer E wrong because 100 IAM no. The limit applies to the AWS account.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/BucketRestrictions.html

222.Explain
Answer A wrong because user write no.

Answer B wrong because add role running.

Answer C wrong because add user no.

Answer D correct because launch role.

Answer E correct because role write.

Answer F wrong because user launch no.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html

223.Explain
Answer A correct because default deny.

Answer B wrong because allow not deny.

Answer C correct because allow default deny.

Answer D wrong because deny allow.

Answer E wrong because default deny.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_evaluation-logic.html

224.Explain
Answer A wrong because NAT public.

Answer B wrong because routing ok assumed.

Answer C wrong because OS IP no.

Answer D correct because EIP public access.

link ref: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html

225.Explain
Answer A wrong because 0 no.

Answer B wrong because 1 hour no.

Answer C wrong because 1 day no.

Answer D wrong because forever no.

Answer E correct because 30s default visibility.

link ref: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html

226.Explain
Answer A wrong because XML UnsubscribeURL.

Answer B wrong because JSON DuplicateFlag no.

Answer C wrong because XML DuplicateFlag no.

Answer D correct because JSON unsubscribeURL.

link ref: https://docs.aws.amazon.com/sns/latest/dg/sns-message-formats.html

227.Explain
Answer A wrong because storage-class.

Answer B wrong because MD5 integrity.

Answer C wrong because token session.

Answer D correct because server-side-encryption header.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html

228.Explain
Answer A correct because Tomcat supported.

Answer B correct because .NET supported.

Answer C wrong because Websphere no.

Answer D wrong because JBoss no.

Answer E wrong because Jetty no.

link ref: https://aws.amazon.com/elasticbeanstalk/

229.Explain
Answer A correct because Join http GetAtt DNSName.

Answer B wrong because no Url att.

Answer C wrong because no ElasticLoadBalancerUrl.

Answer D wrong because Ref DNSName no.

link ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-elasticloadbalancing-loadbalancer.html

230.Explain
Answer A wrong because Hosting no restrict.

Answer B correct because bucket policy restrict.

Answer C wrong because Federation identities.

Answer D correct because ACL restrict.

Answer E wrong because CloudFront distribution.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-best-practices.html

231.Explain
Answer A wrong because deleted on fail.

Answer B correct because rollback deletes.

Answer C wrong because stops on fail.

Answer D wrong because not advance.

link ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-rollback.html

232.Explain
Answer A correct because (Required) This specifies the Amazon Resource Name (ARN) of the SNS topic to which the message will be published. This is the primary target identifier.

Answer B correct because (Optional) This specifies the text that will appear in the Subject line of an email notification or as the title in some push notifications. The maximum length is 100 characters.

Answer C wrong because This is not used for publishing to a topic. For direct publishing to a single endpoint (not via a topic), you use TargetArn or PhoneNumber, not Destination.

Answer D wrong because The format (e.g., raw text or JSON) is typically configured per subscription or specified in the MessageStructure parameter (if publishing a JSON object with different formats for different protocols), but is not a standalone argument named Format.

Answer E correct because (Required) This contains the content of the notification that will be delivered to the subscribed endpoints (e.g., the email body, the push notification text, or the content of the SQS message).

Answer F wrong because SNS does not have a native Language parameter to define the message language. Internationalization is handled by the application consuming the message.

Request Parameters
 - Message
 - MessageAttributes
 - MessageDeduplicationId
 - MessageGroupId
 - MessageStructure
 - PhoneNumber
 - Subject
 - TargetArn
 - TopicArn

link ref: https://docs.aws.amazon.com/sns/latest/api/API_Publish.html

233.Explain
Answer A wrong because CloudWatch does not expose IP addresses. It stores monitoring metrics (CPU, network, etc.), not network identity info.

Answer B wrong because These commands return only local network interface information. They show the private IP but not the public IP assigned by AWS, especially if it's behind a NAT or Elastic IP.

Answer C wrong because Userdata contains user-supplied boot script, not dynamic network information. It has nothing to do with IP address discovery.

Answer D correct
EC2 provides a special Instance Metadata Service (IMDS) that is accessible only from inside the instance:
http://169.254.169.254/latest/meta-data/
From there, software can retrieve:
Public IP â†’ /public-ipv4
Private IP â†’ /local-ipv4
No credentials needed, secure, and the AWS-recommended method.

link ref: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instance-metadata.html

234.Explain
Answer A wrong because region specific.

Answer B wrong because no country.

Answer C correct because same region.

Answer D wrong because AZ no.

link ref: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html

235.Explain
Answer A wrong because DescribeInstances no AMIs.

Answer B wrong because no DescribeAMIs.

Answer C correct because DescribeImages AMIs.

Answer D wrong because no GetAMIs.

Answer E wrong because can list.

link ref: https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_DescribeImages.html

236.Explain
Answer A correct because Customers are responsible for creating, rotating, deleting, and enforcing policies (like MFA) on their own IAM Users and Access Keys.

Answer B wrong because AWS is responsible for the physical security and decommissioning of the underlying hardware (e.g., wiping disks) used for services like EBS or S3.

Answer C correct because Customers configure these network firewalls to control inbound and outbound traffic to their EC2 instances and VPC subnets. This is a core part of network security configuration.

Answer D correct because While AWS provides the encryption service (KMS), the customer decides whether to enable encryption on their volumes and manages the associated encryption keys (Customer Managed Keys).

Answer E wrong because AWS manages the security of the physical data centers, including controlling access to servers, compute hardware, and storage racks.

Answer F correct because For IaaS (Infrastructure as a Service) like EC2, the customer is responsible for guest operating system management, including applying security patches, updates, and configuration management.

link ref: https://aws.amazon.com/compliance/shared-responsibility-model/

237.Explain
Answer A correct because smaller page less impact.

Answer B wrong because parallel more.

Answer C wrong because range no scan.

Answer D wrong because prewarm no.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html

238.Explain
Answer A wrong because SSL no.

Answer B wrong because random no.

Answer C correct because encrypted FS.

Answer D wrong because S3 no EBS.

Answer E wrong because IAM access no encrypt.

link ref: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html

239.Explain
Answer A wrong because ap-northeast-1 no.

Answer B wrong because us-west-2 no.

Answer C correct because us-east-1 default.

Answer D wrong because eu-west-1 no.

Answer E wrong because us-central-1 no.

link ref: https://docs.aws.amazon.com/sdkref/latest/guide/settings-reference.html

240.Explain
Answer A correct because SWF ensures that tasks (activities and decisions) are only assigned to one worker instance at a time, preventing duplicate processing. This is a key difference from SQS, which offers "at least once" delivery. SWF guarantees exactly-once task assignment.

Answer B wrong because SWF stores the workflow state and history internally as a managed service. It does not require an Amazon S3 bucket for its operational storage.

Answer C correct because SWF is designed for long-running, human-centric processes where state persistence is critical. A single workflow execution can run for a maximum of 365 days (one year).

Answer D wrong because SWF sends signals (tasks) to workers and deciders via polling. It does not natively use Amazon SNS for task assignment notifications; it typically uses SQS queues internally for task lists.

Answer E correct because The core architecture of SWF relies on two main components: Workers execute the activities (business logic), and Deciders coordinate the workflow, determining the next step based on the outcome of previous tasks.

Answer F wrong because SWF is a fully managed service. While your workers and deciders often run on EC2 instances, the SWF service itself and the workflow domain do not mandate the use of EC2; workers and deciders can run anywhere (e.g., Lambda, on-premises).

link ref: https://aws.amazon.com/swf/

241.Explain
Answer A wrong because While this introduces a different IP address, it is still only one IP address. The ELB would still stick all 40,000 requests to one or two targets in the same manner, but from a different region.

Answer B correct
This is the most cost-effective solution requiring no infrastructure changes.
By forcing the client (the load-testing software) to re-resolve the ELB's DNS name before each request, the load tester will receive the full list of IP addresses associated with the ELB.
Since the ELB IPs are typically spread across all configured Availability Zones (us-west-2a and us-west-2b), the tester will be directed to different ELB nodes, which, in turn, will distribute traffic more broadly across all four web servers.

Answer C correct
A globally distributed service uses many different source IP addresses from various regions.
    Because the ELB's session stickiness works on a per-client (IP) basis, using thousands of unique client IPs ensures that the traffic is naturally dispersed across all the backend targets, leading to far more even distribution. This effectively simulates real public user traffic.

Answer D wrong because The load balancing configuration is already across two zones (us-west-2a and us-west-2b). Changing the second zone doesn't solve the problem of stickiness and the single client IP.

Answer E wrong because This is only a change in the type of cookie used (Application vs. AWS-generated). The ELB would still see one client and continue to apply stickiness, sending all traffic to the same subset of targets. The problem is the stickiness itself, not the cookie type.

link ref: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html

242.Explain
Answer A correct because HTTP.

Answer B wrong because UDP no.

Answer C correct because SMS.

Answer D wrong because DynamoDB no.

Answer E wrong because pipes no.

link ref: https://aws.amazon.com/sns/features/

243.Explain
Answer A wrong because This attempts to circumvent the 400 KB item size limit by splitting the data, but it is highly inefficient and complex. It requires multiple write/read operations (multiplying WCU/RCU consumption) and complex application logic for reassembly, leading to very high costs and latency.

Answer B wrong because This is only slightly better than the previous option but still involves storing the large image data in DynamoDB. The large size of the image item will still consume a high amount of RCU/WCU in the new Images table, just shifting the throughput burden to a different table.

Answer C wrong because Images are large binary data. DynamoDB items have a 400 KB hard limit (and most images exceed this). Even if the image is small enough (e.g., compressed thumbnail), its size directly translates to consumed capacity. A 100 KB image would consume 100 WCUs per write ($100 \text{ KB} / 1 \text{ KB}$) and 25 RCUs per strongly consistent read ($100 \text{ KB} / 4 \text{ KB}$), significantly inflating the required provisioned throughput.

Answer D correct
Lowest Throughput Impact: By storing only a small S3 URL string (metadata) in the Product table item, the overall item size remains small.
Capacity Unit Calculation: DynamoDB capacity is consumed in 1 KB chunks for writes and 4 KB chunks for reads. A small URL will consume the minimum required capacity (1 WCU for writes, 0.5 or 1 RCU for reads, depending on consistency).
Decoupling: The heavy throughput operation of fetching the large image is completely offloaded to Amazon S3, which is designed for high throughput object retrieval and does not use the DynamoDB provisioned throughput capacity.

link ref: https://aws.amazon.com/dynamodb/

244.Explain
Answer A wrong because Not a Limit. "Hash key" is synonymous with Partition Key. There is no practical limit on the number of unique partition key values you can use across a table or account. The limit applies to the length of the key, not the count.

Answer B wrong because Practically Unlimited. DynamoDB scales automatically to accommodate massive data sizes, and there is no predefined storage limit for the total amount of data you can store in a table or account (Source 1.1).

Answer C correct The default limit is typically 2,500 tables per AWS Region for each account (Source 3.5), but this is a soft limit that can be increased by submitting a service quota increase request to AWS.

Answer D wrong because Hard Limit. There is a hard limit of 5 Local Secondary Indexes (LSIs) per table (Source 1.5, 3.6). Hard limits cannot be raised by AWS support.

Answer E correct This refers to the total number of Read Capacity Units (RCUs) and Write Capacity Units (WCUs) that can be provisioned across all DynamoDB tables in an AWS Region for the account. This is a soft limit that can be increased via AWS Support.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html

245.Explain
Answer A correct because The Visibility Timeout is the duration that an SQS message remains hidden from other consumers after it's retrieved. Since the task takes 5 minutes, the initial timeout must be set to at least 5 minutes (plus buffer) to prevent other consumers from retrieving and processing the message while the task is running. Deletion must happen after successful processing.
-> MINIMIZES DUPLICATES. Hides the message while processing, then confirms completion via deletion.

Answer B wrong because Deleting the message before processing is complete is risky. If processing fails after deletion, the message is permanently lost, resulting in data loss.
-> Risk of Data Loss.

Answer C wrong because DelaySeconds only applies when sending a message to the queue, instructing SQS to hide the message for a certain time before it becomes available for initial consumption. It has no effect on a message after it has been retrieved.
-> Uses Wrong SQS Parameter.

Answer D wrong because Combines the misuse of DelaySeconds with the data loss risk of deleting before processing.
-> Uses Wrong Parameter & Risk of Data Loss.

link ref: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-visibility-timeout.html

246.Explain
Answer A wrong because anonymous no.

Answer B correct because pre-signed download.

Answer C wrong because MFA no.

Answer D wrong because encryption no access.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html

247.Explain
Answer A correct because User ID even.

Answer B wrong because Status same.

Answer C wrong because Device hot.

Answer D wrong because Game few.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html

248.Explain
Answer A wrong because hash name range office sort.

Answer B correct because range name hash office query.

Answer C wrong because hash name no range.

Answer D wrong because hash office no range.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/LSI.html

249.Explain
Answer A wrong because VPC both.

Answer B correct because EBS stop/start.

Answer C wrong because ASG both.

Answer D wrong because instance-store no stop.

link ref: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/RootDeviceStorage.html

250.Explain
Answer A wrong because S3 costs.
- You pay for storage (GB-per-month), requests (PUT, GET, LIST), and data transfer out. The Free Tier includes a limited amount of storage and requests.

Answer B wrong because EC2 costs.
- You pay for instance hours (compute time), storage (EBS volumes), and data transfer. The Free Tier typically includes a limited number of hours on specific instance types (e.g., t2.micro or t3.micro).

Answer C correct because Auto Scaling free.
- No additional charge for the service. You pay only for the AWS resources (like EC2 instances or CloudWatch monitoring) that Auto Scaling launches and manages.

Answer D wrong because ELB costs.
- You pay for each Load Balancer hour and the number of Load Balancer Capacity Units (LCUs) consumed, based on traffic and resource utilization. The Free Tier includes a limited number of hours and LCUs.

Answer E correct because CloudFormation free.
- No additional charge for using CloudFormation with core AWS resources (in the AWS::* namespace). You pay only for the resources it provisions (like EC2, S3, RDS), exactly as if you launched them manually. Note: Charges can apply for third-party resource providers or custom hooks.

Answer F wrong because SWF costs.
- You pay based on the number of workflow executions and the number of tasks performed during those executions.

link ref: https://aws.amazon.com/pricing/

251.Explain
Answer A wrong because limit 5TB now, but was 5GB.

Answer B correct because multi-part for >5GB.

Answer C wrong because no large API.

Answer D wrong because no support increase.

Answer E wrong because region not limit.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html

252.Explain
Answer A correct because ASG deployed.

Answer B wrong because Route 53 separate.

Answer C correct because ELB deployed.

Answer D correct because RDS optional.

Answer E wrong because EIP manual.

Answer F wrong because SQS separate.

link ref: https://aws.amazon.com/elasticbeanstalk/

253.Explain
Answer A wrong because IAM user creds insecure.

Answer B wrong because root creds bad.

Answer C correct because web federation temp creds.

Answer D wrong because cross-account for accounts.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html

254.Explain
Answer A wrong because Perl supported.

Answer B correct because PHP supported.

Answer C wrong because Pascal not.

Answer D correct because Java supported.

Answer E wrong because SQL not SDK.

link ref: https://aws.amazon.com/tools/

255.Explain
Answer A wrong because 1 too low.

Answer B correct because 600/60=10 WCU for 1KB writes.

Answer C wrong because 60 over.

Answer D wrong because 600 over.

Answer E wrong because 3600 way over.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html

256.Explain
Answer A wrong because 5xx server errors.

Answer B wrong because 200 success.

Answer C wrong because 306 unused.

Answer D correct because 4xx client errors.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/CommonErrors.html

257.Explain
Answer A wrong because This controls the duration a message remains invisible to other consumers after being received. It prevents duplicate processing and has no effect on reducing the number of empty poll requests.

Answer B correct This enables Long Polling. When a consumer makes a ReceiveMessage request, SQS will hold the connection open for up to the specified time (max 20 seconds) until a message arrives. This significantly reduces the number of empty responses by only returning an empty response when the wait time expires.

Answer C wrong because This defines how long SQS stores messages in the queue (1 minute to 14 days) before automatically deleting them. It affects data durability, but has no effect on reducing empty poll requests.

Answer D wrong because This attribute makes a specific message invisible for a set duration after it is sent to the queue. It controls when a message becomes available for the first time, but has no effect on the frequency of empty poll requests.

link ref: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html

258.Explain
Answer A wrong because no www.

Answer B correct because s3-website-region format.

Answer C wrong because no endpoint.

Answer D wrong because no tokyo.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteEndpoints.html

259.Explain
Answer A wrong because The table size would grow by approximately $3.6$ million items (1,000 items/sec * 3,600 sec/hr) every hour, leading to massive storage costs and potential performance degradation.

Answer B wrong because Deleting $3.6$ million items requires $3.6$ million Delete API calls, which directly consumes Write Capacity Units (WCUs) and is billed. This is expensive and wastes provisioned throughput on cleanup.

Answer C correct Deleting an entire DynamoDB table is a single free control-plane operation that removes all items and associated storage instantly (from a billing perspective). This is far more efficient and cheaper than running $3.6$ million individual delete operations. The process would be: Create Table A $\rightarrow$ Write for 1 hour $\rightarrow$ Analyze $\rightarrow$ Create Table B, Delete Table A $\rightarrow$ Write for 1 hour $\rightarrow$ Analyze $\rightarrow$ Create Table C, Delete Table B, etc.

Answer D wrong because This addresses the table creation side but fails to address the deletion and cost-saving side. This would lead to 24 unused tables being retained after a day, wasting storage. (Note: Using Time to Live (TTL) would be the standard DynamoDB approach for item expiration, as it doesn't consume WCUs, but the "delete table" strategy is an even more aggressive form of partition-by-time data management often used for temporary, high-throughput data.)

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStarted.DeleteTable.html

260.Explain
Answer A wrong because Memory is local to the instance; sessions are lost if the load balancer routes to another server.

Answer B wrong because Local storage is ephemeral and tied to a single instance; not shared across servers.

Answer C wrong because EBS is block storage and not designed for fast, centralized access for multiple servers. It would work but adds high latency and complexity.

Answer D correct
The issue described is lost session state when a userâ€™s request is routed to a different web server behind the Elastic Load Balancer (ELB).

Storing session state in-memory on the instance (or on instance storage / EBS) causes session data to be local to that instance, so if the user hits a different instance, the session is lost.

ElastiCache (Redis or Memcached) provides a centralized, in-memory store for session data. All web servers can access it, preventing users from being logged out unexpectedly.

This is the standard approach for scalable, load-balanced web applications needing consistent session state.

Answer E wrong because Glacier is archival storage; not designed for fast, real-time access.

link ref: https://aws.amazon.com/elasticache/

261.Explain
Answer A wrong because EBS not for serving.

Answer B correct because signed URLs control access.

Answer C wrong because CloudFront can use signed too.

Answer D wrong because SG for EC2, not S3.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/ShareObjectPreSignedURL.html

262.Explain
Answer A wrong because pessimistic not used.

Answer B correct because optimistic control.

Answer C correct because conditional writes.

Answer D wrong because no restrict reads.

Answer E wrong because no restrict writes.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate

263.Explain
Answer A wrong because (Manual and Static) This is a manual, non-programmatic process. It is not suitable for a dynamic mobile application where tokens frequently change (e.g., when an app is reinstalled or the token expires).

Answer B wrong because (Incorrect Responsibility) The third-party Push Notification Service (PNS) like Apple (APNS) or Google (FCM) issues the token to the device, but it is AWS SNS's responsibility to register that token to create an SNS endpoint ARN. The PNS does not call SNS APIs.

Answer C wrong because (Too Generic) A Token Vending Service (TVS) is a pattern often used for issuing temporary security credentials (like STS tokens) or unique IDs, not specifically for registering external device tokens. While a custom backend is required, the term is too general and doesn't specify the necessary SNS API action.

Answer D correct because The mobile application's backend service (or an associated Lambda function) must receive the device token from the mobile app and then call the CreatePlatformEndpoint API function. This registers the unique token under the SNS Platform Application and returns an Endpoint ARN, which is required to send direct notifications to that specific device.

link ref: https://docs.aws.amazon.com/sns/latest/dg/sns-mobile-application-as-subscriber.html

264.Explain
Answer A wrong because no storage instances.

Answer B wrong because range secondary.

Answer C correct because hot hash key throttling.

Answer D wrong because sort not capacity.

Answer E wrong because no auto scaling then.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-design.html

265.Explain
Answer A wrong because sequential hotspots.

Answer B correct because instanceID first distributes.

Answer C wrong because year first less distribution.

Answer D wrong because hour first hotspots.

Answer E wrong because year hotspots.

link ref: https://aws.amazon.com/premiumsupport/knowledge-center/s3-request-limit-avoid/

266.Explain
Answer A wrong because not exactly once/FIFO.

Answer B wrong because not exactly once.

Answer C wrong because FIFO order.

Answer D correct because at-least-once, no order.
Messages are stored redundantly across multiple servers, meaning occasional duplicates can occur during consumption. The consumer application must be idempotent to handle this.
Messages are generally delivered in close to the order they were sent, but the exact FIFO order is not guaranteed due to the distributed nature of the queue.

link ref: https://aws.amazon.com/sqs/features/

267.Explain
Answer A wrong because IAM no LDAP login.

Answer B correct because LDAP to STS assume role.

Answer C wrong because STS no LDAP.

Answer D correct because broker LDAP to STS federated.

Answer E wrong because broker to STS assume.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html

268.Explain
Answer A correct because Required. The new landing page (welcome.html) must physically exist in the root of the S3 bucket so the web server can serve it when requested.

Answer B wrong because The visitor is accessing the root (http://www.companyc.com), not a subdirectory. Creating a subfolder does not change the document returned for the root URL.

Answer C correct because Required. The Index Document property in S3 Static Website Hosting settings tells S3 which file name to return when a request is made to the root domain or any subdirectory (e.g., http://www.companyc.com/). Changing this from index.html to welcome.html meets the core requirement.

Answer D wrong because This only moves the old page; it does not configure S3 to serve the new page (welcome.html) upon visiting the root URL.

Answer E wrong because The Error Document property specifies which file to return when a resource is not found (e.g., a 404 error). It is not used for the default successful landing page.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/IndexDocumentSupport.html

269.Explain
Answer A wrong because Triple DES not S3.

Answer B correct because AES-256 SSE.

Answer C wrong because Blowfish not.

Answer D wrong because RC5 not.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html

270.Explain
Answer A correct because annotations for custom data indexed.

Answer B wrong because metadata not indexed.

Answer C wrong because env vars config.

Answer D wrong because plugins SDK.

link ref: https://docs.aws.amazon.com/xray/latest/devguide/xray-concepts.html#xray-concepts-annotations

271.Explain
Answer A wrong Elastic Beanstalk does not allow you to change the load balancer type (CLB, ALB, or NLB) of an existing environment. A new environment must be created.

Answer B correct If we clone we also clone the type of load balancer. If we want an Application Load Balancer, we have to create a new environmment with the same configurations,
deploy the application that is current running on the classic load balancer then use swap-environment-cnames action to point the same url to the new ALB

Answer C wrong cloning an existing environment in the AWS Elastic Beanstalk console doesn't allow you to directly select or change the load balancer typeâ€”cloning copies the source environment's configuration, including sticking with the Classic Load Balancer if that's what the original uses.

Answer D wrong Directly editing the load balancer type configuration of an existing environment is not supported. Elastic Beanstalk requires the environment to be rebuilt (or cloned) when making changes to core infrastructure components like the load balancer type. Rebuilding an environment takes the application offline, which is undesirable for production migrations.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.elb.html

272.Explain
Answer A wrong because CodePipeline CI/CD.

Answer B wrong because S3 storage.

Answer C wrong because CodeBuild builds.

Answer D correct because CodeCommit git repo.

link ref: https://aws.amazon.com/codecommit/

273.Explain
Answer A wrong because CodePipeline orchestrates.

Answer B wrong because CodeBuild compiles.

Answer C wrong because Beanstalk PaaS.

Answer D correct because CodeDeploy deploys to EC2/on-prem.

link ref: https://aws.amazon.com/codedeploy/

274.Explain
Answer A wrong because DLQs capture errors after they occur. This step is about preventing the fundamental throttling error caused by exceeding the account-level quota.

Answer B wrong because This only defines how the function is invoked. It does not address the resource limit (concurrency quota) that will cause the function to fail under the required load.

Answer C wrong because Application-level error handling addresses code exceptions (e.g., failed database connection). The throttling error is a service quota error and cannot be resolved by code logic alone; it requires a change to the AWS account settings.

Answer D correct 
AWS Lambda functions have specific service quotas (limits) that restrict how many instances of your function can run at the same time across all functions in a single AWS account and region.
The Concurrent Execution Limit
1. Calculate Required Concurrency:
  - Required Concurrency = Average requests per second (lambda) x Average execution time(T)
  - Required Concurrency = 50 requests/second x 100 seconds = 5,000 concurrent execution
2. Default Limit: The default concurrent execution limit for AWS Lambda in most regions is 1,000 instances per account per region.
3. Preventing Errors: Since the required concurrency (5,000) is five times greater than the default limit (1,000), attempting to deploy and run the application will immediately result in throttling errors once the load reaches the 1,000 limit.
4. Action Required: To meet the requirement of 5,000 concurrent executions, the Developer must Contact AWS Support to request a limit increase for the Concurrent Execution quota for the AWS account and region.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/lambda-limits.html

275.Explain
Answer A wrong because RDS app level slow.

Answer B wrong because shared file complex.

Answer C correct because Memcached low latency.

Answer D wrong because DynamoDB higher latency.

link ref: https://aws.amazon.com/elasticache/memcached/

276.Explain
Answer A correct because GSI separate provision, underprovisioned.

Answer B wrong because read on primary ok.

Answer C wrong because streams for changes.

Answer D wrong because other table separate.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html

277.Explain
Answer A wrong because Uncontrolled simultaneous updates. This is the cause of the overwriting problem, as the last write to arrive wins, regardless of the data's state.

Answer B correct because To prevent overwriting, the developer adds an attribute (like a version number) to the item. The update request uses a ConditionExpression to ensure the write only succeeds if the stored version number matches the version the editor originally read. If another editor updated the item in between, the condition fails, and the update is rejected.

Answer C wrong because atomic Ensures an operation on a single item either succeeds entirely or fails entirely. This guarantees item-level integrity but does not check the item's content against a prior state to prevent stale data from overwriting newer changes.

Answer D wrong because batch A convenience feature to combine up to 25 PutItem or DeleteItem requests into a single network call. This is for efficiency , does not offer conditional logic to prevent concurrent overwrites.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/WorkingWithItems.html#WorkingWithItems.ConditionalUpdate

278.Explain
Answer A wrong because view type for content.

Answer B correct because event source mapping triggers.

Answer C wrong because SNS not needed.

Answer D wrong because timeout not trigger.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html

279.Explain
Answer A wrong because High-resolution is correct, but 30-second publishing is too slow â†’ cannot reflect load in last 15 seconds.

Answer B correct 
Scaling based on user load in the last 15 seconds requires CloudWatch metrics with a granularity of 1 second (high-resolution).

CloudWatch can evaluate scaling policies using the most recent datapoints, so publishing every 5 seconds ensures the system always has fresh data when making scaling decisions.

This meets the requirement of reacting within 15 seconds.

Answer C wrong because Standard resolution is 1-minute granularity â†’ CANNOT scale based on last 15 seconds + 30 sec interval is too slow.

Answer D wrong because Publishing 5-second data still becomes aggregated into 1-minute granularity â†’ scaling cannot react fast enough.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html

280.Explain
Answer A wrong because SQS/EC2 not real-time.

Answer B wrong because S3/Redshift batch.

Answer C wrong because Data Pipeline scheduled.

Answer D correct because Kinesis Streams real-time ingest.

link ref: https://aws.amazon.com/kinesis/data-streams/

281.Explain
Answer A correct because specific Create/DeleteBranch.

Answer B wrong because Put* too broad.

Answer C wrong because Update* not for branches.

Answer D wrong because * all access.

link ref: https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control-permissions-reference.html

282.Explain
Answer A wrong because ACM certs.

Answer B correct because Parameter Store secure storage.

Answer C wrong because Trusted Advisor recommendations.

Answer D correct because KMS encrypts.

Answer E wrong because GuardDuty security.

link ref: https://docs.aws.amazon.com/systems-manager/latest/userguide/parameter-store.html

283.Explain
Answer A wrong because dockerrun for ECS.

Answer B wrong because buildspec for CodeBuild.

Answer C correct because appspec for deployment hooks.

Answer D wrong because ebextensions config.

link ref: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file.html

284.Explain
Answer A wrong because Encrypt not for large.

Answer B wrong because GenerateRandom no KMS.

Answer C wrong because encrypted key not decrypt.

Answer D correct because plaintext data key from KMS.

link ref: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#data-keys

285.Explain
Answer A wrong Eliminated - there is no mention of caching being involved in the problem, this metric is irrelevant for troubleshooting a timeout issue.

Answer B correct because A high value for IntegrationLatency can indicate that the API Gateway is experiencing delays in receiving responses from Lambda.

Answer C wrong Eliminated - this metric is related to caching, which is not mentioned as part of the problem.

Answer D correct because high value for Latency can indicate where delays are occurring overall, including the Lambda function's processing time and any overhead in API Gateway.

Answer E wrong Eliminated - While this metric provides information about the volume of requests, it does not help identify the cause of a timeout or latency issues.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html

286.Explain
Answer A correct
CloudFront + signed URLs allows secure, time-limited, user-specific access to files stored in S3.
No need to create per-customer infrastructure â†’ lowest cost and easy access control.
Best practice for distributing large download files (like firmware) globally.
Customers get fast download speeds from edge locations, and URLs expire to prevent unauthorized sharing.

Answer B wrong because Complex and expensive (hundreds or thousands of distributions). Hard to manage and no added security benefit.

Answer C wrong because Possible but unnecessary and more expensive. Lambda@Edge adds cost and complexity unless custom authorization logic is needed.

Answer D wrong because Much higher cost and slower performance. API Gateway isnâ€™t optimized for large binary file delivery; CloudFront is.

link ref: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html

287.Explain
Answer A correct because backoff handles throttling.

Answer B wrong because SQS bus adds layer.

Answer C wrong because API Gateway throttles more.

Answer D wrong because Firehose for streams.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Programming.Errors.html

288.Explain
Answer A correct because ElastiCache (using Redis or Memcached) is designed for high-speed, low-latency access, making it ideal for storing session data. It is highly available and fault-tolerant. By storing sessions externally, the web servers become stateless, allowing them to scale horizontally without losing user data, thus fulfilling all requirements.

Answer B wrong because CloudFront is used for caching static content and improving delivery speed globally. It is not designed as a persistent, centralized session state store.

Answer C wrong because S3 is optimized for durable storage and large objects. The latency (speed of access) is too high for the millisecond-level reads and writes required by session management in a high-traffic web application.

Answer D wrong because Session stickiness routes a user's requests to the same server for a duration. This limits true horizontal scaling, reduces fault tolerance (if that server fails, the session is lost), and often leads to an imbalanced load, making it unsuitable for a high-availability, highly scalable application.

link ref: https://aws.amazon.com/elasticache/

289.Explain
Answer A wrong because intrinsic for functions.

Answer B wrong because express framework.

Answer C correct because SAM model for serverless.

Answer D wrong because plugin not.

link ref: https://aws.amazon.com/serverless/sam/

290.Explain
Answer A wrong because pessimistic DynamoDB no.

Answer B wrong because CloudFront/ASG not session.

Answer C wrong because WAF security.

Answer D correct because DynamoDB external sessions.

Answer E correct because ELB/ASG for elasticity.

link ref: https://aws.amazon.com/elasticloadbalancing/

291.Explain
Answer A wrong because logging basic.

Answer B wrong because CloudTrail API.

Answer C correct because X-Ray traces distributed.

Answer D wrong because Inspector security.

link ref: https://aws.amazon.com/xray/

292.Explain
Answer A wrong because VPC endpoints facilitate private access to AWS services (like CloudWatch) from within a VPC. They are not required for the core function of metric filtering or searching logs.
-> Network configuration does not affect the filtering logic.

Answer B correct because Metric filters are not retroactive. A CloudWatch Logs metric filter defines a pattern to search for in incoming log events. When an event matches the pattern, a data point is sent to the associated CloudWatch Metric. It does not process or generate metrics for logs that were already stored before the filter was defined.
-> The filter will only start counting exceptions from the moment it is created onwards.

Answer C wrong because Streaming to Amazon OpenSearch Service (formerly Elasticsearch) is an optional step for advanced log analytics and searching. CloudWatch can filter and create metrics directly from the logs it stores without any external service.
-> Filtering can be done natively within CloudWatch Logs.

Answer D wrong because Exporting logs to S3 is typically done for archival or large-scale, offline analysis. Filtering and generating metrics are native features that occur before or instead of exporting to S3.
-> Exporting logs is not a prerequisite for metric filtering.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html

293.Explain
Answer A wrong because Optional. Used for logic, not for enabling SAM processing.
Used for conditional creation of resources during stack deployment.

Answer B wrong because Optional. Used for configuration, not for enabling SAM processing.
Defines properties that are inherited by all resources of a specific type (e.g., all AWS::Serverless::Function resources).  

Answer C correct because Transform for SAM.
MUST be included. This is the essential differentiator that enables SAM resource types (like AWS::Serverless::Function).
Specifies the serverless application model. It tells CloudFormation to pre-process the template using the SAM specification before deployment. For SAM templates, this value must be set to AWS::Serverless-2016-10-31.

Answer D wrong because Standard, but not a document root section. Properties are nested under individual resources, not at the document root level.
Defines configuration values for a specific resource type within the Resources section.

link ref: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification.html

294.Explain
Answer A wrong because Multi-AZ HA.

Answer B wrong because SQS messaging.

Answer C correct because ElastiCache caches queries.

Answer D wrong because replica scales reads.

link ref: https://aws.amazon.com/elasticache/

295.Explain
Answer A wrong because This would penalize all Lambda functions in the account by lowering the total available concurrency pool. It addresses the symptom by lowering the ceiling, but doesn't solve the core problem of Lambda 2 consuming the pool whenever needed.

Answer B wrong because This only addresses load distribution at the API Gateway level. It does not increase the underlying AWS Lambda account concurrency limit or prevent Lambda 2 from monopolizing the existing limit. Lambda 1 would still be throttled.

Answer C correct because This is the correct solution using Reserved Concurrency. By setting a specific, lower limit on Lambda 2, the Developer prevents it from consuming the entire account capacity. This guarantees that the remaining capacity (Unreserved Concurrency) is always available for other functions, including the one backing /MyAPI (Lambda 1).

Answer D wrong because This would cause API Gateway to reject incoming requests before they even get to the Lambda service. While it reduces load, it doesn't fix the root problem of Lambda 2 monopolizing resources and shifts the throttling point to the API Gateway level.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/configuration-concurrency.html

296.Explain
Answer A correct because Redis cluster is the best solution because it is a managed, highly available, in-memory key-value store specifically designed for low-latency session management.

Fault Tolerance: Redis clusters support replication and sharding across multiple Availability Zones (AZs). If one Redis node fails, the data is preserved on the replicas, ensuring no loss of user session data.

Downtime Reduction: By externalizing the session data, the web application instances become stateless. If an EC2 instance fails, the ELB routes the next request to a healthy instance, which can immediately retrieve the user's session from ElastiCache, resulting in zero downtime or session loss for the user.

Answer B wrong because An EBS volume is block storage that is typically attached to a single EC2 instance at a time. It cannot be shared reliably and simultaneously by multiple servers to serve session data, making it unsuitable for horizontal scaling and fault tolerance.

Answer C wrong because This is the current, failed approach (local storage). This data is lost when the server is terminated or fails, directly violating the fault tolerance requirement.

Answer D wrong because While this moves the data off the web servers, it requires the company to manually manage the database, replication, clustering, backup, and failover (high operational overhead). Amazon ElastiCache is a fully managed service that handles this complexity, making it the most effective (managed) solution.

link ref: https://aws.amazon.com/elasticache/redis/

297.Explain
Answer A wrong because JSON is a supported format, but YAML is also fully supported for .ebextensions configuration files. The format is not the primary error; the file name is.

Answer B correct because Elastic Beanstalk only recognizes configuration files located in the .ebextensions directory if they have the file extension .config (e.g., healthcheckurl.config). The current file name, healthcheckurl.yaml, will be ignored, meaning the health check URL option is never applied.

Answer C wrong because The option_settings section is the correct place to set environment and configuration options like the health check URL. The resources section is used to declare and configure AWS resources (like databases or S3 buckets) that the environment needs.

Answer D wrong because The namespace aws:elasticbeanstalk:application is the standard and correct namespace for setting the application-wide health check URL. Changing it to a custom namespace would prevent Elastic Beanstalk from recognizing and applying the setting.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html

298.Explain
Answer A wrong because instance size not for Lambda.

Answer B wrong because time increase not compute.

Answer C wrong because call-time specify not possible.

Answer D correct because more memory increases CPU proportionally.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/resource-model.html

299.Explain
Answer A wrong because async Lambda still waits for login.

Answer B correct because Memcached caches user data, speeding login.

Answer C wrong because ALB balances, but bottleneck is DB.

Answer D correct because async DB call doesn't block.

Answer E wrong because batch increases latency.

link ref: https://aws.amazon.com/elasticache/memcached/

300.Explain
Answer A wrong because It enables the use of Cognito Sync but is the authorization layer, not the synchronization mechanism itself. Using the Identity Pool alone does not solve the data push requirement.

Answer B wrong because It does not offer a built-in mechanism to automatically push updates to all devices after initial sign-in for frequently changing data. While you can update attributes, the push sync feature belongs to Cognito Sync.

Answer C correct Amazon Cognito Sync (part of the Identity Pools feature) is a managed AWS service designed specifically for this use case.
It allows you to store and synchronize application-related user data (like user preferences, game state, or small user-defined profile attributes) as key-value pairs in datasets.
It provides client libraries that automatically handle local data caching, offline data storage, and synchronization when the device is online.
Crucially, you can enable Push Synchronization (Push Sync), which uses Amazon SNS to send a silent push notification to all devices associated with the identity whenever the data changes in the cloud, prompting the devices to synchronize the update immediately.
This is entirely serverless and requires no custom backend (Lambda, API Gateway, etc.) for the synchronization logic.

Answer D wrong because This requires you to write and manage a custom backend Lambda function to handle the event and potentially notify other services, which contradicts the requirement to "not want to manage a back end."

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-sync.html

301.Explain
Answer A wrong **`createDeployment`** is used to apply changes to the API configuration (like resources, methods, integrations), not to activate API keys.

Answer B wrong **`updateAuthorizer`** manages authentication logic (e.g., Lambda Authorizers, Cognito User Pools), but API key validation is separate from authorization.

Answer C wrong **`importApiKeys`** is used for bulk importing keys, not for activating a single key in a Usage Plan.

Answer D correct The **`createUsagePlanKey`** method is necessary to link the new API key to the active **Usage Plan**. Once linked, the key is recognized by the API stage and allowed to access the service, resolving the `403 Forbidden` error.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html

302.Explain
Answer A correct because Cognito web federation for social logins.

Answer B wrong because SAML for enterprise.

Answer C wrong because keys in code insecure.

Answer D wrong because STS assume for roles.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/amazon-cognito-integrating-amazon-cognito-with-web-and-mobile-apps.html

303.Explain
Answer A wrong because GetMetricData retrieves metrics, not raw log data. Logs must be first sent to CloudWatch Logs.

Answer B wrong because CloudTrail records AWS API calls, not application logs from EC2.

Answer C wrong because Unnecessary. CloudWatch Events cannot collect logs, and spinning up a new EC2 is not required for monitoring existing application logs.

Answer D correct
The CloudWatch Logs agent collects log files from the EC2 instance and pushes them to CloudWatch Logs.

Once in CloudWatch Logs, administrators can:

Monitor application logs

Create metrics from log data

Set alarms for specific events or errors

This is the standard method to make EC2 application logs available for monitoring.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/QuickStartEC2Instance.html

304.Explain
Answer A wrong because DeleteItem conditional slow for millions.

Answer B wrong because BatchWriteItem limited.

Answer C wrong because recursive slow.

Answer D correct because recreate table daily efficient for temp data.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GettingStarted.CreateTable.html

305.Explain
Answer A wrong because bucket name not cause duplicates.

Answer B correct because Lambda retries on failure, causing duplicates.

Answer C wrong because no S3 outage mentioned.

Answer D wrong because intermittent stop not cause log duplicates.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/invocation-retries.html

306.Explain
Answer A wrong because Lambda executes the business logic of a service; it is not an interface management or routing layer.

Answer B wrong because X-Ray is a monitoring/observability tool; it has no role in managing or refactoring the application's interface or architecture.

Answer C wrong because SQS is an asynchronous communication mechanism. It does not provide a single, synchronous HTTP interface for consumers to connect to the services.

Answer D correct because It provides the required single, unified HTTP interface, abstracting the complex backend microservices from consumers, solving manageability and scaling issues.

link ref: https://aws.amazon.com/api-gateway/

307.Explain
Answer A correct because S3 with CloudFront for static content.

Answer B wrong because EC2 self-managed.

Answer C wrong because ECS/Redis container/dynamic.

Answer D wrong because Lambda/API for dynamic.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html

308.Explain
Answer A correct
Amazon CloudWatch organizes metrics using namespaces and metric names.

A custom namespace allows you to group metrics for your applications separate from AWS default metrics.

By defining one unique metric per application in the custom namespace, you can create a single CloudWatch dashboard that displays all metrics graphically on one screen.

This approach provides a clear, organized view of key performance indicators for all applications.

Answer B wrong because Dimensions are used to filter or slice metrics; they do not define new metric names or group metrics for dashboards.

Answer C wrong because Events track discrete occurrences, not continuous performance metrics. They cannot directly feed graphical dashboards.

Answer D wrong because Alarms monitor thresholds for metrics and trigger actions; they do not provide graphical visualization by themselves.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html

309.Explain
Answer A wrong because STS temp, but user pools for passwords.

Answer B wrong because IAM not for password reset.

Answer C wrong because KMS encryption.

Answer D correct because user pools handle sign-up and reset.

link ref: https://aws.amazon.com/cognito/

310.Explain
Answer A wrong because **AWS CodeCommit** is a source control service (Git repository); it stores code but does not manage the deployment workflow.

Answer B wrong because **AWS CodeBuild** is a continuous integration service that compiles code and runs tests; it's a **step** in the deployment process but not the orchestration mechanism for sequencing deployments across environments.

Answer C wrong because **AWS Data Pipeline** is used for automating the movement and transformation of data, not for code deployment and application lifecycle management. This choice is conceptually wrong for this scenario.

Answer D correct because **AWS CodeDeploy** is a fully managed deployment service. To meet the requirement of a phased, sequential rollout:
    1. A developer creates a single **CodeDeploy Application**.
    2. Within that application, they define three separate **Deployment Groups** (e.g., Development, QA, Production), each pointing to the instances/environments for that stage.
    3. The overall workflow (the sequence of deploying to Development -> QA -> Production) is then managed by an orchestration tool like **AWS CodePipeline**, which integrates CodeDeploy. However, among the choices provided, **CodeDeploy** is the service that contains the necessary organizational structure (Deployment Groups) to define these distinct targets.
    *Self-Correction/Clarification*: While **AWS CodePipeline** is the ultimate orchestrator for the entire CI/CD sequence, among the choices given, **CodeDeploy** provides the direct mechanism to target the different environments (Deployment Groups) in the sequence specified by the CI/CD pipeline.

link ref: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployment-groups.html

311.Explain
Answer A wrong because new table per date not minimal cost.

Answer B wrong because increasing units costly during spikes.

Answer C correct because random suffix even distribution.

Answer D wrong because GSI for queries, not writes.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-uniform-load.html

312.Explain
Answer A wrong because S3 not in-memory.

Answer B wrong because RDS relational.

Answer C correct because ElastiCache in-memory for consistent results.

Answer D wrong because Kinesis streaming.

link ref: https://aws.amazon.com/elasticache/

313.Explain
Answer A correct
When using Elastic Beanstalk with a multi-container Docker environment, Elastic Beanstalk internally launches a managed Amazon ECS cluster.
To know which containers to run, their images, ports, links, and volumes, EB requires a task definition (Dockerrun.aws.json v2).
So the environment cannot start without an ECS task definition.

Answer B wrong because EB automatically creates and manages its own ECS cluster. You don't need to create one manually.

Answer C wrong because A Dockerfile is only needed for single-container Docker environments. Multi-container uses Dockerrun.aws.json (task definition), not Dockerfile.

Answer D wrong because The EB CLI is optional for deployment convenience, but not required for configuring container instances. You can deploy via console or CI/CD.

link ref: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html

314.Explain
Answer A wrong because Credentials are still stored somewhere â†’ must be rotated & protected manually â†’ higher management overhead and less secure than instance profiles.

Answer B correct 
An EC2 instance profile allows the instance to receive temporary credentials automatically via the AWS metadata service.

No hard-coded credentials â†’ MOST secure

No human rotation or maintenance â†’ MINIMAL overhead

IAM permissions can be tightly scoped for least privilege.

Answer C wrong because Extremely unsafe. Root has full access â†’ never used by applications. Violates AWS security best practices.

Answer D wrong because CodeCommit is a code repository, not a credential manager. Storing secrets in source control is a severe security risk.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html

315.Explain
Answer A wrong because RDS queryable but not for metrics.

Answer B wrong because X-Ray with Lambda for tracing.

Answer C wrong because Kinesis/DynamoDB complex.

Answer D correct because CloudWatch custom metrics with alarms cost-effective.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html

316.Explain
Answer A wrong
Downtime Tolerance: High. ðŸ”´
Rollback Mechanism: Slow (Requires building new environment).
 - All instances are updated simultaneously, leading to downtime and a slow recovery if the deployment fails.

Answer B wrong
Downtime Tolerance: Low. ðŸŸ¡
Rollback Mechanism: Slow (Manual reversal/rebuild).
 - Updates a batch of instances at a time. It minimizes downtime but leaves the environment in a mixed state (old and new code running simultaneously), which is risky. Rollbacks are slow as they require a second rolling deployment or rebuild.

Answer C wrong
Downtime Tolerance: N/A (Not a deployment strategy).
Rollback Mechanism: N/A 
 - Snapshots refer to saving the state of a resource (like an RDS database) and are not an Elastic Beanstalk deployment type.

Answer D correct because immutable no outage, quick rollback.
Downtime Tolerance: Zero. ðŸŸ¢
Rollback Mechanism: Fast (Traffic is instantly redirected).
 - Immutable deployment builds a completely new, separate Auto Scaling Group with the new version. Once the new instances pass health checks, the load balancer is switched. If the deployment fails, the new instances are simply terminated, and traffic remains on the old, working environment, ensuring zero downtime and instant rollback.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html

317.Explain
Answer A wrong because S3 is a secure, durable object storage service intended for storing files and objects, not for fast, low-latency key-value or in-memory caching typically required for relational database query results.

Answer B wrong because CloudFront is a Content Delivery Network (CDN), primarily used to cache and deliver static and dynamic web content (like HTML, images, API responses) globally from edge locations. It is not designed to directly cache database query results from an RDS instance.

Answer C wrong because This involves manual setup and management of a caching solution on each application server (e.g., using a library or an in-memory store like local Redis). This is complex, difficult to scale, and introduces data synchronization challenges between multiple EC2 instances, making it far less efficient than a centralized, managed service like ElastiCache.

Answer D correct ElastiCache (using Redis or Memcached) is an in-memory caching service specifically designed to handle high volumes of read traffic for frequently accessed data, like repeatedly accessed items. It significantly reduces the load on the primary RDS database, is highly performant, and is fully managed by AWS, making it the most efficient and standard solution for this problem.

link ref: https://aws.amazon.com/elasticache/

318.Explain
Answer A wrong because 3 RCU for 3 items, but strong consistent 2x.

Answer B correct because 3 items * 5KB = 15KB, strong read 2 RCU per 4KB, so 6 RCU.

Answer C wrong because write 70 for 10 items * 7KB.

Answer D wrong because read 3, write 10 is wrong.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html

319.Explain
Answer A wrong because pointer advance needed.

Answer B wrong because event source sync.

Answer C correct because unhandled error causes retry.

Answer D wrong because keeping up not issue.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html

320.Explain
Answer A wrong Associating an IAM role provides authorization to *AWS services*, but it does not provide the *credentials* (username/password) required to connect to a traditional database running on EC2.

Answer B correct **AWS Systems Manager Parameter Store** is a managed service designed for secure configuration and secret management. Using the **SecureString** data type ensures the secret is encrypted using KMS. When the secret is rotated in Parameter Store, the application retrieves the new value via the Parameter Store API. Since the application code only calls the API, there are **no required code changes** when the secret value changes, and the secrets are **never stored plaintext** on the instance or in the code, making this the **SAFEST** approach.

Answer C wrong Storing secrets in S3 object metadata is not a secure or recommended practice for database credentials and lacks native rotation features.

Answer D wrong Hardcoding secrets is the **least safe** method and requires a code change and full redeployment every time the secret is rotated, violating the requirement.

link ref: https://aws.amazon.com/systems-manager/parameter-store/

321.Explain
Answer A wrong because another function duplicate.

Answer B correct because update-function-code updates code.

Answer C wrong because remove not needed.

Answer D wrong because alias for versions.

link ref: https://docs.aws.amazon.com/cli/latest/reference/lambda/update-function-code.html

322.Explain
Answer A correct because private subnet for VPC access.

Answer B wrong because NACL outbound default.

Answer C correct because NAT for internet from private.

Answer D wrong because public subnet no DB access.

Answer E wrong because env var not for access.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html

323.Explain
Answer A wrong Deleting and re-uploading the ZIP file to S3 with a different object key would require updating the CloudFormation template/stack, making this approach overly complex. If the developer overwrote the existing ZIP file on S3, Lambda would still be pointing to the previous state unless instructed to update.

Answer B correct When a Lambda function is managed by an **AWS CloudFormation stack**, the stack explicitly defines the source code location using the `Code` property, which includes `S3Bucket`, `S3Key`, and optionally `S3ObjectVersion`. Even if the developer **manually overwrites** the `.ZIP` file in S3, **CloudFormation does not automatically know** that the code has changed. To deploy the new code, the developer must trigger a stack update, typically by updating one of these properties. The simplest way to force CloudFormation to recognize the change (if the `S3Key` hasn't changed) is to update the **`S3ObjectVersion`** property or change the `S3Key` itself (often done automatically via the `aws cloudformation package` or `sam package` commands, but required manually in this scenario). This update tells CloudFormation to trigger the Lambda update process.

Answer C wrong The deployment package (`.ZIP` file) is uploaded directly to S3; it does not need to be base64-encoded

Answer D wrong The function's execution role already has permissions to *read* the code during the initial deployment. If the function could be invoked but the code was old, the issue is with the deployment trigger, not the role's S3 read permissions.

link ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html

324.Explain
Answer A correct In Amazon ECS, the most common and robust pattern is to use the **sidecar container model**. This means deploying a separate **Docker image that runs the X-Ray daemon** alongside your application container within the same ECS Task Definition. This container collects trace data and relays it to the X-Ray service. 

Answer B correct For X-Ray to track calls and services, the application code itself must be modified. This involves **adding instrumentation** (using the **X-Ray SDK**) to record metadata about incoming requests and outgoing calls (to AWS services, external APIs, etc.) and send trace data to the X-Ray daemon.

Answer C wrong While possible, installing the daemon directly on the underlying EC2 instance is the older, less flexible pattern for ECS; the sidecar container (Option A) is the modern best practice.

Answer D wrong While an EC2 instance role is needed for *classic* EC2-based IAM access, the **Task IAM Role (Option F)** is the correct and more granular method for granting permissions to containers in ECS.

Answer E wrong because register app not needed.

Answer F correct The X-Ray daemon container needs permissions to communicate with the X-Ray service. The daemon runs within the context of the ECS Task, so the **IAM role for tasks** (Task IAM Role) must be configured with the necessary permissions (specifically `xray:PutTraceSegments` and `xray:PutTelemetryRecords`) to allow the daemon to upload trace data to AWS X-Ray.

link ref: https://docs.aws.amazon.com/xray/latest/devguide/xray-daemon-ecs.html
https://docs.aws.amazon.com/xray/latest/devguide/scorekeep-ecs.html

325.Explain
Answer A wrong because acl condition not for encryption.

Answer B wrong because RDS not for S3.

Answer C wrong because SecureTransport for transit.

Answer D correct because default SSE-S3 encrypts at rest.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html

326.Explain
Answer A wrong because Parameters inputs.

Answer B wrong because Outputs results.

Answer C correct because Mappings for region-specific AMIs.

Answer D wrong because Resources define.

link ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/mappings-section-structure.html

327.Explain
Answer A wrong because scan consumes more.

Answer B wrong because strong consistent more RCU.

Answer C correct because query eventual minimal RCU.

Answer D wrong because scan strong more.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Query.html

328.Explain
Answer A wrong because This forces a configuration update but does not guarantee existing cached objects are immediately removed. It's an unnecessarily destructive and complex action that still relies on the cache's Time-to-Live (TTL) to expire.

Answer B wrong because This changes how content is cached (e.g., whether page.html?v=1 is cached separately from page.html?v=2), but it does not clear the existing stale content that users are currently seeing.

Answer C correct. Invalidation is the explicit mechanism provided by CloudFront to tell the edge locations to immediately delete or mark as expired the specified objects (e.g., /index.html or /* for everything). The next request for those objects will force CloudFront to fetch the newest version from the origin.

Answer D wrong because Disabling and re-enabling a distribution is a lengthy configuration process that causes service downtime and is not the proper way to clear caches. This is inefficient and impacts the user experience severely.

link ref: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html

329.Explain
Answer A wrong AWS CloudFormation cannot directly pull code from an AWS CodeCommit repository during stack creation for an `AWS::Lambda::Function` resource. This would require an intermediary like CodePipeline or CodeBuild.

Answer B correct The `AWS::Lambda::Function` resource supports an inline code property, `ZipFile`, where you can **write the function code directly inside the CloudFormation template**. This method is suitable for small, simple functions.

Answer C correct This is the standard and most common method for deploying larger Lambda functions. The **deployment package (.ZIP file)** containing the function code and dependencies is **uploaded to an Amazon S3 bucket**. The `AWS::Lambda::Function` resource in the CloudFormation template then references the code location using the **`S3Bucket` and `S3Key`** properties. Tools like AWS SAM or `aws cloudformation package` automate this process.

Answer D wrong You upload the `.ZIP` file to **Amazon S3** (Option C), not directly to AWS CloudFormation. CloudFormation uses S3 as the source repository for deployment artifacts.

Answer E wrong Similar to CodeCommit, CloudFormation cannot directly pull code from an external private Git repository for the Lambda function resource.

link ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-code.html

330.Explain
When deploying an AWS Lambda function, the execution environment needs access to all required code and dependencies.

Answer A wrong because Lambda functions cannot directly reference and load code or libraries hosted externally on Amazon S3 during execution time; the necessary code must be included in the deployment package or a Lambda Layer.

Answer B correct because For custom libraries and dependencies (especially those not available in the standard runtime environment), the developer must **install them locally** (e.g., using `npm install` or `pip install` with a local target directory) and then **bundle the entire folder structure** (including the function code and the library files) into a **$\text{ZIP}$ file** for upload. When the function executes, the runtime environment finds the library within the deployed package.

Answer C wrong because Lambda blueprints are starter templates and do not automatically include custom, proprietary, or specific third-party libraries.

Answer D wrong because You cannot modify the core function runtime (e.g., Python, Node.js) provided by AWS to include custom libraries; you must package the libraries with your code or use Lambda Layers.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/python-package.html

331.Explain
Answer A wrong because SSL Pass-through means the ELB forwards the encrypted traffic directly to the EC2 instances without decrypting it. The EC2 instances must then perform the decryption, which increases the CPU load, violating the constraint.

Answer B correct because Required. This step is necessary to allow the ELB to encrypt/decrypt traffic. The certificate must be installed on the ELB itself.

Answer C wrong because This is not a standard or valid configuration option for an ELB related to SSL/TLS offloading.

Answer D wrong because If the certificates are installed on the EC2 instances, the traffic must be configured for SSL Passthrough or SSL Termination on the EC2 instance. If the EC2 instance handles the decryption, the CPU load will increase, violating the constraint.

Answer E correct because Required. SSL Termination means the ELB handles the entire process of decrypting incoming traffic from the client. The traffic is then sent unencrypted (usually over HTTP) to the backend EC2 instances.

link ref: https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-create-https-ssl-load-balancer.html

332.Explain
Answer A correct because zip with all libs.

Answer B wrong because runtime script increases time.

Answer C wrong because S3 env var for path, but not standard.

Answer D wrong because buildspec for CodeBuild.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/python-package.html

333.Explain
Answer A wrong because sticky for same instance.

Answer B wrong because SQS not for sessions.

Answer C correct because DynamoDB scalable for sessions.

Answer D wrong because draining for termination.

link ref: https://aws.amazon.com/blogs/aws/elastic-load-balancer-support-for-amazon-dynamodb/

334.Explain
Answer A wrong because Glue ETL not real-time. Glue ETL jobs are typically run periodically (e.g., hourly or daily) and are better suited for large-scale data warehousing transformations. This approach is not near-real time and adds latency and operational complexity (managing the schedule and job execution).

Answer B wrong data flow and complexity. ElastiCache is a caching service, not a persistent database for a core service like Payments. This pattern introduces significant complexity by requiring custom application logic (triggers) in the Accounts service to manage cache invalidation, which violates the goal of decoupling the services.

Answer C wrong because Firehose delivery, not updates.Kinesis Data Firehose is designed to capture, transform, and load streaming data into destinations like S3 or Redshift for analytics. While you could use it as part of a pipeline, it is less suited than DynamoDB Streams for the primary goal of item-level replication between two DynamoDB tables. DynamoDB Streams is the native, simpler source for this specific task.

Answer D correct because DynamoDB Streams is a built-in feature that captures a time-ordered, guaranteed sequence of item-level modifications (inserts, updates, deletes) in near-real time. You simply enable it on the Accounts table and configure an AWS Lambda function to read the stream and write the relevant changes to the Payments table. This creates a clean, decoupled event-driven architecture.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html

335.Explain
Answer A wrong because Subversion centralized.

Answer B wrong because CodeBuild builds.

Answer C correct because CodeCommit distributed git.

Answer D wrong because CodeStar projects.

link ref: https://aws.amazon.com/codecommit/

336.Explain
Answer A wrong because Requires a server â†’ not serverless, extra cost, operational overhead, and single point of failure.

Answer B wrong because Lambda environment variables do not control scheduling. They simply store configuration values.

Answer C correct because CloudWatch Events (now Amazon EventBridge) supports scheduled rules using rate or cron expressions.

This is fully serverless, automated, managed, and cost-efficient â†’ perfect for triggering Lambda.

Answer D wrong because SNS cannot schedule messages on a timer. It only pushes messages upon publish events â€” no built-in scheduling capability.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/services-cloudwatchevents.html

337.Explain
Answer A wrong because A Query operation on the base table requires the Partition Key (user_id). This would only return all sports for one user, which doesn't help build a leaderboard by sport.

Answer B correct because The GSI will efficiently fetch all items for a single sport (using the PK sport_name) and, crucially, because score is the sort key, the results will be returned pre-sorted by score.

A simple Query request with ScanIndexForward=false (descending order) returns the leaderboard (top performers) directly and efficiently.

Answer C wrong because A Scan reads every single item in the entire table and then filters the results. This consumes massive read capacity units (RCUs) and is extremely slow for large tables.

Answer D wrong because A Local Secondary Index (LSI) must use the same Partition Key as the base table (user_id). It cannot be defined with a different partition key (sport_name), making this index configuration impossible.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html

338.Explain
Answer A wrong because provider for auth.

Answer B wrong because Lambda user creation complex.

Answer C wrong because KMS for encryption.

Answer D correct because Cognito unauth roles for limited access.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/iam-roles.html

339.Explain
To allow an application on an EC2 instance to read objects encrypted with **SSE-KMS**, the application's identity must be granted permission on both the identity level (IAM) and the resource level (KMS Key Policy).

1.  **IAM Policy (Identity):** The application runs with the permissions defined in the **IAM role** attached to the EC2 instance. This IAM role must have a policy that explicitly grants the actions **`kms:Decrypt`** and **`kms:GenerateDataKey`** on the target KMS key.
2.  **KMS Key Policy (Resource):** The KMS **Key Policy** must allow the IAM role (or the root account that manages the role) to be granted these permissions. This is the master access control list for the key. Both policies must explicitly allow the action for the request to succeed.

Answer A wrong An **S3 bucket policy** controls access to the S3 data, but it does not grant the EC2 instance's IAM role the permission to use the KMS key, which is the missing cryptographic permission.

Answer B correct **Grant access to the key in the IAM EC2 role attached to the application's EC2 instances.** This provides the necessary **`kms:Decrypt`** permission at the identity level.

Answer C correct **Write a key policy that enables IAM policies to grant access to the key.** This ensures the KMS key's resource policy permits the role (from Option B) to use the key.

Answer D wrong S3 ACLs are for basic object/bucket permissions and cannot be used for KMS authorization.

Answer E wrong Parameter Store is for secure storage; it does not grant permissions to use a key.

link ref: https://docs.aws.amazon.com/kms/latest/developerguide/iam-policies.html#iam-policy-example-s3

340.Explain
Answer A correct because delay hides on add.

Answer B wrong because after consume not delay.

Answer C wrong because poll time consumer side.

Answer D wrong because delete delay not exist.

link ref: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html

341.Explain
Answer A wrong because Developers still must upload the full bundle each time â†’ slow uploads. Requires manual S3 management.

Answer B wrong because FTP does not integrate with Elastic Beanstalk, adds administrative overhead, and is not secure or scalable.

Answer C correct
Elastic Beanstalk deployments can be slow when Developers around the world upload large application bundles directly from their laptops because of:
Limited and varying internet connectivity
Uploading the full app bundle every time

Using AWS CodeCommit solves this with minimal effort:
Developers push incremental code changes (fast, small uploads)
Elastic Beanstalk can deploy directly from CodeCommit
No need to configure or manage external servers
No need to repeatedly upload full bundles from slow networks
This results in faster deployments and lower upload times with least admin overhead.

Answer D wrong because Requires server setup + maintenance + security. Upload still depends on internet speed, and introduces single-point-of-failure.

link ref: https://aws.amazon.com/codecommit/

342.Explain
Answer A wrong because EMR big data.

Answer B correct because DAX caches DynamoDB reads.

Answer C wrong because SQS messaging.

Answer D wrong because CloudFront CDN.

link ref: https://aws.amazon.com/dynamodb/dax/

343.Explain
Answer A wrong because default SSE-S3 not client-side.

Answer B wrong because Cognito auth, not encryption.

Answer C wrong because Lambda for processing.

Answer D correct because client-side with KMS secure transmission/storage.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingClientSideEncryption.html

344.Explain
Answer A wrong because role AROA... is assumed role.

Answer B wrong because default role not used.

Answer C correct because ASge... is access key of principal.

Answer D wrong because account owns service.

link ref: https://docs.aws.amazon.com/STS/latest/APIReference/API_GetCallerIdentity.html

345.Explain
Answer A correct because pagination handles large lists.

Answer B wrong because shorthand syntax for input.

Answer C wrong because parameters for commands.

Answer D wrong because quoting for args.

link ref: https://docs.aws.amazon.com/cli/latest/userguide/cli-usage-pagination.html

346.Explain
Answer A wrong because security groups for network.

Answer B wrong because ECR for images.

Answer C wrong because agent runs containers.

Answer D correct because task definition defines ports.

link ref: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definition_parameters.html

347.Explain
Answer A correct because DynamoDB NoSQL for metadata indexing.

Answer B wrong because EC2 not storage.

Answer C wrong because Lambda compute.

Answer D wrong because RDS relational slower for indexing.

link ref: https://aws.amazon.com/dynamodb/

348.Explain
Answer A wrong because VPC Flow for network.

Answer B correct because CloudWatch Logs for app logs.

Answer C wrong because CloudSearch for search.

Answer D wrong because CloudTrail for API.

link ref: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html

349.Explain
Answer A wrong because KPL improves producer performance by batching and retrying, but does **not** increase stream capacity.

Answer B wrong because reducing retention frees storage, not ingestion throughput.

Answer C correct because each shard supports 1 MB/s write (1,000 records/s). `UpdateShardCount` increases shards â†’ scales write capacity to handle peak load.

Answer D wrong because `PutRecords` batches records efficiently (max 500), but total throughput is still limited by shard count.

link ref: https://docs.aws.amazon.com/streams/latest/dev/amazon-kinesis-streams.html

350.Explain
Answer A correct because nested for reusable patterns.

Answer B wrong because credentials insecure.

Answer C wrong because remove mappings reduces flexibility.

Answer D wrong because Include for snippets, but public risky.

link ref: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/best-practices.html

351.Explain
Answer A correct because backoffs for rate limits.

Answer B wrong because load balance not for API.

Answer C wrong because EC2 not help.

Answer D wrong because delay worsens.

link ref: https://docs.aws.amazon.com/general/latest/gr/api-retries.html

352.Explain
Answer A wrong because script with cron managed.

Answer B correct because TTL auto-deletes old items.

Answer C wrong because new table daily costly.

Answer D wrong because ItemExpiration not standard; use TTL.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/time-to-live-ttl-how-to.html

353.Explain
Answer A wrong because split smaller still serial.

Answer B wrong because sync one by one slow.

Answer C correct because async event parallel.

Answer D wrong because join first serial.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html

354.Explain
Answer A correct because multi-part for large objects.

Answer B wrong because Direct Connect network, not size.

Answer C wrong because no support contact for size.

Answer D wrong because region not issue.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html

355.Explain
Answer A wrong because docker pull direct not auth.

Answer B correct because get-login for docker login, then pull.

Answer C wrong because get-login output to run.

Answer D wrong because get-download for layers, not pull.

link ref: https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html

356.Explain
Answer A correct because user pools for email sign-up.

Answer B wrong because Mobile Hub deprecated.

Answer C wrong because Sync for data.

Answer D wrong because cloud logic for backend.

link ref: https://aws.amazon.com/cognito/

357.Explain
Answer A wrong because user creds in code insecure.

Answer B correct because execution role for Lambda secure.

Answer C wrong because bucket policy principal for S3, but Lambda needs role.

Answer D wrong because managed policy too broad.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/lambda-intro-execution-role.html

358.Explain
Answer A wrong because KMS not for traffic.

Answer B correct because Origin Protocol HTTPS only.

Answer C wrong because port 443 for origin.

Answer D correct because Viewer Policy HTTPS or redirect.

Answer E wrong because Restrict Viewer for signed URLs.

link ref: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https.html

359.Explain
Answer A wrong because 50 for eventual.

Answer B wrong because 100 for eventual 100 items.

Answer C correct because strong consistent 2x RCU, 100 items * 5KB /4KB *2 =200.

Answer D wrong because 500 too much.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ProvisionedThroughput.html

360.Explain
Answer A wrong because S3 logs are access logs.

Answer B wrong because CloudTrail is API calls.

Answer C correct because CloudWatch collects Lambda logs.

Answer D wrong because DynamoDB no logs.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/monitoring-cloudwatchlogs.html

361.Explain
Answer A wrong because EBS (Elastic Block Store) is a persistent block storage service, not natively attached to Lambda. Using it requires setting up a separate EC2 instance or complex mounting, which is completely overkill and costly for a temporary 100 MB file.

Answer B wrong because EFS (Elastic File System) is a scalable, network-attached file system. While it can be mounted to Lambda, accessing it requires a network connection (VPC setup), which adds latency, complexity, and separate costs for storage and data transfer.

Answer C correct 
AWS Lambda provides ephemeral storage directly inside the execution environment at:  /tmp
Default size = 512 MB (can be increased up to 10 GB if configured)
Fastest storage with no cost
Automatically removed when the execution environment is recycled
(but best practice is to delete files when finished)
Since the application needs only 100 MB temporary files and doesn't need them afterward, /tmp is the most efficient and cheapest option.

Answer D wrong because S3 (Simple Storage Service) is object storage accessed over the internet or VPC network. Writing the file requires an API network call, which is slower than writing to local /tmp. While lifecycle policies handle deletion, the initial write and network usage make it less efficient than local storage.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/runtimes-context.html

362.Explain
Answer A wrong because Automates infrastructure provisioning but does not deploy applications directly or manage Tomcat runtime for you.

Answer B correct
The developer wants to:
âœ” Deploy a web application quickly
âœ” Run it on a Tomcat server
âœ” Avoid managing the underlying infrastructure

Elastic Beanstalk is the perfect solution because:
It supports Tomcat platform out of the box
You just upload your WAR or ZIP package

It automatically handles:
EC2 instances
Auto Scaling
Load Balancing
Patching
Monitoring

This makes it the fastest and easiest way to deploy a Java/Tomcat application without worrying about servers.

Answer C wrong because Not a compute platform â€” cannot run a web app or host Tomcat.

Answer D wrong because CI/CD orchestration tool. Requires a compute platform like Elastic Beanstalk or EC2 to deploy to.

link ref: https://aws.amazon.com/elasticbeanstalk/

363.Explain
Answer A correct ElastiCache (using Redis or Memcached) is a fully managed, high-speed, low-latency in-memory data store specifically designed for use cases like session store. By externalizing the session data here, any EC2 instance can retrieve the session state for any user request, enabling true horizontal scaling and reliability.

Answer B wrong because EBS volumes are block storage attached to a single EC2 instance at a time (unless using Multi-Attach, which is complex and still disk-based). It is too slow (higher latency) for session data and cannot be easily shared by multiple, independent EC2 instances to serve the same session.

Answer C wrong because Instance store is physically attached to the host and is ephemeral (data is lost if the instance is stopped, hibernated, or terminated). This fundamentally violates the reliability requirement for session data.

Answer D wrong because The root filesystem is part of the instance's local storage (either EBS or Instance Store). Writing session data here means it is not shared across the other EC2 instances, forcing you to rely on less effective solutions like ELB sticky sessions, which limit scaling and reliability.

link ref: https://aws.amazon.com/elasticache/

364.Explain
Answer A correct because SQS for async message passing.

Answer B wrong because Cognito auth.

Answer C wrong because Kinesis streaming.

Answer D correct because SNS for pub/sub.

Answer E wrong because ElastiCache caching.

link ref: https://aws.amazon.com/sqs/

365.Explain
Answer A wrong because This is a major security violation. If that single key is compromised, every application and resource using it is exposed. Access keys should be unique and limited to a single application or user.

Answer B correct because 
The AWS root user has unrestricted access to all resources in your account. The root user credentials should be protected extremely tightly.
Security Risk: If the root user access keys are compromised, the attacker gains full control of the entire AWS account.
Best Practice: The root user should never be used for daily administrative tasks or application access. After initial setup, its access keys should be deleted (if created) and its password should be protected with Multi-Factor Authentication (MFA) and secured offline.

Answer C wrong because Unused keys are a security liability. They should be rotated or deleted promptly. Tracking should be done using AWS monitoring tools like IAM Access Analyzer or CloudTrail, not by leaving unused credentials active.

Answer D wrong because While encryption is better than plaintext, embedding keys in code (even if encrypted) is still a bad practice. The code repository itself becomes a highly sensitive target. IAM roles or services like AWS Secrets Manager should be used instead.

Answer E correct because 
Access keys (key ID and secret key) are long-term credentials that grant persistent access until they are rotated or deleted.
IAM Roles provide temporary credentials that are automatically rotated by AWS.
Best Practice: For applications running on AWS services (like EC2, Lambda, ECS, etc.), the application should assume an IAM role. This allows the application to get temporary, time-limited permissions, eliminating the need to embed or manage long-term access keys.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html

366.Explain
Answer A wrong because signature for CLI.

Answer B wrong because configure stores keys, insecure.

Answer C correct because role on instance provides creds.

Answer D wrong because params expose keys.

link ref: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html

367.Explain
Answer A wrong because This violates the horizontal scaling requirement. Without the ALB, the application cannot easily scale or benefit from other ALB features (e.g., health checks, SSL termination).

Answer B wrong because This is an infrastructure change that introduces cost without solving the problem, as Classic Load Balancers also rely on the X-Forwarded-For header for HTTP traffic.

Answer C correct 
Alter the application code to inspect the X-Forwarded-For header.

This is the correct approach because the ALB is already performing the necessary action (injecting the client IP) by default.

The developer simply needs to modify the application logic to read the X-Forwarded-For header instead of the connection's source IP address (which is always the ALB).

The client's true IP is typically the left-most IP address in the comma-separated list within the XFF header.

This solution requires zero infrastructure changes and minimal code modification, making it the most cost-effective option while maintaining horizontal scaling.

Answer D wrong because This is highly impractical, requires client-side changes and maintenance, and is less secure, as clients can easily spoof custom headers. The standard mechanism is already in place.

link ref: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/x-forwarded-headers.html

368.Explain
Answer A wrong because CLI disassociate not exist.

Answer B wrong because AWS CLI no disassociate.

Answer C wrong because policy not for disassociate.

Answer D correct Most Comprehensive Solution. The complete process involves creating a new environment (Green) that connects to the existing RDS instance (which must first be set to Retain), swapping the URLs, and then terminating the old environment.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.managing.db.html

369.Explain
Answer A correct because The prompt states the pipeline is triggered by changes to the master branch. If the developer committed the change to a feature branch, a development branch, or any branch other than master, the Source stage of CodePipeline will not detect the change, and the entire pipeline will not start running the updates. Therefore, CodeDeploy will not be executed.

Answer B correct because AWS CodePipeline operates sequentially. The stages are typically: Source -> Build -> Test -> Deploy.
The stages mentioned are CodeBuild (for test and build) and CodeDeploy (for deployment).
If the CodeBuild stage fails (e.g., unit tests failed, or the build process encountered an error), CodePipeline will immediately stop the execution, and the pipeline will terminate in a Failed state.
The subsequent stage, CodeDeploy, will never be reached or executed.

Answer C wrong because CodePipeline itself does not run on a cluster of EC2 instances; it's a managed service. CodeDeploy targets EC2 instances, and while an inactive instance might cause the deployment to fail on that specific instance, it wouldn't stop the CodeDeploy stage from running or prevent the update from being deployed to the other active instances. The issue implies the deployment didn't even start.

Answer D wrong because The pipeline has been operating successfully for several months with no modifications. This means the configuration was correct and running previously, making an incorrect configuration an unlikely cause for a recent failure.

Answer E wrong because Since the pipeline has been running successfully for several months, it must have had the necessary permissions from the start to access the source code. A sudden, un-modified permissions issue is highly improbable.

link ref: https://docs.aws.amazon.com/codepipeline/latest/userguide/troubleshooting.html

370.Explain
Answer A wrong because User Pools are for user authentication (sign-up, sign-in, tokens), not for cross-device data synchronization and pushing updates. This feature belongs to Cognito Sync (Identity Pools).

Answer B wrong because The SyncCallback interface in the Cognito client SDK is used to manage local synchronization events (like merge conflicts or data deletions) and confirm when the local sync operation has finished. It does not handle the initial, silent push notification from the cloud to the device.

Answer C wrong because A Cognito Stream is used to stream synchronized data out to Amazon Kinesis (or Kinesis Firehose) for analysis or backup. It is an output mechanism and does not provide the input for pushing notifications back to the devices.

Answer D correct because The requirement is to silently notify devices when data changes in the cloud, allowing the devices to pull the updated profile data. This is achieved using Cognito's Push Synchronization feature.
- Cognito Sync (part of Identity Pools) is the core service used for cross-device data synchronization.
- Push Synchronization leverages Amazon Simple Notification Service (SNS) behind the scenes.
  + When an update is written to the Cognito Sync store, the service sends a silent push notification through SNS to all other devices registered to that specific user identity.
  + This notification is not visible to the user but serves as a wake-up call for the mobile application to initiate a synchronization (sync) request and pull the latest data.
- IAM Role: The appropriate IAM role is necessary to grant the Cognito service permission to interact with SNS and send the push notifications.

link ref: https://docs.aws.amazon.com/cognito/latest/developerguide/push-sync.html

371.Explain
Answer A wrong because API Gateway S3 static.

Answer B wrong because Lambda Dynamo serverless.

Answer C correct because EC2 with Aurora for LAMP.

Answer D wrong because Cognito RDS auth + DB.

Answer E wrong because ECS EBS container.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/php-ha-tutorial.html?%20icmpid=docs_tutorial_projects

372.Explain
Answer A correct
SQS long polling allows the consumer to wait for messages to arrive instead of polling immediately and receiving empty responses.

This reduces the delay between message arrival and processing, especially for infrequently updated messages.

It also reduces the number of empty responses and unnecessary API calls.

Answer B wrong because Message size does not affect queue-to-dashboard latency; it affects network payload efficiency.

Answer C wrong because Short polling immediately returns even if no messages exist â†’ increases empty responses and can increase delay when messages arrive just after a poll.

Answer D wrong because Splitting messages increases complexity and may increase delay, as multiple parts must be reassembled before updating the dashboard.

link ref: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html

373.Explain
Answer A wrong because EBS volumes are attached to a single EC2 instance at a time (unless using EBS Multi-Attach, which is complex and only for specific scenarios). This means that scaling the application horizontally (adding new EC2 instances) would require complex sharing solutions or replication, violating the requirement for seamless horizontal scaling.

Answer B correct because Amazon S3 is a highly available, durable object storage service accessible by all instances simultaneously. Moving both shared images and large cache files to S3 ensures that all instances, regardless of how many are scaled up, have access to the same common data store. This satisfies the requirement for horizontal scaling with a modern, decoupled service.

Answer C wrong because Violates Scalability. Leaving the cache data on local disks means that if an instance goes down or a new instance scales up, the cache state is either lost or unavailable to the new instance, leading to an inconsistent user experience or performance issues.

Answer D wrong because Violates Scalability. Storing shared images on local disks means that images uploaded to one EC2 instance are not immediately available to other instances in the scaling group, breaking the core function of a shared application.

link ref: https://aws.amazon.com/s3/

374.Explain
Answer A wrong because SNS sync.

Answer B wrong because SNS async.

Answer C correct because stream sync trigger.

Answer D wrong because async not guarantee order.

link ref: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.Lambda.html

375.Explain
Answer A correct because PATH needed for CLI.
The error aws: command not found indicates that the shell cannot locate the aws command.
This usually happens when the directory containing the AWS CLI executable is not included in the PATH environment variable.
To fix this, the developer should:
Find the installation directory of the AWS CLI (which aws on Linux/macOS or check the installation path on Windows).
Add that directory to the PATH environment variable.
Restart the terminal or command prompt.

Answer B wrong because If installation permissions were denied, the command would not exist at all. The error is specifically about PATH lookup, not file permissions.

Answer C wrong because Credentials affect the execution of commands after aws is found, not whether the aws command itself can be located.

Answer D wrong because On Linux/macOS, lack of executable permissions would result in Permission denied, not "command not found".

link ref: https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html

376.Explain
The key requirements are:
1. **Asynchronous Processing:** The fraud detection takes 10 to 30 minutes, which is too long for a synchronous web request. A queuing system is required.
2. **High Scalability:** The system must handle **100 orders per minute** peak load.
3. **Dynamic Scaling:** The fraud detection fleet needs to scale based on the **backlog** (queue depth) to ensure orders don't pile up during peak times, especially since processing time is long (up to 30 minutes).

Answer A wrong Using a **fixed fleet of 10 EC2 instances** (`min-size=10, max-size=10`) is not scalable. If the processing rate of 10 instances is exceeded, the queue will grow indefinitely, leading to high latency and failure to meet demand.

Answer B correct **Amazon SQS** provides the necessary **asynchronous buffer**. Configuring an **Auto Scaling Group** to use the **SQS Queue Depth metric** (the number of messages visible) as its scaling policy allows the system to be **dynamically sized**. When the load increases (queue depth grows), the ASG automatically launches more EC2 instances (workers) to process the backlog, achieving high scalability and resilience.

Answer C wrong **Amazon Kinesis Stream** and **Lambda** are typically used for real-time, low-latency processing, often measured in seconds or less. The Lambda execution environment has a maximum timeout of 15 minutes, which is **insufficient** for the 30-minute fraud detection task.

Answer D wrong This approach uses **DynamoDB Streams** (for capturing changes) and subscribes a **Lambda function** to read the stream. As with Option C, the 15-minute Lambda timeout **violates** the 30-minute processing time requirement.

link ref: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-using-sqs-queue.html

377.Explain
Answer A wrong because This command changes the locale settings for the environment. It has no effect on the character limit imposed on the total length of environment variables passed to the CodeBuild container.

Answer B wrong because Amazon Cognito is designed for user management, authentication, and authorization. It is not a general-purpose secret or configuration store for CI/CD pipelines.

Answer C wrong because While you could store a configuration file in S3, CodeBuild does not have a native, streamlined feature to automatically import environment variables from S3. This would require custom scripting, making it inefficient.

Answer D correct because Parameter Store is the native AWS solution designed to store configuration data and secrets as key-value pairs. CodeBuild has built-in integration capabilities to read parameters from the Parameter Store (and Secrets Manager), load them securely at runtime, and make them available as environment variables inside the build container, circumventing the direct environment variable limit.

link ref: https://aws.amazon.com/systems-manager/parameter-store/

378.Explain
Answer A wrong because InvalidateCache not API.

Answer B wrong because endpoint custom.

Answer C correct because Cache-Control header invalidates.

Answer D wrong because query param not standard.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-caching.html

379.Explain
Answer A wrong because CloudWatch Lambda not for S3 events.

Answer B correct because S3 Event to Lambda real-time.

Answer C wrong because EC2 cron managed.

Answer D wrong because EMR big data.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html

380.Explain
Answer A wrong because Swagger with Beanstalk not serverless.

Answer B wrong because CodeDeploy not serverless.

Answer C correct because SAM inline Swagger.

Answer D correct because SAM references Swagger file.

Answer E wrong because inline in Lambda not API.

link ref: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-deploying.html

381.Explain
Answer A wrong because Adding the thumbnail generation logic directly into the original function would increase its execution time, thus impacting the user's perceived upload time.

Answer B wrong because This requires the developer to modify the existing, stable Lambda code to add the lambda:Invoke API call, violating the requirement to minimize changes to existing code.

Answer C correct This creates a separate, asynchronous workflow that is triggered after the original file is successfully written to S3. The existing Lambda function is not changed and its execution time remains unaffected, meeting both constraints.

Answer D wrong because This adds an unnecessary SQS Queue and requires the new Lambda function to be scheduled to poll the queue. S3 can notify Lambda directly, making the SQS/scheduler layer redundant for this simple asynchronous task.

link ref: https://docs.aws.amazon.com/lambda/latest/dg/with-s3.html

382.Explain
Answer A wrong because This breaks the API contract for all existing clients currently using the API Gateway's v1 endpoint. It also bypasses API Gateway's security, throttling, and caching features.

Answer B wrong because API Gateway does not have an automatic client migration feature. Phased deployments (Canary releases) only control traffic split between backend integrations (e.g., Lambda versions) on a single stage, not the public endpoint version clients use for migration.

Answer C correct because This is the standard URI-based versioning strategy. The existing stage (e.g., prod or v1) remains deployed and unchanged, serving the old version for six months. A new stage, /v2, is deployed and points to the Lambda function containing the breaking change. Clients are given the new /v2 endpoint and can gradually migrate without affecting existing traffic on /v1.

Answer D wrong because CloudFront is a Content Delivery Network (CDN) used for caching and global delivery. While it can front API Gateway, it is not the service responsible for managing different API versions side-by-side; that responsibility lies with API Gateway Stages.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html

383.Explain
Answer A correct because Cognito user pools with custom authorizer for JWT.

Answer B wrong because custom broker complex.

Answer C wrong because DynamoDB with STS insecure.

Answer D wrong because RDS for creds bad practice.

link ref: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-use-lambda-authorizer.html

384.Explain
Answer A wrong because root standard.

Answer B wrong because bin binaries.

Answer C wrong because ebextension subfolder.

Answer D correct because .ebextensions for configs.

link ref: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/ebextensions.html

385.Explain
Answer A wrong because more RAM increases CPU but connection is bottleneck.

Answer B wrong because DB size not connections.

Answer C correct because global connection reuse reduces init time.

Answer D wrong because DynamoDB no pooling needed.

link ref: https://docs.aws.amazon.com/lambda/latest/operatorguide/connection-reuse.html

386.Explain
Answer A wrong because region not CORS.

Answer B wrong because same bucket not required.

Answer C wrong because port 80 security unrelated.

Answer D correct because CORS enables cross-bucket access.

link ref: https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html

387.Explain
Answer A wrong because user id prefixes not optimal for PUTs.

Answer B wrong because timestamps sequential hotspots.

Answer C wrong because file hashes for files, but folders for prefixes.

Answer D correct because hex hashes on folders distribute PUT requests.

link ref: https://aws.amazon.com/premiumsupport/knowledge-center/s3-request-limit-avoid/

388.Explain
Answer A correct If the application needs to list the bucket (or otherwise perform actions that require ListBucket) and the role lacks that permission, the app will fail. In any case, ensure the role has s3:GetObject (and s3:ListBucket if the app lists objects).

Answer B wrong s3:ListParts is required for resuming a multipart upload or retrieving metadata about an incomplete upload. It is not a necessary permission for simply retrieving or getting an existing object (s3:GetObject).

Answer C wrong kms:ListKeys is an administrative permission used to list all CMKs in an account. It is not required for cryptographic operations like decryption. Only kms:Decrypt and kms:GenerateDataKey are relevant for S3 data access.

Answer D correct Objects encrypted with a customer-managed CMK require the principal (the EC2 instance role) to be allowed to use the key (kms:Decrypt / kms:GenerateDataKey) â€” either via the key policy or via an IAM policy and a permissive key policy. If the key policy blocks the role, S3 cannot return decrypted object bytes to the app.

Answer E correct When you restrict access (for example to a VPC endpoint), the bucket policy must explicitly allow requests coming from that endpoint (commonly using the aws:sourceVpce condition) or the endpoint policy must permit it. Without that, requests from the VPC via the gateway endpoint will be denied.

Answer F wrong The traffic from the EC2 instance to S3 via a Gateway Endpoint is outbound. Furthermore, a Gateway VPC Endpoint is a route target, not a network interface, and does not rely on security group rules on the EC2 instance itself for outbound traffic to S3.

389.Explain
Answer A wrong Not required; this permission is about modifying bucket encryption settings, not uploading objects.

Answer B wrong The user can already call s3:PutObject; the failure is due to KMS, not S3 bucket policy.

Answer C correct During server side encryption S3 will try to generate a unique key from KMS and will not work if the requester IAM role does not have KMS access permissions

Answer D wrong Bucket ACLs are outdated and unnecessary here; permissions already work through IAM + KMS. ACLs would not solve the SSE-KMS failure.

390.Explain
An EC2 instance in Account A needs to read from a Kinesis stream located in Account B.
To enable secure cross-account access, the standard pattern is:

1ï¸âƒ£ In Account B

Create an IAM role that has the required Kinesis read permissions (e.g., kinesis:GetRecords, kinesis:GetShardIterator, etc.).

2ï¸âƒ£ Trust relationship

Modify the trust policy of that role so that it can be assumed by the instance profile role in Account A.

Then the application running on EC2 (Account A) assumes the role in Account B and reads the stream.

Answer A correct This grants the EC2 instance profile role (IAM role attached to the instances) the necessary IAM permissions (e.g., kinesis:GetRecords, kinesis:DescribeStream) to perform read actions on the Kinesis stream ARN in Account B. Without these permissions in the policy attached to the role, the application cannot attempt the actions even if cross-account access is allowed.

Answer B wrong This would be part of an assume-role setup (where Account A's role assumes a role in B for access), but the question focuses on direct access via resource policies, and this step alone doesn't enable cross-account assumption or direct reads.

Answer C wrong Trust policies are only added to the role being assumed (in Account B) to allow principals from Account A to assume it. The instance profile role in Account A needs a permissions policy (not a trust policy) for sts:AssumeRole. Adding a trust policy to the instance profile role doesn't make sense here.

Answer D wrong Trust policies control who can assume a role (for delegation), not direct read permissions on resources like streams. Read access is handled via IAM permissions policies or resource-based policies, not trusts.

Answer E correct Kinesis Data Streams supports resource-based policies (introduced in late 2023), which act like bucket policies for S3. The stream owner in Account B attaches a policy to the stream resource, specifying the instance profile role ARN from Account A as the principal and allowing the read actions. This enables direct cross-account access without needing to assume a role.

391.Explain
Answer A correct To embed (or nest) another CloudFormation stack within a parent template, define a resource in the Resources section of the parent template using the logical ID for the nested stack, followed by the "Type" property set to "AWS::CloudFormation::Stack". This resource type creates the nested stack and allows you to reference its outputs or parameters.
Resources:
  MyNestedStack:
    Type: AWS::CloudFormation::Stack  # Add this line
    Properties:
      TemplateURL: !Sub 'https://s3.amazonaws.com/my-bucket/my-nested-template.yaml'
      Parameters:
        Key1: Value1

Answer B wrong "Mapping" is used in the Mappings section for key-value lookups (e.g., region-specific configs), not for resources. There's no "AWS::CloudFormation::NestedStack" type.

Answer C wrong due to typos (semicolon, misspelled "CloudFormation", and invalid type name). No such type exists.due to typos (semicolon, misspelled "CloudFormation", and invalid type name). No such type exists.

Answer D wrong Again, "Mapping" is for static mappings, not dynamic resources like stacks.

392.Explain
Answer A wrong Uses AWS managed KMS key, which does not support cross-account decryption (cannot grant external principals). SSM Parameter Store requires a customer managed key for shared SecureStrings, making this invalid. Even if fixed, retrieval needs explicit Decrypt=True. Medium (manual decrypt flag; key sharing steps if corrected).

Answer B wrong DynamoDB requires manual encryption/decryption on EC2 (via KMS calls), adding code complexity. Cross-account access uses identity-based IAM policies on the table (no native resource policies), needing more configuration. Not optimized for secrets. High (app-level decrypt; IAM policy management).

Answer C correct AWS Secrets Manager is purpose-built for securely storing and retrieving sensitive data like access tokens. It automatically encrypts secrets at rest using the specified KMS key and ensures encryption in transit via HTTPS. For cross-account access, attach a resource-based policy to the secret ARN, specifying the external account's IAM role as the principal with actions like secretsmanager:GetSecretValue. The EC2 instance's IAM role needs secretsmanager:GetSecretValue permission on the secret ARN. Retrieval via the AWS SDK/CLI (e.g., GetSecretValue) returns the decrypted value directly, minimizing code complexity.
A customer managed KMS key is required for cross-account decryption (AWS managed keys do not support external principals in their key policies). Grant the external role kms:Decrypt on the key via its key policy.

Answer D wrong Uses AWS managed KMS key, preventing cross-account decryption. S3 requires manual retrieval and decryption on EC2, plus bucket policy for access. S3 is for objects, not secrets, increasing error risk. High (manual decrypt; object handling).

393.Explain
Answer A wrong Vague / incomplete: it implies sending events to the main bus but does not mention the required permission configuration on the main event bus to accept cross-account events. The explicit permission and sender rule are needed (covered by D).

Answer B wrong Pushing events directly from each account to the SQS queue using SQS resource policies is possible but more error-prone and harder to manage at scale (you must maintain SQS policies for each account and configure each rule to target the cross-account SQS). Centralizing via the main event bus is cleaner.

Answer C wrong Polling by a Lambda that scans all accounts is inefficient, slower, and less reliable than event-driven delivery. It adds unnecessary cost and complexity.

Answer D correct The recommended, scalable pattern is to centralize events in a main account EventBridge event bus.

To implement this you:

Give the main-account event bus permissions (resource policy) to accept events from the other accounts.

Create an EventBridge rule in each source account that matches EC2 instance lifecycle events and sends (puts) those events to the main account event bus (via PutEvents specifying the main bus ARN).

Create an EventBridge rule on the main account event bus that matches EC2 lifecycle events and adds the single SQS queue in the main account as the rule target.

This approach is efficient, low-latency, and keeps event routing centralized and auditable.

394.Explain
Answer A wrong Event notifications are for reacting to events after they occur; they cannot prevent unauthorized access or enforce per-user access at request time.

Answer B wrong Filtering in the UI is only a client-side control and is not secure. A malicious user could bypass it and directly call S3 if S3 access is not enforced.

Answer C wrong This can be secure, but it introduces unnecessary complexity and cost and may hit payload limits (API Gateway/Lambda are not ideal for direct transfer of large objects up to 300 MB). It also creates a proxy that you must scale and manage.

Answer D correct Enforcement happens server-side (by S3 + IAM) rather than trusting the client or UI â€” users cannot access othersâ€™ files even if they tamper with the client.

It scales and supports large files (300 MB) because clients can upload/download directly to S3 (for example using pre-signed URLs) â€” no API Gateway/Lambda payload limits or extra data-path proxying.

You can combine this with best-practices for additional security: enforce TLS, enable SSE (KMS) for at-rest encryption, use S3 Block Public Access, and restrict access via bucket policy / VPC endpoint if needed.

Minimal management overhead while giving strong, auditable access control.

395.Explain
Answer A wrong Optimized for batch jobs (e.g., HPC workloads) with queuing and compute management, but lacks native sequential orchestration or built-in error/retry logic for data flowsâ€”requires custom scripting for reprocessing. Higher maintenance for workflow coordination.

Answer B correct AWS Step Functions is a serverless orchestration service designed for coordinating distributed applications and microservices, including data processing workflows. It excels at managing sequential execution of tasks (e.g., business rules and transformations), with built-in support for error handling, retries, and reprocessing via states like Catch, Retry, and Choice. For scalability, it automatically handles high volumes of data flows without provisioning infrastructure, integrating seamlessly with services like AWS Lambda (for custom rules), AWS Glue (for ETL transformations), Amazon Kinesis or S3 (for ingestion), and more. This minimizes maintenance, as Step Functions manages state tracking, execution history, and fault tolerance nativelyâ€”no servers to manage or custom retry logic to code.

Answer C wrong Great for serverless ETL (e.g., Spark jobs on ingested data) and data catalogs, but orchestration is limited to job dependencies via triggers/workflows, not flexible sequential business rules or custom error handling. Better as a Step Functions integration point.

Answer D wrong Provides scalable compute for individual rules/transformations, but sequencing and error orchestration would need manual implementation (e.g., via callbacks or external state), increasing complexity and maintenance. Not a dedicated orchestrator.

396.Explain
Answer A wrong Would prevent invocation, not allow reading from S3 but fail only when writing to DynamoDB.

Answer B wrong GSIs are optional and unrelated to inserting records; tables do not require a GSI for writes.

Answer C correct The Lambda function is triggered successfully by an S3 event (indicating proper invocation and S3 read permissions via IAM actions like s3:GetObject), but execution fails specifically during the DynamoDB write operation. This points to missing IAM permissions on the Lambda execution role, such as dynamodb:PutItem, dynamodb:UpdateItem, or dynamodb:BatchWriteItem on the target table ARN. AWS Lambda uses its execution role for service integrations, and without these, API calls to DynamoDB will return an AccessDeniedException.

Answer D wrong DynamoDB is a regional service â€” not tied to Availability Zones. Lambda can write to it from anywhere in the region.

397.Explain
Answer A wrong Causes template sprawl and is hard to manage â€” not scalable.

Answer B wrong Would deploy multiple EC2 instances, not restrict instance selection.

Answer C wrong Inefficient; users could still pick one and you still need validation logic.

Answer D correct To restrict EC2 instances to a list of approved instance types, the recommended CloudFormation approach is to:

Define a parameter for the instance type (e.g., InstanceType)

Use the AllowedValues property to limit the choices to the approved list

Example:

Parameters:
  InstanceType:
    Type: String
    AllowedValues:
      - t3.micro
      - t3.small
      - m5.large
      - c5.xlarge
    Description: Choose an approved EC2 instance type

398.Explain
The BatchGetItem API returns UnprocessedKeys when requests exceed limits like 16 MB response size, 1 MB per partition, or provisioned throughput (causing throttling). These are not errors but partial results requiring handling for resiliency.

Answer A wrong Immediate retries ignore backoff, worsening throttling and reducing resiliency.

Answer B correct AWS recommends retrying only the unprocessed keys with exponential backoff (e.g., doubling wait time per attempt, up to a max like 30s) and jitter (randomized delay) to avoid thundering herd issues and respect capacity. Immediate retries can exacerbate throttling. Implement via a loop in code, e.g., in Python (Boto3)

Answer C wrong AWS SDKs (e.g., Boto3, Java SDK) expose UnprocessedKeys but do not auto-retry themâ€”you must implement logic manually, just like with low-level APIs.

Answer D correct Boosting read capacity units (RCUs) reduces throttling frequency, as BatchGetItem consumes ~1 RCU per 4 KB read (eventually consistent). Monitor via CloudWatch (ThrottledRequests metric) and scale proactively.

Answer E wrong Irrelevant, as BatchGetItem is read-only; writes use BatchWriteItem.

399.Explain
Answer A wrong The SDK generates segments, but you still need the X-Ray daemon to relay data to AWS. SDK alone is not sufficient.

Answer B correct The AWS X-Ray daemon is a standalone process designed for on-premises environments, including Linux servers. It listens for trace data (e.g., UDP on port 2000) from instrumented applications and relays segments directly to the X-Ray service using IAM credentials (e.g., via environment variables or IAM role if applicable). Download the daemon binary from AWS (e.g., S3), make it executable, and run it with minimal flags like ./xray -n us-east-1 -r us-east-1 for region and configuration. This setup requires no code changes beyond basic SDK integration (if not already present) and propagates traces from API Gateway via HTTP headers automatically.
Since API Gateway tracing is enabled, it injects the X-Amzn-Trace-Id header into downstream requests, allowing the daemon to handle relay with low overheadâ€”no custom logic or additional services needed.
The daemon handles:
Sampling
Buffering
Retrying
Network communication with AWS X-Ray

Answer C wrong Overly complex and not needed; traces should be sent directly via daemon.

Answer D wrong Same as C; too much configuration and intended for internal telemetry, not standard tracing flow.

400.Explain
Answer A correct AWS Secrets Manager is the recommended service for securely storing, retrieving, and managing sensitive data like third-party API keys. It provides automatic encryption at rest (using AWS KMS) and in transit, fine-grained access control via IAM policies, auditing through CloudTrail, and optional rotation. Retrieval via the AWS SDK (e.g., GetSecretValue in Boto3) is asynchronous and cached in memory for subsequent calls, ensuring no performance impact after the initial fetch (typically <100 ms latency). This integrates seamlessly with code without embedding secrets.
It provides:

âœ” Automatic encryption of secrets
âœ” Fine-grained IAM access control
âœ” Automatic secret rotation (if needed)
âœ” Low-latency retrieval that does not impact application performance
âœ” Versioning and auditing

Answer B wrong Storing secrets in code or repositories is insecure and a top security risk.

Answer C wrong More secure than option B, but S3 is not a secrets store. Requires extra management and is less secure than Secrets Manager.

Answer D wrong Similar issue as C â€” DynamoDB is not designed for secret storage and requires custom management logic.

401.Explain
Answer A correct This minimizes code changes as the retrieval method remains consistent across environments; only the Parameter Store paths need updating. Secrets Manager securely stores sensitive credentials.

Answer B wrong It cannot directly store variables such as the API URL or credentials

Answer C wrong Storing variables in encrypted files adds operational overhead. Managing separate files for each environment can quickly become cumbersome.

Answer D wrong Storing sensitive information like credentials directly in ECS task definitions is not secure

402.Explain
Answer A correct Minimal Effort. This requires zero code change to the original Lambda function. You configure the destination directly in the Lambda console or via Infrastructure as Code (e.g., CloudFormation, SAM). The failed payload is automatically passed to the destination function. This is the simplest native mechanism for handling asynchronous invocation failures.

Answer B wrong Low Effort, but More than Destination. While using SQS as a destination is valid, it requires two additional configuration steps: setting up the SQS queue and then setting up the resize Lambda function as a polling event source for that SQS queue.

Answer C wrong High Effort. Step Functions is a complex service designed for orchestrating multi-step workflows. It requires creating and managing a state machine definition (ASL), modifying the original S3 event source to point to EventBridge/Step Functions, and defining the catch/fallback state.

Answer D wrong Low Effort, but More than Destination. This requires two additional configuration steps: creating the SNS topic and then creating the subscription for the resize Lambda function. While the complexity is low, it involves more configuration steps than a direct Lambda-to-Lambda destination.
